{
 "cells": [
  {
   "attachments": {
    "dafa11f4-6f01-4b0f-be85-35634a46872a.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "a206e32c-027f-401a-ac49-ad7c04480bb8",
   "metadata": {},
   "source": [
    "# HDB Price Prediction with Sentiment Analysis\n",
    "\n",
    "## Problem Statement\n",
    "Buying a house in Singapore is an expensive and difficult decision to make - yet almost everyone is going to have to make this decision at some point. Our team feels that taking a data-based approach to assessing the value of a house - in the context of its age, location and amenities, would be an intelligent way to determine if a piece of property is \"worth it\". For our project, we will be focussing on HDBs instead of all properties at one shot because the characteristics of both properties are very different - almost belonging to different markets, and attempting to predict both types of housing at the same time is likely to cause the model to underperform as data comes from two different (research on stratified modeling\" or \"segmented modeling.\"), so long as there is sufficient data for each population, which is the case for our selected problem.\n",
    "\n",
    "![hdb_vs_private_property.png](attachment:dafa11f4-6f01-4b0f-be85-35634a46872a.png)\n",
    "\n",
    "## Data Processing, Analysis and Feature Extraction\n",
    "\n",
    "Dataset Overview\n",
    "The experiment utilized Singapore housing resale data with the following characteristics:\n",
    "\n",
    "Size: 200,837 records with 21 features\n",
    "Features: Includes location data (town, latitude, longitude), property characteristics (flat type, floor area, model), temporal information (month, lease commence date), proximity metrics (MRT distance, walking time), and sentiment analysis scores\n",
    "Target Variable: Resale price (SGD)\n",
    "Notable Missing Data: While property features are complete, sentiment scores have significant missing values (>90%), and MRT-related data has approximately 3.5% missing values, this is because Reddit data on location is sparse - a problem exacerbated by the pagination limits (1000/page)\n",
    "\n",
    "The data sources we intend to use are:\n",
    "1. HDB Resale Price History from GovTech - [https://data.gov.sg/collections/189/view]\n",
    "2. OneMap API for HDB Coordinates - [https://www.onemap.gov.sg/apidocs/search]\n",
    "3. OneMap API for Path Routing - [https://www.onemap.gov.sg/apidocs/routing]\n",
    "4. Kaggle for MRT Location Data - [https://www.kaggle.com/datasets/yxlee245/singapore-train-station-coordinates/]\n",
    "5. Reddit API (PRAW) for Sentiment at Locations - [https://praw.readthedocs.io/en/stable/]\n",
    "    - /askSingapore\n",
    "    - /Singapore\n",
    "    - /SingaporeFI\n",
    "\n",
    "The objective is to conduct price prediction on HDB data based off innovative feature generation methods to outperform existing methods that rely on immediately accessible data. \n",
    "\n",
    "HDB price prediction is essentially a regression type problem in ML, hence we will need to carefully process some of our features, such as Reddit Posts and Town Area Names, into numerical encodings that a ML model will be able to process. Encoding this correctly is important so as not to introduce unwanted dependencies between values that are not related in that way (e.g. one-hot encoding for categorical data like Towns).\n",
    "\n",
    "We will also be visualizing the data to get an understanding of the distributions and statistical patterns that they may exhibit. This is important for us when considering which fields to include/exclude for our experiments.\n",
    "\n",
    "Lastly, once our dataset is built, the data will be split into 3 sets, training data, validation data and test data to ensure that we have an accurate representation of our model's performance when it is deployed for application. By ensuring the test data is not part of the training or validation data, we guarantee that our models are not memorizing solutions to existing states, but is in fact capable of generating a an accurate prediction for previously unseen states.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46f9a6c-f498-4dfb-87e6-f75ec7b832c3",
   "metadata": {},
   "source": [
    "## Experiment Design and Machine Learning Algorithm Implementation\n",
    "\n",
    "Having determined that this is a regression-type problem. We have selected 3 types of models - in increasing levels of computational complexity - to attempt to address the problem:\n",
    "1. Multivariate Regression Relatively fast and lightweight computation (linear) - [https://en.wikipedia.org/wiki/Linear_regression]\n",
    "2. Gradient Boosting (XGBoost) Fast and highly configurable gradient boosting model (non-linear) - [https://xgboost.readthedocs.io/en/stable/]\n",
    "3. Deep Neural Networks Configurable computation and complexity (non-linear) - [https://xgboost.readthedocs.io/en/stable/]\n",
    "\n",
    "<i> Note:\n",
    "Both gradient boosting and deep neural networks (DNNs) can capture non-linear relationships in data, but they do so in fundamentally different ways:\n",
    "</i>\n",
    "\n",
    "\n",
    "### Gradient Boosting\n",
    "\n",
    "    #### Model Structure:\n",
    "     \n",
    "    \n",
    "    Builds an ensemble of sequential weak learners (typically decision trees)\n",
    "    Each new tree corrects errors made by the previous trees\n",
    "    Works by minimizing a loss function through gradient descent\n",
    "    \n",
    "    \n",
    "    #### Feature Handling:\n",
    "    \n",
    "    Naturally handles mixed data types without preprocessing\n",
    "    Automatically captures feature interactions through tree splits\n",
    "    Provides clear feature importance metrics\n",
    "    \n",
    "    \n",
    "    #### Training Process:\n",
    "    \n",
    "    Iterative, additive learning process\n",
    "    Can work well with relatively small datasets\n",
    "    Less computation and memory intensive\n",
    "    \n",
    "    \n",
    "    #### Hyperparameters:\n",
    "    \n",
    "    Tree depth, learning rate, number of trees\n",
    "    Regularization parameters to control overfitting\n",
    "    Relatively fewer hyperparameters to tune\n",
    "\n",
    "\n",
    "\n",
    "### Deep Neural Networks\n",
    "\n",
    "    #### Model Structure:\n",
    "    \n",
    "    Multiple layers of interconnected neurons\n",
    "    Universal function approximators that learn hierarchical representations\n",
    "    Complex architectures with different types of layers (dense, convolutional, recurrent)\n",
    "    \n",
    "    \n",
    "    #### Feature Handling:\n",
    "    \n",
    "    Requires numeric data and often normalization\n",
    "    Learns feature representations automatically\n",
    "    Feature importance less transparent\n",
    "    \n",
    "    \n",
    "    #### Training Process:\n",
    "    \n",
    "    Parallel computation through matrix operations\n",
    "    Generally requires larger datasets\n",
    "    More computationally intensive and memory demanding\n",
    "    \n",
    "    \n",
    "    #### Hyperparameters:\n",
    "    \n",
    "    Network architecture (layers, neurons)\n",
    "    Activation functions, optimizers, learning rates\n",
    "    Regularization techniques (dropout, batch normalization)\n",
    "    Many more hyperparameters to tune\n",
    "\n",
    "\n",
    "\n",
    "### Key Differences in Non-linear Modeling\n",
    "\n",
    "    #### Approach to Non-linearity:\n",
    "    \n",
    "    Gradient boosting: Through hierarchical decision boundaries in trees\n",
    "    DNNs: Through activation functions and complex layer transformations\n",
    "    \n",
    "    \n",
    "    ####  Expressiveness vs. Interpretability:\n",
    "    \n",
    "    Gradient boosting: More interpretable, visualizable decisions\n",
    "    DNNs: Black-box with higher theoretical capacity for complex patterns\n",
    "    \n",
    "    \n",
    "    ####  Data Requirements:\n",
    "    \n",
    "    Gradient boosting: Often performs better with limited data\n",
    "    DNNs: Generally need more data to generalize well\n",
    "\n",
    "\n",
    "<i> Note: In practice, gradient boosting (XGBoost, LightGBM, CatBoost) often performs better for tabular data with reasonable dimensionality, while DNNs excel at high-dimensional, unstructured data like images, text, or audio where hierarchical feature learning is beneficial. </i> \n",
    "\n",
    "\n",
    "\n",
    "## Experiment Results and Analysis\n",
    "\n",
    "\n",
    "\n",
    "### 1. Modeling Approach\n",
    "Three distinct modeling approaches were implemented and compared:\n",
    "\n",
    "#### Linear Regression Models:\n",
    "\n",
    "Hyperparameter-tuned with multiple variants (Linear Regression, Ridge, Lasso, ElasticNet)\n",
    "Principal Component Analysis (PCA) applied for dimensionality reduction\n",
    "Best variant: ElasticNet with =0.001, l1_ratio=0.1\n",
    "\n",
    "\n",
    "#### XGBoost Gradient Boosting:\n",
    "\n",
    "Optimized through RandomizedSearchCV hyperparameter tuning\n",
    "Best configuration: max_depth=9, n_estimators=300, learning_rate=0.1, subsample=1.0\n",
    "\n",
    "\n",
    "#### Deep Neural Network (DNN):\n",
    "\n",
    "Multi-layer architecture with batch normalization and dropout regularization\n",
    "Implementation details not explicitly shown in provided output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82502290-df10-44a0-8435-78e4decf16ea",
   "metadata": {},
   "source": [
    "### 2. Performance Comparison\n",
    "\n",
    "\n",
    "#### Model Comparison\n",
    "\n",
    "| Model             | RMSE     | MAE      | R     |\n",
    "|-------------------|----------|----------|--------|\n",
    "| Linear Regression | 86058.08 | 64809.21 | 0.7673 |\n",
    "| XGBoost           | 25931.77 | 18518.15 | 0.9789 |\n",
    "| DNN               | 39828.89 | 26703.80 | 0.9502 |\n",
    "\n",
    "**Best model based on R score: XGBoost**\n",
    "\n",
    "#### XGBoost Hyperparameter Optimization Results\n",
    "\n",
    "#### Top 5 Configurations by CV Score\n",
    "\n",
    "| Rank | CV Score      | subsample | scale_pos_weight | sampling_method | reg_lambda | reg_alpha | n_estimators | min_child_weight | max_leaves | max_depth | max_delta_step | max_bin | learning_rate | grow_policy | gamma | colsample_bytree |\n",
    "|------|---------------|-----------|------------------|-----------------|------------|-----------|--------------|------------------|------------|-----------|----------------|---------|---------------|-------------|-------|------------------|\n",
    "| 1    | 676114736.5612 | 1.0       | 1.5              | uniform         | 1          | 1         | 350          | 1                | 0          | 10        | 0              | 512     | 0.075         | lossguide   | 0     | 0.55             |\n",
    "| 2    | 679837518.6431 | 1.0       | 1.5              | uniform         | 1.25       | 1         | 300          | 1                | 0          | 10        | 0              | 256     | 0.075         | lossguide   | 0     | 0.6              |\n",
    "| 3    | 681004025.3180 | 1.0       | 0.5              | uniform         | 1          | 1         | 350          | 1                | 0          | 9         | 0              | 512     | 0.1           | depthwise   | 0     | 0.55             |\n",
    "| 4    | 682258765.8153 | 1.0       | 1.5              | uniform         | 1.25       | 1.5       | 300          | 1                | 0          | 10        | 0              | 512     | 0.1           | lossguide   | 0     | 0.6              |\n",
    "| 5    | 683345554.8013 | 1.0       | 0.5              | uniform         | 1.25       | 0.75      | 300          | 1                | 0          | 9         | 0              | 256     | 0.1           | lossguide   | 0     | 0.6              |\n",
    "\n",
    "#### Performance Metrics Comparison\n",
    "\n",
    "| Configuration | RMSE     | MAE      | R     |\n",
    "|---------------|----------|----------|--------|\n",
    "| Config_1      | 25612.60 | 18231.61 | 0.9794 |\n",
    "| Config_2      | 25718.57 | 18297.42 | 0.9792 |\n",
    "| Config_3      | 25777.55 | 18404.43 | 0.9791 |\n",
    "| Config_4      | 25763.58 | 18354.98 | 0.9791 |\n",
    "| Config_5      | 25983.51 | 18523.94 | 0.9788 |\n",
    "\n",
    "#### Best Configuration (Config_1) Parameters\n",
    "\n",
    "| Parameter         | Value    |\n",
    "|-------------------|----------|\n",
    "| subsample         | 1.0      |\n",
    "| scale_pos_weight  | 1.5      |\n",
    "| sampling_method   | uniform  |\n",
    "| reg_lambda        | 1        |\n",
    "| reg_alpha         | 1        |\n",
    "| n_estimators      | 350      |\n",
    "| min_child_weight  | 1        |\n",
    "| max_leaves        | 0        |\n",
    "| max_depth         | 10       |\n",
    "| max_delta_step    | 0        |\n",
    "| max_bin           | 512      |\n",
    "| learning_rate     | 0.075    |\n",
    "| grow_policy       | lossguide|\n",
    "| gamma             | 0        |\n",
    "| colsample_bytree  | 0.55     |\n",
    "\n",
    "<i> Note: RMSE emphasizes larger errors due to squaring and is in the same units as your target variable (resale price). It's useful for penalizing large prediction errors.\n",
    "MAE measures the average absolute error and is less sensitive to outliers than RMSE. It's also in the same units as your target variable.\n",
    "R indicates the proportion of variance in the dependent variable explained by the model (ranges from 0 to 1). It's scale-independent, making it useful for comparing models. \n",
    "\n",
    "subsample (1.0): Controls the fraction of samples used for training each tree. Value of 1.0 means use all data for each tree, which helps capture all patterns in your dataset.\n",
    "scale_pos_weight (1.5): Balances positive and negative weights. Your value of 1.5 gives more weight to positive examples, useful when dealing with imbalanced datasets.\n",
    "sampling_method (uniform): Determines how samples are selected. \"Uniform\" means random sampling with equal probability for all observations.\n",
    "reg_lambda (1) and reg_alpha (1): L2 and L1 regularization terms respectively. These help prevent overfitting by penalizing complex models. Your balanced values provide moderate regularization.\n",
    "n_estimators (350): The number of trees (boosting rounds) in your ensemble. Higher value like 350 typically improves performance but increases computational cost.\n",
    "min_child_weight (1): Minimum sum of instance weight needed in a child node. Low value of 1 allows the model to create very specific splits.\n",
    "max_leaves (0): Maximum number of leaves for each tree. Zero means no limit when using lossguide grow policy.\n",
    "max_depth (10): Maximum depth of a tree. Your value of 10 allows for moderately complex trees that can capture intricate patterns.\n",
    "max_delta_step (0): Limits the maximum delta value for each tree's weight estimation. Zero means no constraint.\n",
    "max_bin (512): Number of bins for bucketing continuous features. Higher value (512) provides finer granularity for splits.\n",
    "learning_rate (0.075): Step size shrinkage to prevent overfitting. Your moderate value balances learning speed and stability.\n",
    "grow_policy (lossguide): Controls how trees grow. \"Lossguide\" grows by the highest loss change rather than level-wise, often creating more efficient trees.\n",
    "gamma (0): Minimum loss reduction required for a split. Zero means no minimum threshold.\n",
    "colsample_bytree (0.55): Fraction of features used when constructing each tree. Your value means each tree randomly samples 55% of features, increasing diversity among trees.\n",
    "</i>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d3bef1-b03c-44ff-8bf9-7d152cbe2f58",
   "metadata": {},
   "source": [
    "### 3. Analysis and Findings\n",
    "\n",
    "#### Model Performance\n",
    "\n",
    "XGBoost significantly outperformed other models across all metrics, achieving an impressive R of 0.9801, indicating it captures 98% of variance in housing prices\n",
    "The DNN showed strong but secondary performance compared to XGBoost\n",
    "Linear models performed adequately but were substantially less accurate, suggesting the relationship between features and housing prices is highly non-linear\n",
    "\n",
    "\n",
    "#### Error Distribution:\n",
    "\n",
    "XGBoost achieved the lowest RMSE (25,266.07) and MAE (18,066.53), representing average errors of approximately 4-5% of typical Singapore housing prices\n",
    "Linear model errors were over 3 times higher than XGBoost\n",
    "\n",
    "\n",
    "#### Feature Importance:\n",
    "\n",
    "While not explicitly shown in the output, XGBoost's superior performance suggests it effectively captured complex interactions between location, property characteristics, and sentiment variables\n",
    "Feature importance analysis would likely highlight floor area, location (latitude/longitude), and remaining lease as significant predictors\n",
    "\n",
    "#### Sentiment Analysis Impact:\n",
    "\n",
    "Despite high missingness in sentiment data, the model pipeline effectively handled these gaps\n",
    "The code includes functionality to analyze sentiment impact on predictions, though specific results aren't shown in the output\n",
    "\n",
    "\n",
    "\n",
    "### 4. Areas for Improvement\n",
    "\n",
    "#### Data Enhancement:\n",
    "\n",
    "Address the significant missing values in sentiment analysis data\n",
    "Consider additional feature engineering for location-based factors, especially neighborhood amenities\n",
    "Explore time-series aspects of pricing to capture market trends\n",
    "\n",
    "\n",
    "#### Modeling Refinements:\n",
    "\n",
    "Further fine-tune XGBoost as the best-performing model\n",
    "Consider ensemble approaches combining XGBoost with DNN predictions\n",
    "For the DNN, explore more complex architectures and regularization techniques\n",
    "Implement explicit three-way data splitting (train/validation/test) for more robust evaluation\n",
    "\n",
    "\n",
    "#### Evaluation Extensions:\n",
    "\n",
    "Implement cross-validation for the test set to ensure model generalizability\n",
    "Add residual analysis to identify systematic error patterns\n",
    "Integrate business-relevant metrics like percentage error relative to property value\n",
    "\n",
    "\n",
    "### 5. Conclusion\n",
    "XGBoost demonstrates exceptional predictive capability for Singapore housing prices, significantly outperforming both traditional linear models and deep learning approaches. Its superior performance indicates that housing prices follow complex, non-linear patterns that gradient boosting algorithms can effectively capture. The model's high R suggests it would provide reliable predictions for HDB property prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d329af6-661b-4799-ba3e-2f66b8eab22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resale Prices\n",
    "\n",
    "\"\"\"This dataset contains resale flat transactions from 1999. It includes details on flat type, block, street name, storey range, floor area (sqm), flat model, lease commencement date, and resale price.\"\"\"\n",
    "\n",
    "# We use data from 2017 onwards as Singapore Reddit threads were not as popular prior and would not serve as a significant signal\n",
    "# https://data.gov.sg/collections/189/view\n",
    "import pandas as pd\n",
    "hdb_data = pd.read_csv(r\"ResaleFlatPrices/Resale flat prices based on registration date from Jan-2017 onwards.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf342f6d-c457-4d44-aefd-3b1f529e2061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MRT Data\n",
    "# https://www.kaggle.com/datasets/yxlee245/singapore-train-station-coordinates/\n",
    "mrt_data = pd.read_csv(r\"mrt_locations/mrt_lrt_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05e47e75-0d0b-483b-a315-6610caed6272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>town</th>\n",
       "      <th>flat_type</th>\n",
       "      <th>block</th>\n",
       "      <th>street_name</th>\n",
       "      <th>storey_range</th>\n",
       "      <th>floor_area_sqm</th>\n",
       "      <th>flat_model</th>\n",
       "      <th>lease_commence_date</th>\n",
       "      <th>remaining_lease</th>\n",
       "      <th>resale_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01</td>\n",
       "      <td>ANG MO KIO</td>\n",
       "      <td>2 ROOM</td>\n",
       "      <td>406</td>\n",
       "      <td>ANG MO KIO AVE 10</td>\n",
       "      <td>10 TO 12</td>\n",
       "      <td>44.0</td>\n",
       "      <td>Improved</td>\n",
       "      <td>1979</td>\n",
       "      <td>61 years 04 months</td>\n",
       "      <td>232000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01</td>\n",
       "      <td>ANG MO KIO</td>\n",
       "      <td>3 ROOM</td>\n",
       "      <td>108</td>\n",
       "      <td>ANG MO KIO AVE 4</td>\n",
       "      <td>01 TO 03</td>\n",
       "      <td>67.0</td>\n",
       "      <td>New Generation</td>\n",
       "      <td>1978</td>\n",
       "      <td>60 years 07 months</td>\n",
       "      <td>250000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01</td>\n",
       "      <td>ANG MO KIO</td>\n",
       "      <td>3 ROOM</td>\n",
       "      <td>602</td>\n",
       "      <td>ANG MO KIO AVE 5</td>\n",
       "      <td>01 TO 03</td>\n",
       "      <td>67.0</td>\n",
       "      <td>New Generation</td>\n",
       "      <td>1980</td>\n",
       "      <td>62 years 05 months</td>\n",
       "      <td>262000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01</td>\n",
       "      <td>ANG MO KIO</td>\n",
       "      <td>3 ROOM</td>\n",
       "      <td>465</td>\n",
       "      <td>ANG MO KIO AVE 10</td>\n",
       "      <td>04 TO 06</td>\n",
       "      <td>68.0</td>\n",
       "      <td>New Generation</td>\n",
       "      <td>1980</td>\n",
       "      <td>62 years 01 month</td>\n",
       "      <td>265000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01</td>\n",
       "      <td>ANG MO KIO</td>\n",
       "      <td>3 ROOM</td>\n",
       "      <td>601</td>\n",
       "      <td>ANG MO KIO AVE 5</td>\n",
       "      <td>01 TO 03</td>\n",
       "      <td>67.0</td>\n",
       "      <td>New Generation</td>\n",
       "      <td>1980</td>\n",
       "      <td>62 years 05 months</td>\n",
       "      <td>265000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200832</th>\n",
       "      <td>2025-02</td>\n",
       "      <td>YISHUN</td>\n",
       "      <td>EXECUTIVE</td>\n",
       "      <td>328</td>\n",
       "      <td>YISHUN RING RD</td>\n",
       "      <td>01 TO 03</td>\n",
       "      <td>142.0</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>1988</td>\n",
       "      <td>62 years 05 months</td>\n",
       "      <td>845000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200833</th>\n",
       "      <td>2025-01</td>\n",
       "      <td>YISHUN</td>\n",
       "      <td>EXECUTIVE</td>\n",
       "      <td>614</td>\n",
       "      <td>YISHUN ST 61</td>\n",
       "      <td>04 TO 06</td>\n",
       "      <td>146.0</td>\n",
       "      <td>Maisonette</td>\n",
       "      <td>1987</td>\n",
       "      <td>61 years 05 months</td>\n",
       "      <td>800000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200834</th>\n",
       "      <td>2025-02</td>\n",
       "      <td>YISHUN</td>\n",
       "      <td>EXECUTIVE</td>\n",
       "      <td>723</td>\n",
       "      <td>YISHUN ST 71</td>\n",
       "      <td>07 TO 09</td>\n",
       "      <td>146.0</td>\n",
       "      <td>Maisonette</td>\n",
       "      <td>1986</td>\n",
       "      <td>60 years 05 months</td>\n",
       "      <td>818888.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200835</th>\n",
       "      <td>2025-01</td>\n",
       "      <td>YISHUN</td>\n",
       "      <td>EXECUTIVE</td>\n",
       "      <td>836</td>\n",
       "      <td>YISHUN ST 81</td>\n",
       "      <td>01 TO 03</td>\n",
       "      <td>146.0</td>\n",
       "      <td>Maisonette</td>\n",
       "      <td>1988</td>\n",
       "      <td>62 years 02 months</td>\n",
       "      <td>960000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200836</th>\n",
       "      <td>2025-02</td>\n",
       "      <td>YISHUN</td>\n",
       "      <td>EXECUTIVE</td>\n",
       "      <td>824</td>\n",
       "      <td>YISHUN ST 81</td>\n",
       "      <td>01 TO 03</td>\n",
       "      <td>145.0</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>1987</td>\n",
       "      <td>61 years 10 months</td>\n",
       "      <td>868888.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200837 rows  11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          month        town  flat_type block        street_name storey_range  \\\n",
       "0       2017-01  ANG MO KIO     2 ROOM   406  ANG MO KIO AVE 10     10 TO 12   \n",
       "1       2017-01  ANG MO KIO     3 ROOM   108   ANG MO KIO AVE 4     01 TO 03   \n",
       "2       2017-01  ANG MO KIO     3 ROOM   602   ANG MO KIO AVE 5     01 TO 03   \n",
       "3       2017-01  ANG MO KIO     3 ROOM   465  ANG MO KIO AVE 10     04 TO 06   \n",
       "4       2017-01  ANG MO KIO     3 ROOM   601   ANG MO KIO AVE 5     01 TO 03   \n",
       "...         ...         ...        ...   ...                ...          ...   \n",
       "200832  2025-02      YISHUN  EXECUTIVE   328     YISHUN RING RD     01 TO 03   \n",
       "200833  2025-01      YISHUN  EXECUTIVE   614       YISHUN ST 61     04 TO 06   \n",
       "200834  2025-02      YISHUN  EXECUTIVE   723       YISHUN ST 71     07 TO 09   \n",
       "200835  2025-01      YISHUN  EXECUTIVE   836       YISHUN ST 81     01 TO 03   \n",
       "200836  2025-02      YISHUN  EXECUTIVE   824       YISHUN ST 81     01 TO 03   \n",
       "\n",
       "        floor_area_sqm      flat_model  lease_commence_date  \\\n",
       "0                 44.0        Improved                 1979   \n",
       "1                 67.0  New Generation                 1978   \n",
       "2                 67.0  New Generation                 1980   \n",
       "3                 68.0  New Generation                 1980   \n",
       "4                 67.0  New Generation                 1980   \n",
       "...                ...             ...                  ...   \n",
       "200832           142.0       Apartment                 1988   \n",
       "200833           146.0      Maisonette                 1987   \n",
       "200834           146.0      Maisonette                 1986   \n",
       "200835           146.0      Maisonette                 1988   \n",
       "200836           145.0       Apartment                 1987   \n",
       "\n",
       "           remaining_lease  resale_price  \n",
       "0       61 years 04 months      232000.0  \n",
       "1       60 years 07 months      250000.0  \n",
       "2       62 years 05 months      262000.0  \n",
       "3        62 years 01 month      265000.0  \n",
       "4       62 years 05 months      265000.0  \n",
       "...                    ...           ...  \n",
       "200832  62 years 05 months      845000.0  \n",
       "200833  61 years 05 months      800000.0  \n",
       "200834  60 years 05 months      818888.0  \n",
       "200835  62 years 02 months      960000.0  \n",
       "200836  61 years 10 months      868888.0  \n",
       "\n",
       "[200837 rows x 11 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdb_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f32ad757-9616-4e9a-aeb0-7df70db3b487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_name</th>\n",
       "      <th>type</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jurong East</td>\n",
       "      <td>MRT</td>\n",
       "      <td>1.333207</td>\n",
       "      <td>103.742308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bukit Batok</td>\n",
       "      <td>MRT</td>\n",
       "      <td>1.349069</td>\n",
       "      <td>103.749596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bukit Gombak</td>\n",
       "      <td>MRT</td>\n",
       "      <td>1.359043</td>\n",
       "      <td>103.751863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Choa Chu Kang</td>\n",
       "      <td>MRT</td>\n",
       "      <td>1.385417</td>\n",
       "      <td>103.744316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yew Tee</td>\n",
       "      <td>MRT</td>\n",
       "      <td>1.397383</td>\n",
       "      <td>103.747523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>Punggol Point</td>\n",
       "      <td>LRT</td>\n",
       "      <td>1.416932</td>\n",
       "      <td>103.906680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>Samudera</td>\n",
       "      <td>LRT</td>\n",
       "      <td>1.415955</td>\n",
       "      <td>103.902185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>Nibong</td>\n",
       "      <td>LRT</td>\n",
       "      <td>1.411865</td>\n",
       "      <td>103.900321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>Sumang</td>\n",
       "      <td>LRT</td>\n",
       "      <td>1.408501</td>\n",
       "      <td>103.898605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>Soo Teck</td>\n",
       "      <td>LRT</td>\n",
       "      <td>1.405436</td>\n",
       "      <td>103.897287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>157 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      station_name type       lat         lng\n",
       "0      Jurong East  MRT  1.333207  103.742308\n",
       "1      Bukit Batok  MRT  1.349069  103.749596\n",
       "2     Bukit Gombak  MRT  1.359043  103.751863\n",
       "3    Choa Chu Kang  MRT  1.385417  103.744316\n",
       "4          Yew Tee  MRT  1.397383  103.747523\n",
       "..             ...  ...       ...         ...\n",
       "152  Punggol Point  LRT  1.416932  103.906680\n",
       "153       Samudera  LRT  1.415955  103.902185\n",
       "154         Nibong  LRT  1.411865  103.900321\n",
       "155         Sumang  LRT  1.408501  103.898605\n",
       "156       Soo Teck  LRT  1.405436  103.897287\n",
       "\n",
       "[157 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f7e7e65-e8e3-4c4e-8cbb-9e2dd6b1b63c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'geopy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgeopy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistance\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m geodesic\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpolyline\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'geopy'"
     ]
    }
   ],
   "source": [
    "# Nearest MRT and Walking Distance Routing [Feature Generation]\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from geopy.distance import geodesic\n",
    "import polyline\n",
    "import json\n",
    "\n",
    "class OneMapAPI:\n",
    "    \"\"\"OneMap API wrapper for Singapore addresses and routing\n",
    "        https://www.onemap.gov.sg/apidocs/search\"\"\"\n",
    "    \n",
    "    def __init__(self, token):\n",
    "        self.token = token\n",
    "        self.base_url = \"https://www.onemap.gov.sg\"\n",
    "        self.headers = {\"Authorization\": f\"Bearer {self.token}\"}\n",
    "        \n",
    "    def get_coordinates(self, address):\n",
    "        \"\"\"Get latitude and longitude for an address\"\"\"\n",
    "        url = f\"{self.base_url}/api/common/elastic/search\"\n",
    "        params = {\n",
    "            \"searchVal\": address,\n",
    "            \"returnGeom\": \"Y\",\n",
    "            \"getAddrDetails\": \"Y\",\n",
    "            \"pageNum\": 1\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, params=params, headers=self.headers)\n",
    "            data = response.json()\n",
    "            \n",
    "            if data.get(\"found\", 0) > 0:\n",
    "                result = data[\"results\"][0]\n",
    "                return float(result[\"LATITUDE\"]), float(result[\"LONGITUDE\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting coordinates for {address}: {e}\")\n",
    "        \n",
    "        return None, None\n",
    "    \n",
    "    def get_walking_route(self, start_coords, end_coords):\n",
    "        \"\"\"Get walking route between two coordinate points\"\"\"\n",
    "        start_str = f\"{start_coords[0]},{start_coords[1]}\"\n",
    "        end_str = f\"{end_coords[0]},{end_coords[1]}\"\n",
    "        \n",
    "        url = f\"{self.base_url}/api/public/routingsvc/route\"\n",
    "        params = {\n",
    "            \"start\": start_str,\n",
    "            \"end\": end_str,\n",
    "            \"routeType\": \"walk\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, params=params, headers=self.headers)\n",
    "            data = response.json()\n",
    "            \n",
    "            if data.get(\"status\") == 0:\n",
    "                # Successfully found route\n",
    "                walking_distance = data[\"route_summary\"][\"total_distance\"]  # in meters\n",
    "                walking_time = data[\"route_summary\"][\"total_time\"]  # in seconds\n",
    "                route_geometry = data.get(\"route_geometry\", \"\")\n",
    "                \n",
    "                # Decode polyline to get coordinates if needed\n",
    "                route_coords = polyline.decode(route_geometry) if route_geometry else []\n",
    "                \n",
    "                return {\n",
    "                    \"distance_meters\": walking_distance,\n",
    "                    \"time_seconds\": walking_time,\n",
    "                    \"route_coordinates\": route_coords,\n",
    "                    \"raw_response\": data\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting route from {start_coords} to {end_coords}: {e}\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "def process_hdb_locations(hdb_df, mrt_df, onemap_api, batch_size=50, delay=0.01):\n",
    "    \"\"\"\n",
    "    Process HDB locations, find their coordinates, and calculate nearest MRT station with routing\n",
    "    \n",
    "    Args:\n",
    "        hdb_df: DataFrame with HDB data containing 'block' and 'street_name' columns\n",
    "        mrt_df: DataFrame with MRT data containing 'station_name', 'lat', 'lng' columns\n",
    "        onemap_api: OneMapAPI instance\n",
    "        batch_size: Number of addresses to process before saving interim results\n",
    "        delay: Delay in seconds between API calls to avoid rate limiting\n",
    "    \"\"\"\n",
    "    # Create a copy of the dataframe to work with\n",
    "    df = hdb_df.copy()\n",
    "    \n",
    "    # Add columns for results\n",
    "    df['latitude'] = None\n",
    "    df['longitude'] = None\n",
    "    df['nearest_mrt'] = None\n",
    "    df['mrt_distance_km'] = None\n",
    "    df['walking_distance_m'] = None\n",
    "    df['walking_time_min'] = None\n",
    "    \n",
    "    # Create a set of unique addresses to minimize API calls\n",
    "    addresses = set()\n",
    "    for _, row in df.iterrows():\n",
    "        address = f\"{row['block']} {row['street_name']}\"\n",
    "        addresses.add(address)\n",
    "    \n",
    "    print(f\"Found {len(addresses)} unique addresses to process\")\n",
    "    \n",
    "    # Create address to coordinates mapping\n",
    "    address_coords = {}\n",
    "    \n",
    "    # Process addresses in batches\n",
    "    addresses_list = list(addresses)\n",
    "    total_batches = (len(addresses_list) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for batch_idx in range(total_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min((batch_idx + 1) * batch_size, len(addresses_list))\n",
    "        \n",
    "        print(f\"Processing batch {batch_idx+1}/{total_batches} (addresses {start_idx+1}-{end_idx})\")\n",
    "        \n",
    "        for address in addresses_list[start_idx:end_idx]:\n",
    "            if address not in address_coords:\n",
    "                lat, lng = onemap_api.get_coordinates(address)\n",
    "                if lat and lng:\n",
    "                    address_coords[address] = (lat, lng)\n",
    "                    # print(f\"Got coordinates for '{address}': ({lat}, {lng})\")\n",
    "                else:\n",
    "                    print(f\"Could not find coordinates for '{address}'\")\n",
    "                time.sleep(delay)  # Delay to avoid API rate limits\n",
    "        \n",
    "        # Save interim results\n",
    "        interim_file = f\"address_coords_batch_{batch_idx+1}.json\"\n",
    "        with open(interim_file, 'w') as f:\n",
    "            json.dump(address_coords, f)\n",
    "        print(f\"Saved interim results to {interim_file}\")\n",
    "    \n",
    "    # Update the dataframe with coordinates\n",
    "    for idx, row in df.iterrows():\n",
    "        address = f\"{row['block']} {row['street_name']}\"\n",
    "        if address in address_coords:\n",
    "            lat, lng = address_coords[address]\n",
    "            df.at[idx, 'latitude'] = lat\n",
    "            df.at[idx, 'longitude'] = lng\n",
    "    \n",
    "    # Find nearest MRT station for each HDB flat\n",
    "    for idx, row in df.iterrows():\n",
    "        if pd.notna(row['latitude']) and pd.notna(row['longitude']) and pd.isna(row['nearest_mrt']) and pd.isna(row['walking_time_min']):\n",
    "            hdb_coords = (row['latitude'], row['longitude'])\n",
    "            \n",
    "            # Calculate straight-line distances to all MRT stations\n",
    "            mrt_distances = []\n",
    "            for _, mrt_row in mrt_df.iterrows():\n",
    "                mrt_coords = (mrt_row['lat'], mrt_row['lng'])\n",
    "                distance_km = geodesic(hdb_coords, mrt_coords).km\n",
    "                mrt_distances.append((mrt_row['station_name'], distance_km, mrt_coords))\n",
    "            \n",
    "            # Sort by distance and get the top 3 nearest stations\n",
    "            mrt_distances.sort(key=lambda x: x[1])\n",
    "            top_stations = mrt_distances[:3] \n",
    "            \n",
    "            # For each of the top stations, get the walking route\n",
    "            best_route = None\n",
    "            for station_name, straight_dist, mrt_coords in top_stations:\n",
    "                route = onemap_api.get_walking_route(hdb_coords, mrt_coords)\n",
    "                if route:\n",
    "                    if best_route is None or route['distance_meters'] < best_route['distance_meters']:\n",
    "                        best_route = route\n",
    "                        best_route['station_name'] = station_name\n",
    "                        best_route['straight_distance_km'] = straight_dist\n",
    "                time.sleep(delay)  # Delay to avoid API rate limits\n",
    "            \n",
    "            # Update the dataframe with the best route information\n",
    "            if best_route:\n",
    "                df.at[idx, 'nearest_mrt'] = best_route['station_name']\n",
    "                df.at[idx, 'mrt_distance_km'] = best_route['straight_distance_km']\n",
    "                df.at[idx, 'walking_distance_m'] = best_route['distance_meters']\n",
    "                df.at[idx, 'walking_time_min'] = best_route['time_seconds'] / 60\n",
    "                \n",
    "            # Save interim results every 3000 records\n",
    "            if (idx + 1) % 3000 == 0:\n",
    "                interim_file = f\"hdb_with_mrt_routing_interim_{idx+1}.csv\"\n",
    "                df.to_csv(interim_file, index=False)\n",
    "                print(f\"Saved interim results to {interim_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your actual token\n",
    "    ONEMAP_API_TOKEN = \"eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJmY2NjMTQ3MTRkMTVjNzBlNTA4ODI5OTkzOTgwYmEyMyIsImlzcyI6Imh0dHA6Ly9pbnRlcm5hbC1hbGItb20tcHJkZXppdC1pdC1uZXctMTYzMzc5OTU0Mi5hcC1zb3V0aGVhc3QtMS5lbGIuYW1hem9uYXdzLmNvbS9hcGkvdjIvdXNlci9wYXNzd29yZCIsImlhdCI6MTc0MDQwNzQ5NiwiZXhwIjoxNzQwNjY2Njk2LCJuYmYiOjE3NDA0MDc0OTYsImp0aSI6IndFVmJrMVNYbnBpRkxGengiLCJ1c2VyX2lkIjo2MDk0LCJmb3JldmVyIjpmYWxzZX0.otlQ2o7iEhWunwU5lmEwbG_fWmjV_2WH6VL4IsxOlrE\"\n",
    "    \n",
    "    # Initialize OneMap API\n",
    "    onemap = OneMapAPI(ONEMAP_API_TOKEN)\n",
    "    \n",
    "    # Process the data\n",
    "    file_path = \"hdb_with_mrt_routing_interim_37000.csv\"\n",
    "    result_df = process_hdb_locations(pd.read_csv(file_path), mrt_data, onemap)\n",
    "    \n",
    "    # Save final results\n",
    "    result_df.to_csv(\"hdb_with_mrt_routing.csv\", index=False)\n",
    "    print(\"Processing complete. Results saved to 'hdb_with_mrt_routing.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bac11e7-e958-4c2b-a9fb-3d566b8a81c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdb_data = pd.read_csv('hdb_with_mrt_routing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d3d0fa-4bb3-430f-94de-2a4e97d6885e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdb_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a10ac98-afae-414e-8f4f-22d9b5a17cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section on Clustering Based on Lat Lng [Feature Generation]\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from kneed import KneeLocator\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def find_optimal_clusters(data, max_k=20):\n",
    "    \"\"\"\n",
    "    Find the optimal number of clusters using multiple methods:\n",
    "    1. Elbow method\n",
    "    2. Silhouette score\n",
    "    3. Calinski-Harabasz Index\n",
    "    4. Davies-Bouldin Index\n",
    "\n",
    "    When clustering geographic housing data, the best metric depends on your specific dataset and goals:\n",
    "    \n",
    "    Inertia (Sum of Squared Distances) - Shows how compact clusters are. Lower values mean tighter clusters, but this doesn't account for cluster separation.\n",
    "    Silhouette Score - Balanced measure (range -1 to 1) that considers both cohesion and separation. Higher values indicate better-defined clusters. This is generally very reliable for geographic data.\n",
    "    Calinski-Harabasz Index - Ratio of between-cluster to within-cluster dispersion. Higher values indicate better separation. Works well when clusters are expected to be convex and of similar sizes.\n",
    "    Davies-Bouldin Index - Measures average similarity between clusters. Lower values indicate better clustering with more separated clusters.\n",
    "    \n",
    "    For geographic housing data specifically:\n",
    "    \n",
    "    Silhouette score is often most interpretable and reliable\n",
    "    Calinski-Harabasz works well if you expect similarly-sized neighborhood clusters\n",
    "    The elbow method with inertia is useful for initial estimation\n",
    "    \n",
    "    Use the elbow method to identify a reasonable range for k\n",
    "    Within that range, choose the k-value that maximizes the silhouette score\n",
    "    Validate with Calinski-Harabasz or Davies-Bouldin index\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame with latitude and longitude columns\n",
    "        max_k: Maximum number of clusters to test (approximately number of towns [26])\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with optimal k values from different methods\n",
    "        and the final recommended k value\n",
    "    \"\"\"\n",
    "    # Extract coordinates and scale them\n",
    "    coords = data[['latitude', 'longitude']].values\n",
    "    scaler = StandardScaler()\n",
    "    scaled_coords = scaler.fit_transform(coords)\n",
    "    \n",
    "    # Initialize metrics\n",
    "    inertia = []\n",
    "    silhouette = []\n",
    "    calinski = []\n",
    "    davies = []\n",
    "    k_range = range(9,11) #Original range: 2, max_k + 1, best number of clusters found to be 10\n",
    "    \n",
    "    # Calculate metrics for different k values\n",
    "    for k in k_range:\n",
    "        print(f'Computing for {k} clusters...')\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(scaled_coords)\n",
    "        \n",
    "        # Inertia (Within-cluster sum of squares)\n",
    "        inertia.append(kmeans.inertia_)\n",
    "        \n",
    "        # Silhouette score (higher is better)\n",
    "        sil = silhouette_score(scaled_coords, kmeans.labels_)\n",
    "        silhouette.append(sil)\n",
    "        \n",
    "        # Calinski-Harabasz Index (higher is better)\n",
    "        cal = calinski_harabasz_score(scaled_coords, kmeans.labels_)\n",
    "        calinski.append(cal)\n",
    "        \n",
    "        # Davies-Bouldin Index (lower is better)\n",
    "        dav = davies_bouldin_score(scaled_coords, kmeans.labels_)\n",
    "        davies.append(dav)\n",
    "    \n",
    "    # Find optimal k using Elbow method\n",
    "    kl = KneeLocator(\n",
    "        k_range, inertia, curve=\"convex\", direction=\"decreasing\"\n",
    "    )\n",
    "    k_elbow = kl.elbow if kl.elbow else k_range[np.argmin(np.diff(inertia))]\n",
    "    \n",
    "    # Find optimal k using Silhouette method\n",
    "    k_silhouette = k_range[np.argmax(silhouette)]\n",
    "    \n",
    "    # Find optimal k using Calinski-Harabasz\n",
    "    k_calinski = k_range[np.argmax(calinski)]\n",
    "    \n",
    "    # Find optimal k using Davies-Bouldin\n",
    "    k_davies = k_range[np.argmin(davies)]\n",
    "    \n",
    "    # Combine the results\n",
    "    results = {\n",
    "        'elbow': k_elbow,\n",
    "        'silhouette': k_silhouette,\n",
    "        'calinski_harabasz': k_calinski,\n",
    "        'davies_bouldin': k_davies,\n",
    "        'k_values': list(k_range),\n",
    "        'inertia': inertia,\n",
    "        'silhouette_scores': silhouette,\n",
    "        'calinski_scores': calinski,\n",
    "        'davies_scores': davies\n",
    "    }\n",
    "    \n",
    "    # Determine final k based on majority vote or average\n",
    "    k_values = [k_elbow, k_silhouette, k_calinski, k_davies]\n",
    "    results['recommended_k'] = int(round(np.median(k_values)))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def apply_clustering(data, optimal_k):\n",
    "    \"\"\"\n",
    "    Apply KMeans clustering with the optimal number of clusters.\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame with latitude and longitude columns\n",
    "        optimal_k: The optimal number of clusters\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with cluster assignments\n",
    "    \"\"\"\n",
    "    # Extract coordinates and scale them\n",
    "    coords = data[['latitude', 'longitude']].values\n",
    "    scaler = StandardScaler()\n",
    "    scaled_coords = scaler.fit_transform(coords)\n",
    "    \n",
    "    # Apply KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "    clusters = kmeans.fit_predict(scaled_coords)\n",
    "    \n",
    "    # Add cluster labels to the data\n",
    "    result_df = data.copy()\n",
    "    result_df['geo_cluster'] = clusters\n",
    "    \n",
    "    # Return centers in original scale for interpretation\n",
    "    cluster_centers = scaler.inverse_transform(kmeans.cluster_centers_)\n",
    "    centers_df = pd.DataFrame(\n",
    "        cluster_centers, \n",
    "        columns=['center_latitude', 'center_longitude']\n",
    "    )\n",
    "    centers_df['cluster'] = range(optimal_k)\n",
    "    \n",
    "    return result_df, centers_df\n",
    "\n",
    "def try_dbscan_clustering(data):\n",
    "    \"\"\"\n",
    "    Try DBSCAN clustering as an alternative to KMeans\n",
    "    since DBSCAN can find clusters of arbitrary shapes and\n",
    "    doesn't require specifying the number of clusters.\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame with latitude and longitude columns\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with DBSCAN cluster assignments\n",
    "    \"\"\"\n",
    "    # Extract coordinates and scale them\n",
    "    coords = data[['latitude', 'longitude']].values\n",
    "    scaler = StandardScaler()\n",
    "    scaled_coords = scaler.fit_transform(coords)\n",
    "    \n",
    "    # Find optimal eps parameter using k-distance graph\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    \n",
    "    neighbors = NearestNeighbors(n_neighbors=10)\n",
    "    neighbors_fit = neighbors.fit(scaled_coords)\n",
    "    distances, indices = neighbors_fit.kneighbors(scaled_coords)\n",
    "    \n",
    "    # Sort the distances to the 10th nearest neighbor\n",
    "    distances = np.sort(distances[:, 9])\n",
    "    \n",
    "    # Estimate eps from knee point in k-distance graph\n",
    "    kl = KneeLocator(\n",
    "        range(len(distances)), distances, curve=\"convex\", direction=\"increasing\"\n",
    "    )\n",
    "    knee_point = kl.knee if kl.knee else len(distances) // 10\n",
    "    eps = distances[knee_point]\n",
    "    \n",
    "    # Apply DBSCAN\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=10)\n",
    "    clusters = dbscan.fit_predict(scaled_coords)\n",
    "    \n",
    "    # Add cluster labels to the data\n",
    "    result_df = data.copy()\n",
    "    result_df['dbscan_cluster'] = clusters\n",
    "    \n",
    "    return result_df, eps\n",
    "\n",
    "def visualize_clusters(data, centers=None, cluster_col='geo_cluster', title='KMeans Clustering'):\n",
    "    \"\"\"\n",
    "    Visualize the clusters on a map.\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame with latitude, longitude, and cluster columns\n",
    "        centers: DataFrame with cluster centers (optional)\n",
    "        cluster_col: Column name for cluster labels\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Generate a colormap with enough distinct colors\n",
    "    n_clusters = len(data[cluster_col].unique())\n",
    "    colors = plt.cm.jet(np.linspace(0, 1, n_clusters))\n",
    "    \n",
    "    # Plot each cluster with a different color\n",
    "    for i, color in enumerate(colors):\n",
    "        if i == -1 and cluster_col == 'dbscan_cluster':  # Noise points in DBSCAN\n",
    "            plt.scatter(\n",
    "                data.loc[data[cluster_col] == i, 'longitude'],\n",
    "                data.loc[data[cluster_col] == i, 'latitude'],\n",
    "                s=10, color='black', alpha=0.1, label='Noise'\n",
    "            )\n",
    "        else:\n",
    "            plt.scatter(\n",
    "                data.loc[data[cluster_col] == i, 'longitude'],\n",
    "                data.loc[data[cluster_col] == i, 'latitude'],\n",
    "                s=20, color=color, alpha=0.6, label=f'Cluster {i}'\n",
    "            )\n",
    "    \n",
    "    # Plot cluster centers if provided\n",
    "    if centers is not None:\n",
    "        plt.scatter(\n",
    "            centers['center_longitude'],\n",
    "            centers['center_latitude'],\n",
    "            s=200, marker='X', color='black', label='Cluster Centers'\n",
    "        )\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add legend with smaller font size\n",
    "    if n_clusters <= 15:  # Only show legend if not too many clusters\n",
    "        plt.legend(fontsize='small')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return plt\n",
    "\n",
    "def main():\n",
    "    # Load the HDB resale data\n",
    "    # Replace 'your_hdb_data.csv' with your actual file path\n",
    "    try:\n",
    "        df = pd.read_csv('merged_data_3.csv')\n",
    "        print(f\"Data loaded successfully. Shape: {df.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Check if latitude and longitude columns exist\n",
    "    required_cols = ['latitude', 'longitude']\n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        print(f\"Required columns {required_cols} not found in the data.\")\n",
    "        return\n",
    "    \n",
    "    # Drop rows with missing latitude or longitude\n",
    "    df_clean = df[['latitude', 'longitude']].dropna(subset=required_cols)\n",
    "    print(f\"Data after dropping missing coordinates: {df_clean.shape}\")\n",
    "    \n",
    "    # Find the optimal number of clusters\n",
    "    print(\"Finding optimal number of clusters...\")\n",
    "    results = find_optimal_clusters(df_clean)\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"\\nOptimal number of clusters based on different methods:\")\n",
    "    print(f\"Elbow method: {results['elbow']}\")\n",
    "    print(f\"Silhouette score: {results['silhouette']}\")\n",
    "    print(f\"Calinski-Harabasz Index: {results['calinski_harabasz']}\")\n",
    "    print(f\"Davies-Bouldin Index: {results['davies_bouldin']}\")\n",
    "    print(f\"\\nRecommended number of clusters: {results['recommended_k']}\")\n",
    "    \n",
    "    # Apply KMeans clustering with the optimal number of clusters\n",
    "    print(f\"\\nApplying KMeans clustering with {results['recommended_k']} clusters...\")\n",
    "    clustered_df, centers_df = apply_clustering(df_clean, results['recommended_k'])\n",
    "    \n",
    "    # Display the first few rows of the clustered data\n",
    "    print(\"\\nFirst few rows of the clustered data:\")\n",
    "    print(clustered_df[['latitude', 'longitude', 'geo_cluster']].head())\n",
    "    \n",
    "    # Display the cluster centers\n",
    "    print(\"\\nCluster centers:\")\n",
    "    print(centers_df)\n",
    "    \n",
    "    # Try DBSCAN as an alternative\n",
    "    print(\"\\nTrying DBSCAN clustering as an alternative...\")\n",
    "    dbscan_df, eps = try_dbscan_clustering(df_clean)\n",
    "    \n",
    "    # Count observations in each cluster\n",
    "    kmeans_counts = clustered_df['geo_cluster'].value_counts().sort_index()\n",
    "    print(\"\\nNumber of observations in each KMeans cluster:\")\n",
    "    print(kmeans_counts)\n",
    "    \n",
    "    dbscan_counts = dbscan_df['dbscan_cluster'].value_counts().sort_index()\n",
    "    print(\"\\nNumber of observations in each DBSCAN cluster (including noise as -1):\")\n",
    "    print(dbscan_counts)\n",
    "    \n",
    "    # Visualize KMeans clusters\n",
    "    plt_kmeans = visualize_clusters(\n",
    "        clustered_df, centers_df, \n",
    "        title=f'KMeans Clustering (k={results[\"recommended_k\"]})'\n",
    "    )\n",
    "    plt_kmeans.savefig('kmeans_clusters.png')\n",
    "    \n",
    "    # Visualize DBSCAN clusters\n",
    "    plt_dbscan = visualize_clusters(\n",
    "        dbscan_df, cluster_col='dbscan_cluster',\n",
    "        title=f'DBSCAN Clustering (eps={eps:.3f}, min_samples=10)'\n",
    "    )\n",
    "    plt_dbscan.savefig('dbscan_clusters.png')\n",
    "    \n",
    "    print(\"\\nVisualization saved as 'kmeans_clusters.png' and 'dbscan_clusters.png'\")\n",
    "    \n",
    "    # Plot metrics for different k values\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Elbow method\n",
    "    axes[0, 0].plot(results['k_values'], results['inertia'], 'bo-')\n",
    "    axes[0, 0].axvline(x=results['elbow'], color='r', linestyle='--')\n",
    "    axes[0, 0].set_xlabel('Number of clusters (k)')\n",
    "    axes[0, 0].set_ylabel('Inertia')\n",
    "    axes[0, 0].set_title('Elbow Method')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Silhouette score\n",
    "    axes[0, 1].plot(results['k_values'], results['silhouette_scores'], 'go-')\n",
    "    axes[0, 1].axvline(x=results['silhouette'], color='r', linestyle='--')\n",
    "    axes[0, 1].set_xlabel('Number of clusters (k)')\n",
    "    axes[0, 1].set_ylabel('Silhouette Score')\n",
    "    axes[0, 1].set_title('Silhouette Method')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Calinski-Harabasz Index\n",
    "    axes[1, 0].plot(results['k_values'], results['calinski_scores'], 'mo-')\n",
    "    axes[1, 0].axvline(x=results['calinski_harabasz'], color='r', linestyle='--')\n",
    "    axes[1, 0].set_xlabel('Number of clusters (k)')\n",
    "    axes[1, 0].set_ylabel('Calinski-Harabasz Score')\n",
    "    axes[1, 0].set_title('Calinski-Harabasz Method')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Davies-Bouldin Index\n",
    "    axes[1, 1].plot(results['k_values'], results['davies_scores'], 'co-')\n",
    "    axes[1, 1].axvline(x=results['davies_bouldin'], color='r', linestyle='--')\n",
    "    axes[1, 1].set_xlabel('Number of clusters (k)')\n",
    "    axes[1, 1].set_ylabel('Davies-Bouldin Score')\n",
    "    axes[1, 1].set_title('Davies-Bouldin Method')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('cluster_metrics.png')\n",
    "    print(\"\\nCluster metrics visualization saved as 'cluster_metrics.png'\")\n",
    "    \n",
    "    # Return the clustered dataframe for further use\n",
    "    print(\"\\nClustering completed. You now have:\")\n",
    "    print(\"1. KMeans clusters in 'geo_cluster' column\")\n",
    "    print(\"2. DBSCAN clusters in 'dbscan_cluster' column\")\n",
    "    \n",
    "    return clustered_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cluster_df = main()\n",
    "    cluster_df.to_csv('cluster_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a20135-2e38-4eaa-9d45-53c64ba42068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import praw\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import datetime\n",
    "import re\n",
    "from thefuzz import fuzz, process\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from collections import defaultdict\n",
    "\n",
    "# Optional: transformers for advanced sentiment analysis\n",
    "try:\n",
    "    from transformers import pipeline\n",
    "    TRANSFORMERS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TRANSFORMERS_AVAILABLE = False\n",
    "    print(\"Transformers library not available. Using NLTK only for sentiment analysis.\")\n",
    "\n",
    "# Optional: spaCy for better entity recognition\n",
    "try:\n",
    "    import spacy\n",
    "    SPACY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SPACY_AVAILABLE = False\n",
    "    print(\"spaCy not available. Using regex-based location detection.\")\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Initialize spaCy for NER if available\n",
    "if SPACY_AVAILABLE:\n",
    "    try:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "    except:\n",
    "        # If model not downloaded, download it\n",
    "        import subprocess\n",
    "        subprocess.call([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize sentiment analyzers\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Initialize the advanced sentiment pipeline if available\n",
    "if TRANSFORMERS_AVAILABLE:\n",
    "    sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# Define categories for more specific sentiment analysis\n",
    "CATEGORIES = {\n",
    "    'amenities': ['park', 'mall', 'food', 'restaurant', 'shopping', 'facility', 'gym', 'pool', 'garden'],\n",
    "    'convenience': ['mrt', 'bus', 'transport', 'school', 'hospital', 'market', 'supermarket', 'hawker'],\n",
    "    'property': ['price', 'expensive', 'cheap', 'cost', 'value', 'resale', 'rent', 'hdb', 'condo', 'renovation']\n",
    "}\n",
    "\n",
    "# Function to load Singapore locations from existing dataset or use default list\n",
    "def load_singapore_locations(file_path=None):\n",
    "    \"\"\"Load Singapore locations from existing dataset or use default list.\"\"\"\n",
    "    try:\n",
    "        if file_path and os.path.exists(file_path):\n",
    "            # Try to load from your dataset file\n",
    "            df = pd.read_csv(file_path)\n",
    "            locations = set()\n",
    "            \n",
    "            # Extract town names\n",
    "            if 'town' in df.columns:\n",
    "                locations.update(df['town'].unique())\n",
    "            \n",
    "            # Extract street names and block info\n",
    "            if 'street_name' in df.columns:\n",
    "                locations.update(df['street_name'].unique())\n",
    "        else:\n",
    "            raise FileNotFoundError(\"File not found\")\n",
    "                \n",
    "    except (FileNotFoundError, pd.errors.EmptyDataError):\n",
    "        # Use HDB town as location set\n",
    "        hdb_data = {'town': ['ANG MO KIO','BEDOK','BISHAN', 'BUKIT BATOK', 'BUKIT MERAH', \n",
    "                   'BUKIT PANJANG','BUKIT TIMAH', 'CENTRAL AREA','CHOA CHU KANG',\n",
    "                   'CLEMENTI', 'GEYLANG', 'HOUGANG', 'JURONG EAST', 'JURONG WEST', \n",
    "                   'KALLANG/WHAMPOA', 'MARINE PARADE', 'PASIR RIS', 'PUNGGOL',\n",
    "                   'QUEENSTOWN', 'SEMBAWANG','SENGKANG', 'SERANGOON', 'TAMPINES', \n",
    "                   'TOA PAYOH','WOODLANDS', 'YISHUN']}\n",
    "        locations = set(hdb_data['town'])\n",
    "        \n",
    "    return list(locations)\n",
    "\n",
    "# Load locations\n",
    "sg_locations = load_singapore_locations(file_path='sg_property_data.csv')\n",
    "print(f\"Loaded {len(sg_locations)} Singapore locations\")\n",
    "\n",
    "def identify_locations(text, sg_locations):\n",
    "    \"\"\"Identify Singapore locations mentioned in the text.\"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return []\n",
    "        \n",
    "    # Preprocess text\n",
    "    cleaned_text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    # Potential locations from exact matches\n",
    "    potential_locations = []\n",
    "    \n",
    "    # Use direct and fuzzy matching to identify locations\n",
    "    for location in sg_locations:\n",
    "        # Direct search for longer location names\n",
    "        if len(location) > 5:\n",
    "            if re.search(r'\\b' + re.escape(location) + r'\\b', text, re.IGNORECASE):\n",
    "                potential_locations.append(location)\n",
    "        # Fuzzy search for shorter locations which might have variations\n",
    "        else:\n",
    "            # Get all words and check if any are close to this location\n",
    "            words = cleaned_text.split()\n",
    "            for word in words:\n",
    "                if len(word) > 3 and fuzz.ratio(word.lower(), location.lower()) > 85:\n",
    "                    potential_locations.append(location)\n",
    "                    break\n",
    "    \n",
    "    # Use spaCy for NER to catch additional locations if available\n",
    "    if SPACY_AVAILABLE:\n",
    "        doc = nlp(text)\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"GPE\" or ent.label_ == \"LOC\":\n",
    "                # Use fuzzy matching to map to known Singapore locations\n",
    "                matches = process.extractBests(\n",
    "                    ent.text, \n",
    "                    sg_locations, \n",
    "                    scorer=fuzz.token_set_ratio, \n",
    "                    score_cutoff=70,\n",
    "                    limit=1\n",
    "                )\n",
    "                if matches:\n",
    "                    potential_locations.append(matches[0][0])\n",
    "    \n",
    "    return list(set(potential_locations))  # Remove duplicates\n",
    "\n",
    "def categorize_text(text):\n",
    "    \"\"\"Categorize the text into predefined categories.\"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return None\n",
    "        \n",
    "    text = text.lower()\n",
    "    \n",
    "    for category, keywords in CATEGORIES.items():\n",
    "        for keyword in keywords:\n",
    "            if re.search(r'\\b' + re.escape(keyword) + r'\\b', text):\n",
    "                return category\n",
    "    \n",
    "    return 'general'\n",
    "\n",
    "def analyze_sentiment(text, advanced=True):\n",
    "    \"\"\"Analyze sentiment of text using both NLTK and HuggingFace if available.\"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return {'compound': 0, 'neg': 0, 'neu': 0, 'pos': 0}\n",
    "        \n",
    "    # Basic NLTK sentiment analysis\n",
    "    nltk_sentiment = sia.polarity_scores(text)\n",
    "    \n",
    "    if advanced and TRANSFORMERS_AVAILABLE:\n",
    "        try:\n",
    "            # More advanced sentiment analysis with HuggingFace\n",
    "            hf_result = sentiment_pipeline(text[:512])[0]  # Truncate to prevent token limit errors\n",
    "            \n",
    "            # Normalize HuggingFace result to match NLTK scale\n",
    "            if hf_result['label'] == 'POSITIVE':\n",
    "                hf_score = hf_result['score']\n",
    "            else:\n",
    "                hf_score = -hf_result['score']\n",
    "            \n",
    "            # Combine both scores (weighted average)\n",
    "            nltk_weight = 0.4\n",
    "            hf_weight = 0.6\n",
    "            \n",
    "            combined_compound = (nltk_sentiment['compound'] * nltk_weight) + (hf_score * hf_weight)\n",
    "            nltk_sentiment['compound'] = max(min(combined_compound, 1.0), -1.0)  # Ensure within [-1,1]\n",
    "        except Exception as e:\n",
    "            print(f\"Error with advanced sentiment analysis: {e}\")\n",
    "            # Fall back to basic NLTK if error occurs\n",
    "            pass\n",
    "            \n",
    "    return nltk_sentiment\n",
    "\n",
    "# New function to display sentiment samples with text and scores\n",
    "def display_sentiment_samples(location_mentions_df, n_samples=5, min_score=None, max_score=None, category=None, location=None):\n",
    "    \"\"\"\n",
    "    Display samples of text with sentiment scores for manual inspection.\n",
    "    \n",
    "    Args:\n",
    "        location_mentions_df (DataFrame): DataFrame with location mentions\n",
    "        n_samples (int): Number of samples to display\n",
    "        min_score (float, optional): Minimum sentiment score to include\n",
    "        max_score (float, optional): Maximum sentiment score to include\n",
    "        category (str, optional): Category to filter by\n",
    "        location (str, optional): Location to filter by\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    filtered_df = location_mentions_df.copy()\n",
    "    \n",
    "    # Apply filters\n",
    "    if min_score is not None:\n",
    "        filtered_df = filtered_df[filtered_df['sentiment_score'] >= min_score]\n",
    "    \n",
    "    if max_score is not None:\n",
    "        filtered_df = filtered_df[filtered_df['sentiment_score'] <= max_score]\n",
    "    \n",
    "    if category is not None:\n",
    "        filtered_df = filtered_df[filtered_df['category'] == category]\n",
    "    \n",
    "    if location is not None:\n",
    "        filtered_df = filtered_df[filtered_df['location'] == location]\n",
    "    \n",
    "    # Sort by sentiment score (highest or lowest based on filters)\n",
    "    if min_score is not None and max_score is None:\n",
    "        # If only min_score is provided, show highest scores first\n",
    "        filtered_df = filtered_df.sort_values('sentiment_score', ascending=False)\n",
    "    elif max_score is not None and min_score is None:\n",
    "        # If only max_score is provided, show lowest scores first\n",
    "        filtered_df = filtered_df.sort_values('sentiment_score', ascending=True)\n",
    "    else:\n",
    "        # In other cases, show most extreme scores first (furthest from 0)\n",
    "        filtered_df['abs_score'] = abs(filtered_df['sentiment_score'])\n",
    "        filtered_df = filtered_df.sort_values('abs_score', ascending=False)\n",
    "        filtered_df = filtered_df.drop('abs_score', axis=1)\n",
    "    \n",
    "    # Take samples\n",
    "    samples = filtered_df.head(n_samples)\n",
    "    \n",
    "    if len(samples) == 0:\n",
    "        print(\"No samples found matching the criteria.\")\n",
    "        return\n",
    "    \n",
    "    # Format and display samples\n",
    "    for i, (_, row) in enumerate(samples.iterrows(), 1):\n",
    "        print(f\"SAMPLE {i}\")\n",
    "        print(f\"Location: {row['location']}\")\n",
    "        print(f\"Category: {row['category']}\")\n",
    "        print(f\"Sentiment Score: {row['sentiment_score']:.3f} (Pos: {row['pos_score']:.2f}, Neg: {row['neg_score']:.2f}, Neu: {row['neu_score']:.2f})\")\n",
    "        print(f\"Date: {row['date']}\")\n",
    "        print(f\"Source Type: {row['source_type']}\")\n",
    "        print(f\"Text: {row['text']}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# Simplified Reddit scraper to get top X posts and top Y comments per post\n",
    "def scrape_reddit_top_posts(subreddits, top_posts_total=1000, top_comments_per_post=10, start_year=2017):\n",
    "    \"\"\"\n",
    "    Scrape Reddit posts and comments focusing on top posts of all time.\n",
    "    \n",
    "    Args:\n",
    "        subreddits (list): List of subreddit names to scrape\n",
    "        top_posts_total (int): Total number of top posts to get across all subreddits\n",
    "        top_comments_per_post (int): Number of top comments to get per post\n",
    "        start_year (int): Only include posts from this year onwards\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (posts_df, comments_df, location_mentions_df) DataFrames with scraped data\n",
    "    \"\"\"\n",
    "    # Check for Reddit API credentials\n",
    "    client_id = os.getenv(\"REDDIT_CLIENT_ID\")\n",
    "    client_secret = os.getenv(\"REDDIT_CLIENT_SECRET\")\n",
    "    user_agent = os.getenv(\"REDDIT_USER_AGENT\", \"python:sg-location-scraper:v1.0 (by u/your_username)\")\n",
    "    \n",
    "    if not client_id or not client_secret:\n",
    "        raise ValueError(\"Reddit API credentials not found. Set REDDIT_CLIENT_ID and REDDIT_CLIENT_SECRET environment variables.\")\n",
    "    \n",
    "    # Initialize Reddit API\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=client_id,\n",
    "        client_secret=client_secret,\n",
    "        user_agent=user_agent\n",
    "    )\n",
    "    \n",
    "    # Create empty lists to store data\n",
    "    posts_data = []\n",
    "    comments_data = []\n",
    "    location_mentions = []\n",
    "    \n",
    "    # Calculate timestamp for start_year (January 1st of the start_year)\n",
    "    start_timestamp = datetime.datetime(start_year, 1, 1).timestamp()\n",
    "    \n",
    "    # Calculate posts per subreddit\n",
    "    posts_per_subreddit = top_posts_total // len(subreddits)\n",
    "    \n",
    "    # Process each subreddit\n",
    "    for subreddit_name in subreddits:\n",
    "        print(f\"Scraping top posts from r/{subreddit_name} from {start_year} onwards...\")\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        \n",
    "        # Get top posts for this subreddit\n",
    "        collected_posts = 0\n",
    "        \n",
    "        # Try different time filters to maximize the chances of getting good data\n",
    "        for time_filter in ['all', 'year', 'month']:\n",
    "            if collected_posts >= posts_per_subreddit:\n",
    "                break\n",
    "                \n",
    "            print(f\"Getting top posts with time filter: {time_filter}\")\n",
    "            \n",
    "            try:\n",
    "                for post in tqdm(subreddit.top(time_filter=time_filter, limit=posts_per_subreddit), \n",
    "                                desc=f\"Top posts from r/{subreddit_name}\"):\n",
    "                    # Skip posts before start_year\n",
    "                    if post.created_utc < start_timestamp:\n",
    "                        continue\n",
    "                        \n",
    "                    # Skip posts that we've already processed (by ID)\n",
    "                    if any(p['id'] == post.id for p in posts_data):\n",
    "                        continue\n",
    "                    \n",
    "                    # Convert timestamp to datetime\n",
    "                    post_date = datetime.datetime.fromtimestamp(post.created_utc)\n",
    "                    post_month = post_date.strftime('%Y-%m')\n",
    "                    \n",
    "                    # Store post data\n",
    "                    post_data = {\n",
    "                        'id': post.id,\n",
    "                        'title': post.title,\n",
    "                        'body': post.selftext,\n",
    "                        'author': str(post.author),\n",
    "                        'created_utc': post.created_utc,\n",
    "                        'score': post.score,\n",
    "                        'subreddit': subreddit_name,\n",
    "                        'url': post.url,\n",
    "                        'month': post_month,\n",
    "                        'date': post_date\n",
    "                    }\n",
    "                    posts_data.append(post_data)\n",
    "                    collected_posts += 1\n",
    "                    \n",
    "                    # Process post for location mentions\n",
    "                    full_text = f\"{post.title} {post.selftext}\"\n",
    "                    locations = identify_locations(full_text, sg_locations)\n",
    "                    \n",
    "                    if locations:\n",
    "                        category = categorize_text(full_text)\n",
    "                        sentiment = analyze_sentiment(full_text)\n",
    "                        \n",
    "                        for location in locations:\n",
    "                            location_mention = {\n",
    "                                'source_id': post.id,\n",
    "                                'source_type': 'post',\n",
    "                                'location': location,\n",
    "                                'sentiment_score': sentiment['compound'],\n",
    "                                'pos_score': sentiment['pos'],\n",
    "                                'neg_score': sentiment['neg'],\n",
    "                                'neu_score': sentiment['neu'],\n",
    "                                'category': category,\n",
    "                                'month': post_month,\n",
    "                                'date': post_date,\n",
    "                                'text': full_text[:300] + ('...' if len(full_text) > 300 else '')  # Store sample text\n",
    "                            }\n",
    "                            location_mentions.append(location_mention)\n",
    "                    \n",
    "                    # Get top comments for this post\n",
    "                    try:\n",
    "                        post.comments.replace_more(limit=0)  # Don't expand \"more comments\" to avoid API rate limits\n",
    "                        \n",
    "                        # Get all top-level comments and sort by score\n",
    "                        top_level_comments = [comment for comment in post.comments if not isinstance(comment, praw.models.MoreComments)]\n",
    "                        top_level_comments.sort(key=lambda x: x.score, reverse=True)\n",
    "                        \n",
    "                        # Take top N comments\n",
    "                        for comment in top_level_comments[:top_comments_per_post]:\n",
    "                            # Store comment data\n",
    "                            comment_date = datetime.datetime.fromtimestamp(comment.created_utc)\n",
    "                            comment_month = comment_date.strftime('%Y-%m')\n",
    "                            \n",
    "                            comment_data = {\n",
    "                                'id': comment.id,\n",
    "                                'post_id': post.id,\n",
    "                                'body': comment.body,\n",
    "                                'author': str(comment.author),\n",
    "                                'created_utc': comment.created_utc,\n",
    "                                'score': comment.score,\n",
    "                                'month': comment_month,\n",
    "                                'date': comment_date\n",
    "                            }\n",
    "                            comments_data.append(comment_data)\n",
    "                            \n",
    "                            # Process comment for location mentions\n",
    "                            locations = identify_locations(comment.body, sg_locations)\n",
    "                            \n",
    "                            if locations:\n",
    "                                category = categorize_text(comment.body)\n",
    "                                sentiment = analyze_sentiment(comment.body)\n",
    "                                \n",
    "                                for location in locations:\n",
    "                                    location_mention = {\n",
    "                                        'source_id': comment.id,\n",
    "                                        'source_type': 'comment',\n",
    "                                        'location': location,\n",
    "                                        'sentiment_score': sentiment['compound'],\n",
    "                                        'pos_score': sentiment['pos'],\n",
    "                                        'neg_score': sentiment['neg'],\n",
    "                                        'neu_score': sentiment['neu'],\n",
    "                                        'category': category,\n",
    "                                        'month': comment_month,\n",
    "                                        'date': comment_date,\n",
    "                                        'text': comment.body[:300] + ('...' if len(comment.body) > 300 else '')  # Store sample text\n",
    "                                    }\n",
    "                                    location_mentions.append(location_mention)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing comments for post {post.id}: {e}\")\n",
    "                    \n",
    "                    if collected_posts >= posts_per_subreddit:\n",
    "                        break\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Error getting top posts with time filter '{time_filter}': {e}\")\n",
    "        \n",
    "        print(f\"Collected {collected_posts} posts from r/{subreddit_name}\")\n",
    "    \n",
    "    # Convert lists to DataFrames\n",
    "    posts_df = pd.DataFrame(posts_data) if posts_data else pd.DataFrame(columns=['id', 'title', 'body', 'author', 'created_utc', 'score', 'subreddit', 'url', 'month', 'date'])\n",
    "    comments_df = pd.DataFrame(comments_data) if comments_data else pd.DataFrame(columns=['id', 'post_id', 'body', 'author', 'created_utc', 'score', 'month', 'date'])\n",
    "    location_mentions_df = pd.DataFrame(location_mentions) if location_mentions else pd.DataFrame(columns=['source_id', 'source_type', 'location', 'sentiment_score', 'pos_score', 'neg_score', 'neu_score', 'category', 'month', 'date', 'text'])\n",
    "    \n",
    "    print(f\"Scraped {len(posts_df)} posts, {len(comments_df)} comments, and found {len(location_mentions_df)} location mentions\")\n",
    "    \n",
    "    if len(posts_df) == 0:\n",
    "        print(\"\\nWARNING: No posts were collected! This could be due to:\")\n",
    "        print(\"1. Reddit API rate limits or authentication issues\")\n",
    "        print(\"2. The subreddits might not have posts in the specified time range\")\n",
    "        print(\"3. The Reddit API might be experiencing issues\")\n",
    "        print(\"\\nPlease check your credentials and try again with different parameters.\")\n",
    "    \n",
    "    return posts_df, comments_df, location_mentions_df\n",
    "\n",
    "# Generate sentiment report\n",
    "def generate_sentiment_report(location_mentions_df, month=None):\n",
    "    \"\"\"Generate sentiment report for locations by month.\"\"\"\n",
    "    # Filter by month if specified\n",
    "    if month:\n",
    "        filtered_df = location_mentions_df[location_mentions_df['month'] == month]\n",
    "    else:\n",
    "        filtered_df = location_mentions_df\n",
    "        \n",
    "    # Group by location, category, and month\n",
    "    grouped = filtered_df.groupby(['location', 'category', 'month']).agg({\n",
    "        'sentiment_score': 'mean',\n",
    "        'source_id': 'count'  # Count of mentions\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    grouped = grouped.rename(columns={'source_id': 'mention_count'})\n",
    "    \n",
    "    return grouped\n",
    "\n",
    "def get_popular_locations(location_mentions_df, month=None, category=None, top_n=10):\n",
    "    \"\"\"Get most popular locations based on sentiment.\"\"\"\n",
    "    # Generate sentiment report\n",
    "    report_df = generate_sentiment_report(location_mentions_df, month)\n",
    "    \n",
    "    # Filter by category if specified\n",
    "    if category:\n",
    "        report_df = report_df[report_df['category'] == category]\n",
    "        \n",
    "    # Group by location and get average sentiment\n",
    "    location_sentiment = report_df.groupby('location')['sentiment_score'].mean().reset_index()\n",
    "    \n",
    "    # Also get mention counts\n",
    "    location_counts = report_df.groupby('location')['mention_count'].sum().reset_index()\n",
    "    \n",
    "    # Merge sentiment and counts\n",
    "    location_data = pd.merge(location_sentiment, location_counts, on='location')\n",
    "    \n",
    "    # Sort by sentiment (descending) to get most positive first\n",
    "    location_data = location_data.sort_values('sentiment_score', ascending=False)\n",
    "    \n",
    "    # Get top N popular and unpopular\n",
    "    popular = location_data.head(top_n)\n",
    "    unpopular = location_data.tail(top_n).sort_values('sentiment_score')\n",
    "    \n",
    "    return popular, unpopular\n",
    "\n",
    "def plot_location_sentiment(location_mentions_df, top_n=10, category=None, month=None):\n",
    "    \"\"\"Plot sentiment analysis results for top and bottom locations.\"\"\"\n",
    "    popular, unpopular = get_popular_locations(\n",
    "        location_mentions_df, month=month, category=category, top_n=top_n\n",
    "    )\n",
    "    \n",
    "    # Plot settings\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Top subplot for popular locations\n",
    "    plt.subplot(2, 1, 1)\n",
    "    sns.barplot(x='sentiment_score', y='location', data=popular, \n",
    "                hue='mention_count', palette='YlGnBu', dodge=False)\n",
    "    plt.title(f'Top {top_n} Popular Locations by Sentiment'\n",
    "              + (f' - {category.capitalize()}' if category else '')\n",
    "              + (f' - {month}' if month else ''))\n",
    "    plt.xlabel('Sentiment Score')\n",
    "    plt.ylabel('Location')\n",
    "    \n",
    "    # Bottom subplot for unpopular locations\n",
    "    plt.subplot(2, 1, 2)\n",
    "    sns.barplot(x='sentiment_score', y='location', data=unpopular, \n",
    "                hue='mention_count', palette='YlOrRd_r', dodge=False)\n",
    "    plt.title(f'Bottom {top_n} Unpopular Locations by Sentiment'\n",
    "              + (f' - {category.capitalize()}' if category else '')\n",
    "              + (f' - {month}' if month else ''))\n",
    "    plt.xlabel('Sentiment Score')\n",
    "    plt.ylabel('Location')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_monthly_trends(location_mentions_df, locations=None, category=None):\n",
    "    \"\"\"Plot sentiment trends over time for specific locations.\"\"\"\n",
    "    if not locations:\n",
    "        # If no locations specified, use top 5 most mentioned\n",
    "        top_locations = location_mentions_df['location'].value_counts().head(5).index.tolist()\n",
    "    else:\n",
    "        top_locations = locations\n",
    "        \n",
    "    # Filter by category if specified\n",
    "    if category:\n",
    "        filtered_df = location_mentions_df[location_mentions_df['category'] == category]\n",
    "    else:\n",
    "        filtered_df = location_mentions_df\n",
    "        \n",
    "    # Convert date column to datetime if it's not already\n",
    "    if 'date' in filtered_df.columns and not pd.api.types.is_datetime64_any_dtype(filtered_df['date']):\n",
    "        filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n",
    "        \n",
    "    # Filter to include only the specified locations\n",
    "    location_df = filtered_df[filtered_df['location'].isin(top_locations)]\n",
    "    \n",
    "    # Group by location and month, calculate average sentiment\n",
    "    monthly_sentiment = location_df.groupby(['location', 'month'])['sentiment_score'].mean().reset_index()\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Plot each location as a separate line\n",
    "    for location in top_locations:\n",
    "        location_data = monthly_sentiment[monthly_sentiment['location'] == location]\n",
    "        \n",
    "        # Sort by month to ensure correct timeline\n",
    "        location_data = location_data.sort_values('month')\n",
    "        \n",
    "        plt.plot(location_data['month'], location_data['sentiment_score'], \n",
    "                 marker='o', linewidth=2, label=location)\n",
    "    \n",
    "    plt.title(f'Monthly Sentiment Trends for Top Locations'\n",
    "              + (f' - {category.capitalize()}' if category else ''))\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Average Sentiment Score')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_sentiment_heatmap(location_mentions_df, top_locations=5):\n",
    "    \"\"\"\n",
    "    Create a heatmap showing sentiment over time for top locations.\n",
    "    \"\"\"\n",
    "    # Get most mentioned locations\n",
    "    most_mentioned = location_mentions_df['location'].value_counts().head(top_locations).index.tolist()\n",
    "    \n",
    "    # Filter data for top locations\n",
    "    top_data = location_mentions_df[location_mentions_df['location'].isin(most_mentioned)]\n",
    "    \n",
    "    # Make sure date is in datetime format\n",
    "    if not pd.api.types.is_datetime64_any_dtype(top_data['date']):\n",
    "        top_data['date'] = pd.to_datetime(top_data['date'])\n",
    "    \n",
    "    # Group by location and month\n",
    "    sentiment_by_month = top_data.groupby(['location', 'month'])['sentiment_score'].mean().reset_index()\n",
    "    \n",
    "    # Create pivot table for heatmap\n",
    "    pivot_data = sentiment_by_month.pivot(index='month', columns='location', values='sentiment_score')\n",
    "    \n",
    "    # Sort rows by month\n",
    "    pivot_data = pivot_data.sort_index()\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    sns.heatmap(pivot_data, annot=True, cmap='RdBu_r', center=0, fmt='.2f',\n",
    "                linewidths=0.5, cbar_kws={'label': 'Sentiment Score'})\n",
    "    plt.title('Monthly Sentiment by Location')\n",
    "    plt.xlabel('Location')\n",
    "    plt.ylabel('Month')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return pivot_data\n",
    "    \n",
    "def plot_category_distribution(location_mentions_df, locations=None):\n",
    "    \"\"\"\n",
    "    Plot the distribution of categories for selected locations.\n",
    "    \n",
    "    Args:\n",
    "        location_mentions_df (DataFrame): DataFrame with location mentions\n",
    "        locations (list, optional): List of locations to include, uses top 5 if None\n",
    "    \"\"\"\n",
    "    # Get top locations if not specified\n",
    "    if locations is None:\n",
    "        locations = location_mentions_df['location'].value_counts().head(5).index.tolist()\n",
    "    \n",
    "    # Filter data for selected locations\n",
    "    filtered_df = location_mentions_df[location_mentions_df['location'].isin(locations)]\n",
    "    \n",
    "    # Calculate category distribution for each location\n",
    "    location_categories = filtered_df.groupby(['location', 'category']).size().reset_index(name='count')\n",
    "    \n",
    "    # Create a grouped bar chart\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Plot\n",
    "    sns.barplot(x='location', y='count', hue='category', data=location_categories, palette='viridis')\n",
    "    \n",
    "    plt.title('Category Distribution by Location')\n",
    "    plt.xlabel('Location')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='Category')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_sentiment_distribution(location_mentions_df, bin_width=0.1):\n",
    "    \"\"\"\n",
    "    Plot the distribution of sentiment scores.\n",
    "    \n",
    "    Args:\n",
    "        location_mentions_df (DataFrame): DataFrame with location mentions\n",
    "        bin_width (float): Width of histogram bins\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Create histogram\n",
    "    sns.histplot(location_mentions_df['sentiment_score'], \n",
    "                 bins=np.arange(-1, 1 + bin_width, bin_width),\n",
    "                 kde=True, color='skyblue')\n",
    "    \n",
    "    plt.title('Distribution of Sentiment Scores')\n",
    "    plt.xlabel('Sentiment Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def merge_with_property_data(location_mentions_df, property_file='hdb_with_mrt_routing.csv', output_file='merged_data.csv'):\n",
    "    \"\"\"\n",
    "    Merge sentiment data with property resale data.\n",
    "    \n",
    "    Args:\n",
    "        location_mentions_df (DataFrame): DataFrame with location mentions\n",
    "        property_file (str): Path to property data CSV file\n",
    "        output_file (str): Path to save merged data CSV file\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Merged data or None if property data file not found\n",
    "    \"\"\"\n",
    "    # Load property data\n",
    "    try:\n",
    "        property_df = pd.read_csv(property_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Property data file {property_file} not found.\")\n",
    "        return None\n",
    "        \n",
    "    # Prepare sentiment data for merging\n",
    "    # Generate sentiment report\n",
    "    sentiment_df = generate_sentiment_report(location_mentions_df)\n",
    "    \n",
    "    # Pivot to have categories as columns\n",
    "    pivot_df = sentiment_df.pivot_table(\n",
    "        index=['location', 'month'],\n",
    "        columns='category',\n",
    "        values='sentiment_score',\n",
    "        aggfunc='mean'\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    pivot_df.columns.name = None\n",
    "    category_cols = pivot_df.columns[2:]\n",
    "    pivot_df.columns = ['location', 'month'] + [f'sentiment_{col}' for col in category_cols]\n",
    "    \n",
    "    # Clean up location names for better matching\n",
    "    pivot_df['location'] = pivot_df['location'].str.upper()\n",
    "    property_df['town'] = property_df['town'].str.upper()\n",
    "    \n",
    "    # Merge based on town and month\n",
    "    merged_df = pd.merge(\n",
    "        property_df,\n",
    "        pivot_df,\n",
    "        left_on=['month', 'town'],\n",
    "        right_on=['month', 'location'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Drop redundant column\n",
    "    if 'location' in merged_df.columns:\n",
    "        merged_df.drop('location', axis=1, inplace=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    merged_df.to_csv(output_file, index=False)\n",
    "    print(f\"Merged data saved to {output_file}\")\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "def analyze_location_sentiment(location_mentions_df, locations, n_samples=3):\n",
    "    \"\"\"\n",
    "    Analyze sentiment for specific locations in detail and display examples.\n",
    "    \n",
    "    Args:\n",
    "        location_mentions_df (DataFrame): DataFrame with location mentions\n",
    "        locations (list): List of location names to analyze\n",
    "        n_samples (int): Number of examples to show for each sentiment category\n",
    "    \"\"\"\n",
    "    for location in locations:\n",
    "        print(f\"\\n{'='*30} LOCATION: {location.upper()} {'='*30}\")\n",
    "        \n",
    "        # Filter data for this location\n",
    "        location_data = location_mentions_df[location_mentions_df['location'] == location]\n",
    "        \n",
    "        if len(location_data) == 0:\n",
    "            print(f\"No data found for location: {location}\")\n",
    "            continue\n",
    "            \n",
    "        # Get basic stats\n",
    "        avg_sentiment = location_data['sentiment_score'].mean()\n",
    "        mention_count = len(location_data)\n",
    "        category_counts = location_data['category'].value_counts()\n",
    "        \n",
    "        print(f\"Mentions: {mention_count}\")\n",
    "        print(f\"Average Sentiment: {avg_sentiment:.3f}\")\n",
    "        print(\"\\nCategory Distribution:\")\n",
    "        for category, count in category_counts.items():\n",
    "            print(f\"  - {category.capitalize()}: {count} mentions\")\n",
    "        \n",
    "        # Display positive examples\n",
    "        print(\"\\n--- POSITIVE EXAMPLES ---\")\n",
    "        display_sentiment_samples(location_data, n_samples=n_samples, min_score=0.3, max_score=None)\n",
    "        \n",
    "        # Display negative examples\n",
    "        print(\"\\n--- NEGATIVE EXAMPLES ---\")\n",
    "        display_sentiment_samples(location_data, n_samples=n_samples, min_score=None, max_score=-0.3)\n",
    "        \n",
    "        # Display neutral examples\n",
    "        print(\"\\n--- NEUTRAL EXAMPLES ---\")\n",
    "        display_sentiment_samples(location_data, n_samples=n_samples, min_score=-0.2, max_score=0.2)\n",
    "\n",
    "# Main run analysis function\n",
    "def run_analysis(start_year=2017, subreddits=None, top_posts_total=1000, top_comments_per_post=10, save_data=True):\n",
    "    \"\"\"\n",
    "    Run the full Reddit scraping and sentiment analysis workflow.\n",
    "    \n",
    "    Args:\n",
    "        start_year (int): Only include posts from this year onwards\n",
    "        subreddits (list, optional): List of subreddits to scrape\n",
    "        top_posts_total (int): Total number of top posts to analyze\n",
    "        top_comments_per_post (int): Number of top comments to analyze per post\n",
    "        save_data (bool): Whether to save the data to CSV files\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (posts_df, comments_df, location_mentions_df) DataFrames with scraped data\n",
    "    \"\"\"\n",
    "    if subreddits is None:\n",
    "        subreddits = [\"askSingapore\", \"Singapore\", \"SingaporeFI\"]\n",
    "    \n",
    "    try:\n",
    "        # 1. Scrape Reddit data\n",
    "        print(f\"Scraping data from {subreddits} from {start_year} onwards...\")\n",
    "        print(f\"Getting top {top_posts_total} posts total and top {top_comments_per_post} comments per post\")\n",
    "        \n",
    "        posts_df, comments_df, location_mentions_df = scrape_reddit_top_posts(\n",
    "            subreddits, \n",
    "            top_posts_total=top_posts_total,\n",
    "            top_comments_per_post=top_comments_per_post,\n",
    "            start_year=start_year\n",
    "        )\n",
    "        \n",
    "        if save_data and len(posts_df) > 0:\n",
    "            timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            posts_df.to_csv(f'reddit_posts_{timestamp}.csv', index=False)\n",
    "            comments_df.to_csv(f'reddit_comments_{timestamp}.csv', index=False)\n",
    "            location_mentions_df.to_csv(f'location_mentions_{timestamp}.csv', index=False)\n",
    "            print(f\"Data saved with timestamp {timestamp}\")\n",
    "        \n",
    "        # Check if we have any data to analyze\n",
    "        if len(location_mentions_df) == 0:\n",
    "            print(\"\\nNo location mentions found in the data. Cannot proceed with analysis.\")\n",
    "            print(\"Try using different subreddits or adjusting the parameters.\")\n",
    "            return posts_df, comments_df, location_mentions_df\n",
    "            \n",
    "        # 2. Display sample data for quality check\n",
    "        print(\"\\nSampling data for quality check...\")\n",
    "        \n",
    "        # Display sample of most positive sentiments\n",
    "        print(\"\\n=== MOST POSITIVE SENTIMENT SAMPLES ===\")\n",
    "        display_sentiment_samples(location_mentions_df, n_samples=3, min_score=0.5)\n",
    "        \n",
    "        # Display sample of most negative sentiments\n",
    "        print(\"\\n=== MOST NEGATIVE SENTIMENT SAMPLES ===\")\n",
    "        display_sentiment_samples(location_mentions_df, n_samples=3, max_score=-0.5)\n",
    "        \n",
    "        # 3. Analyze popular locations\n",
    "        print(\"\\nAnalyzing popular and unpopular locations...\")\n",
    "        popular, unpopular = get_popular_locations(location_mentions_df, top_n=min(5, len(location_mentions_df['location'].unique())))\n",
    "        \n",
    "        print(\"\\nMost Popular Locations:\")\n",
    "        print(popular)\n",
    "        \n",
    "        print(\"\\nLeast Popular Locations:\")\n",
    "        print(unpopular)\n",
    "        \n",
    "        # 4. Plot sentiment analysis\n",
    "        print(\"\\nGenerating sentiment visualizations...\")\n",
    "        \n",
    "        # Overall sentiment\n",
    "        plot_location_sentiment(location_mentions_df, top_n=min(5, len(location_mentions_df['location'].unique())))\n",
    "        \n",
    "        # By category\n",
    "        for category in ['amenities', 'convenience', 'property', 'general']:\n",
    "            if category in location_mentions_df['category'].unique():\n",
    "                category_data = location_mentions_df[location_mentions_df['category'] == category]\n",
    "                if len(category_data['location'].unique()) >= 2:  # Need at least 2 locations for comparison\n",
    "                    plot_location_sentiment(location_mentions_df, top_n=min(3, len(category_data['location'].unique())), category=category)\n",
    "        \n",
    "        # 5. Plot monthly trends for top locations if we have enough months\n",
    "        if len(location_mentions_df['month'].unique()) > 1:\n",
    "            print(\"\\nGenerating monthly trend visualization...\")\n",
    "            top_locations = popular['location'].tolist()[:min(3, len(popular))]\n",
    "            if len(top_locations) > 0:\n",
    "                plot_monthly_trends(location_mentions_df, locations=top_locations)\n",
    "            else:\n",
    "                print(\"Not enough location data for monthly trend visualization.\")\n",
    "        else:\n",
    "            print(\"\\nNot enough months of data for trend visualization.\")\n",
    "        \n",
    "        # 6. Create heatmap of monthly sentiment if we have enough data\n",
    "        if len(location_mentions_df['month'].unique()) > 1 and len(location_mentions_df['location'].unique()) > 1:\n",
    "            print(\"\\nGenerating sentiment heatmap by month...\")\n",
    "            plot_sentiment_heatmap(location_mentions_df, top_locations=min(5, len(location_mentions_df['location'].unique())))\n",
    "        else:\n",
    "            print(\"\\nNot enough data for sentiment heatmap.\")\n",
    "        \n",
    "        # 7. Analyze top locations in detail\n",
    "        if len(popular) > 0:\n",
    "            top_locations = popular['location'].tolist()[:min(3, len(popular))]\n",
    "            print(f\"\\nAnalyzing top locations in detail: {', '.join(top_locations)}\")\n",
    "            analyze_location_sentiment(location_mentions_df, top_locations)\n",
    "        else:\n",
    "            print(\"\\nNo locations to analyze in detail.\")\n",
    "        \n",
    "        # 8. Plot category distribution if we have enough categories\n",
    "        if len(location_mentions_df['category'].unique()) > 1:\n",
    "            print(\"\\nGenerating category distribution visualization...\")\n",
    "            plot_category_distribution(location_mentions_df)\n",
    "        \n",
    "        # 9. Plot sentiment distribution\n",
    "        print(\"\\nGenerating sentiment distribution...\")\n",
    "        plot_sentiment_distribution(location_mentions_df)\n",
    "        \n",
    "        # 10. Merge with property data if available\n",
    "        try:\n",
    "            print(\"\\nAttempting to merge with property data...\")\n",
    "            merged_df = merge_with_property_data(location_mentions_df, output_file='merged_data_1.csv')\n",
    "            \n",
    "            # Analyze correlations if merge was successful\n",
    "            if merged_df is not None and not merged_df.empty:\n",
    "                print(\"\\nAnalyzing correlations between sentiment and property data...\")\n",
    "                \n",
    "                # Select only numeric columns\n",
    "                numeric_cols = merged_df.select_dtypes(include=[np.number]).columns\n",
    "                \n",
    "                if len(numeric_cols) > 1:  # Need at least 2 numeric columns for correlation\n",
    "                    # Calculate correlations\n",
    "                    corr = merged_df[numeric_cols].corr()\n",
    "                    \n",
    "                    # Plot correlation matrix\n",
    "                    plt.figure(figsize=(14, 12))\n",
    "                    sns.heatmap(corr, annot=True, cmap='coolwarm', center=0, linewidths=0.5)\n",
    "                    plt.title('Correlation Matrix between Property Data and Sentiment Scores')\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                    \n",
    "                    # Plot relationship between sentiment and resale prices\n",
    "                    if 'sentiment_general' in merged_df.columns and 'resale_price' in merged_df.columns:\n",
    "                        plt.figure(figsize=(12, 8))\n",
    "                        sns.scatterplot(x='sentiment_general', y='resale_price', \n",
    "                                       hue='town', data=merged_df, alpha=0.6)\n",
    "                        plt.title('Relationship between General Sentiment and Resale Prices')\n",
    "                        plt.xlabel('General Sentiment Score')\n",
    "                        plt.ylabel('Resale Price')\n",
    "                        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "                        plt.tight_layout()\n",
    "                        plt.show()\n",
    "                else:\n",
    "                    print(\"Not enough numeric columns for correlation analysis.\")\n",
    "            else:\n",
    "                print(\"No property data available or merge unsuccessful.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in property data analysis: {e}\")\n",
    "            \n",
    "        return posts_df, comments_df, location_mentions_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(f\"\\nError in analysis: {e}\")\n",
    "        print(\"\\nTo run this analysis, you'll need to set up Reddit API credentials in a .env file.\")\n",
    "        print(\"Reddit API credentials can be obtained by creating an app at https://www.reddit.com/prefs/apps\")\n",
    "        print(\"Then set the following environment variables:\")\n",
    "        print(\"REDDIT_CLIENT_ID=your_client_id\")\n",
    "        print(\"REDDIT_CLIENT_SECRET=your_client_secret\")\n",
    "        print(\"REDDIT_USER_AGENT='python:sg-sentiment-analysis:v1.0 (by u/your_username)'\")\n",
    "        return None, None, None\n",
    "        \n",
    "        # Display sample of most negative sentiments\n",
    "        print(\"\\n=== MOST NEGATIVE SENTIMENT SAMPLES ===\")\n",
    "        display_sentiment_samples(location_mentions_df, n_samples=3, max_score=-0.5)\n",
    "        \n",
    "        # 3. Analyze popular locations\n",
    "        print(\"\\nAnalyzing popular and unpopular locations...\")\n",
    "        popular, unpopular = get_popular_locations(location_mentions_df, top_n=min(5, len(location_mentions_df['location'].unique())))\n",
    "        \n",
    "        print(\"\\nMost Popular Locations:\")\n",
    "        print(popular)\n",
    "        \n",
    "        print(\"\\nLeast Popular Locations:\")\n",
    "        print(unpopular)\n",
    "        \n",
    "        # 4. Plot sentiment analysis\n",
    "        print(\"\\nGenerating sentiment visualizations...\")\n",
    "        \n",
    "        # Overall sentiment\n",
    "        plot_location_sentiment(location_mentions_df, top_n=min(5, len(location_mentions_df['location'].unique())))\n",
    "        \n",
    "        # By category\n",
    "        for category in ['amenities', 'convenience', 'property', 'general']:\n",
    "            if category in location_mentions_df['category'].unique():\n",
    "                category_data = location_mentions_df[location_mentions_df['category'] == category]\n",
    "                if len(category_data['location'].unique()) >= 2:  # Need at least 2 locations for comparison\n",
    "                    plot_location_sentiment(location_mentions_df, top_n=min(3, len(category_data['location'].unique())), category=category)\n",
    "        \n",
    "        # 5. Plot monthly trends for top locations if we have enough months\n",
    "        if len(location_mentions_df['month'].unique()) > 1:\n",
    "            print(\"\\nGenerating monthly trend visualization...\")\n",
    "            top_locations = popular['location'].tolist()[:min(3, len(popular))]\n",
    "            if len(top_locations) > 0:\n",
    "                plot_monthly_trends(location_mentions_df, locations=top_locations)\n",
    "            else:\n",
    "                print(\"Not enough location data for monthly trend visualization.\")\n",
    "        else:\n",
    "            print(\"\\nNot enough months of data for trend visualization.\")\n",
    "        \n",
    "        # 6. Create heatmap of monthly sentiment if we have enough data\n",
    "        if len(location_mentions_df['month'].unique()) > 1 and len(location_mentions_df['location'].unique()) > 1:\n",
    "            print(\"\\nGenerating sentiment heatmap by month...\")\n",
    "            plot_sentiment_heatmap(location_mentions_df, top_locations=min(5, len(location_mentions_df['location'].unique())))\n",
    "        else:\n",
    "            print(\"\\nNot enough data for sentiment heatmap.\")\n",
    "        \n",
    "        # 7. Analyze top locations in detail\n",
    "        if len(popular) > 0:\n",
    "            top_locations = popular['location'].tolist()[:min(3, len(popular))]\n",
    "            print(f\"\\nAnalyzing top locations in detail: {', '.join(top_locations)}\")\n",
    "            analyze_location_sentiment(location_mentions_df, top_locations)\n",
    "        else:\n",
    "            print(\"\\nNo locations to analyze in detail.\")\n",
    "        \n",
    "        # 8. Plot category distribution if we have enough categories\n",
    "        if len(location_mentions_df['category'].unique()) > 1:\n",
    "            print(\"\\nGenerating category distribution visualization...\")\n",
    "            plot_category_distribution(location_mentions_df)\n",
    "        \n",
    "        # 9. Plot sentiment distribution\n",
    "        print(\"\\nGenerating sentiment distribution...\")\n",
    "        plot_sentiment_distribution(location_mentions_df)\n",
    "        \n",
    "        # 10. Merge with property data if available\n",
    "        try:\n",
    "            print(\"\\nAttempting to merge with property data...\")\n",
    "            merged_df = merge_with_property_data(location_mentions_df, , output_file='merged_data_2.csv')\n",
    "            \n",
    "            # Analyze correlations if merge was successful\n",
    "            if merged_df is not None and not merged_df.empty:\n",
    "                print(\"\\nAnalyzing correlations between sentiment and property data...\")\n",
    "                \n",
    "                # Select only numeric columns\n",
    "                numeric_cols = merged_df.select_dtypes(include=[np.number]).columns\n",
    "                \n",
    "                if len(numeric_cols) > 1:  # Need at least 2 numeric columns for correlation\n",
    "                    # Calculate correlations\n",
    "                    corr = merged_df[numeric_cols].corr()\n",
    "                    \n",
    "                    # Plot correlation matrix\n",
    "                    plt.figure(figsize=(14, 12))\n",
    "                    sns.heatmap(corr, annot=True, cmap='coolwarm', center=0, linewidths=0.5)\n",
    "                    plt.title('Correlation Matrix between Property Data and Sentiment Scores')\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                    \n",
    "                    # Plot relationship between sentiment and resale prices\n",
    "                    if 'sentiment_general' in merged_df.columns and 'resale_price' in merged_df.columns:\n",
    "                        plt.figure(figsize=(12, 8))\n",
    "                        sns.scatterplot(x='sentiment_general', y='resale_price', \n",
    "                                       hue='town', data=merged_df, alpha=0.6)\n",
    "                        plt.title('Relationship between General Sentiment and Resale Prices')\n",
    "                        plt.xlabel('General Sentiment Score')\n",
    "                        plt.ylabel('Resale Price')\n",
    "                        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "                        plt.tight_layout()\n",
    "                        plt.show()\n",
    "                else:\n",
    "                    print(\"Not enough numeric columns for correlation analysis.\")\n",
    "            else:\n",
    "                print(\"No property data available or merge unsuccessful.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in property data analysis: {e}\")\n",
    "            \n",
    "        return posts_df, comments_df, location_mentions_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(f\"\\nError in analysis: {e}\")\n",
    "        print(\"\\nTo run this analysis, you'll need to set up Reddit API credentials in a .env file.\")\n",
    "        print(\"Reddit API credentials can be obtained by creating an app at https://www.reddit.com/prefs/apps\")\n",
    "        print(\"Then set the following environment variables:\")\n",
    "        print(\"REDDIT_CLIENT_ID=your_client_id\")\n",
    "        print(\"REDDIT_CLIENT_SECRET=your_client_secret\")\n",
    "        print(\"REDDIT_USER_AGENT='python:sg-sentiment-analysis:v1.0 (by u/your_username)'\")\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd29e582-0507-46bf-b8fd-958193f25dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Run with custom settings \n",
    "run_analysis(\n",
    "    start_year=2017,\n",
    "    subreddits=[\"askSingapore\", \"Singapore\", \"SingaporeFI\"],\n",
    "    top_posts_total=10000,\n",
    "    top_comments_per_post=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a1d57a-c433-4054-a341-a1e154ccf8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized Reddit scraper that only counts posts with location mentions toward quota\n",
    "def scrape_reddit_location_focused(subreddits, location_posts_target=1000, max_posts_to_check=5000, top_comments_per_post=10, start_year=2017):\n",
    "    \"\"\"\n",
    "    Scrape Reddit posts and comments focusing on posts that mention Singapore locations.\n",
    "    \n",
    "    Args:\n",
    "        subreddits (list): List of subreddit names to scrape\n",
    "        location_posts_target (int): Target number of posts with location mentions to collect\n",
    "        max_posts_to_check (int): Maximum number of posts to check per subreddit before stopping\n",
    "        top_comments_per_post (int): Number of top comments to get per post\n",
    "        start_year (int): Only include posts from this year onwards\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (posts_df, comments_df, location_mentions_df) DataFrames with scraped data\n",
    "    \"\"\"\n",
    "    # Check for Reddit API credentials\n",
    "    client_id = os.getenv(\"REDDIT_CLIENT_ID\")\n",
    "    client_secret = os.getenv(\"REDDIT_CLIENT_SECRET\")\n",
    "    user_agent = os.getenv(\"REDDIT_USER_AGENT\", \"python:sg-location-scraper:v1.0 (by u/your_username)\")\n",
    "    \n",
    "    if not client_id or not client_secret:\n",
    "        raise ValueError(\"Reddit API credentials not found. Set REDDIT_CLIENT_ID and REDDIT_CLIENT_SECRET environment variables.\")\n",
    "    \n",
    "    # Initialize Reddit API\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=client_id,\n",
    "        client_secret=client_secret,\n",
    "        user_agent=user_agent\n",
    "    )\n",
    "    \n",
    "    # Create empty lists to store data\n",
    "    posts_data = []\n",
    "    comments_data = []\n",
    "    location_mentions = []\n",
    "    \n",
    "    # Calculate timestamp for start_year (January 1st of the start_year)\n",
    "    start_timestamp = datetime.datetime(start_year, 1, 1).timestamp()\n",
    "    \n",
    "    # Calculate target posts with location mentions per subreddit\n",
    "    location_posts_per_subreddit = location_posts_target // len(subreddits)\n",
    "    \n",
    "    # Process each subreddit\n",
    "    for subreddit_name in subreddits:\n",
    "        print(f\"Scraping r/{subreddit_name} from {start_year} onwards...\")\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        \n",
    "        # Track posts with location mentions\n",
    "        location_posts_count = 0\n",
    "        total_posts_checked = 0\n",
    "        \n",
    "        # Try different time filters to maximize the chances of getting good data\n",
    "        for time_filter in ['all']:\n",
    "            if location_posts_count >= location_posts_per_subreddit or total_posts_checked >= max_posts_to_check:\n",
    "                break\n",
    "                \n",
    "            print(f\"Getting posts with time filter: {time_filter}\")\n",
    "            \n",
    "            try:\n",
    "                for post in tqdm(subreddit.top(time_filter=time_filter, limit=max_posts_to_check), \n",
    "                               desc=f\"Checking posts from r/{subreddit_name}\"):\n",
    "                    # Increment checked posts counter\n",
    "                    total_posts_checked += 1\n",
    "                    \n",
    "                    # Skip posts before start_year\n",
    "                    if post.created_utc < start_timestamp:\n",
    "                        continue\n",
    "                        \n",
    "                    # Skip posts that we've already processed (by ID)\n",
    "                    if any(p['id'] == post.id for p in posts_data):\n",
    "                        continue\n",
    "                    \n",
    "                    # Convert timestamp to datetime\n",
    "                    post_date = datetime.datetime.fromtimestamp(post.created_utc)\n",
    "                    post_month = post_date.strftime('%Y-%m')\n",
    "                    \n",
    "                    # Process post for location mentions first to see if it's relevant\n",
    "                    full_text = f\"{post.title} {post.selftext}\"\n",
    "                    locations = identify_locations(full_text, sg_locations)\n",
    "                    \n",
    "                    # Create post data dictionary\n",
    "                    post_data = {\n",
    "                        'id': post.id,\n",
    "                        'title': post.title,\n",
    "                        'body': post.selftext,\n",
    "                        'author': str(post.author),\n",
    "                        'created_utc': post.created_utc,\n",
    "                        'score': post.score,\n",
    "                        'subreddit': subreddit_name,\n",
    "                        'url': post.url,\n",
    "                        'month': post_month,\n",
    "                        'date': post_date,\n",
    "                        'has_location': len(locations) > 0\n",
    "                    }\n",
    "                    \n",
    "                    # Always save the post data for reference\n",
    "                    posts_data.append(post_data)\n",
    "                    \n",
    "                    # If this post has locations, process it further and count it toward our target\n",
    "                    if locations:\n",
    "                        location_posts_count += 1\n",
    "                        \n",
    "                        category = categorize_text(full_text)\n",
    "                        sentiment = analyze_sentiment(full_text)\n",
    "                        \n",
    "                        for location in locations:\n",
    "                            location_mention = {\n",
    "                                'source_id': post.id,\n",
    "                                'source_type': 'post',\n",
    "                                'location': location,\n",
    "                                'sentiment_score': sentiment['compound'],\n",
    "                                'pos_score': sentiment['pos'],\n",
    "                                'neg_score': sentiment['neg'],\n",
    "                                'neu_score': sentiment['neu'],\n",
    "                                'category': category,\n",
    "                                'month': post_month,\n",
    "                                'date': post_date,\n",
    "                                'text': full_text[:300] + ('...' if len(full_text) > 300 else '')  # Store sample text\n",
    "                            }\n",
    "                            location_mentions.append(location_mention)\n",
    "                    \n",
    "                        # Get top comments for posts with location mentions\n",
    "                        try:\n",
    "                            post.comments.replace_more(limit=0)  # Don't expand \"more comments\" to avoid API rate limits\n",
    "                            \n",
    "                            # Get all top-level comments and sort by score\n",
    "                            top_level_comments = [comment for comment in post.comments if not isinstance(comment, praw.models.MoreComments)]\n",
    "                            top_level_comments.sort(key=lambda x: x.score, reverse=True)\n",
    "                            \n",
    "                            # Take top N comments\n",
    "                            for comment in top_level_comments[:top_comments_per_post]:\n",
    "                                # Store comment data\n",
    "                                comment_date = datetime.datetime.fromtimestamp(comment.created_utc)\n",
    "                                comment_month = comment_date.strftime('%Y-%m')\n",
    "                                \n",
    "                                # Check for location mentions in the comment\n",
    "                                comment_locations = identify_locations(comment.body, sg_locations)\n",
    "                                \n",
    "                                comment_data = {\n",
    "                                    'id': comment.id,\n",
    "                                    'post_id': post.id,\n",
    "                                    'body': comment.body,\n",
    "                                    'author': str(comment.author),\n",
    "                                    'created_utc': comment.created_utc,\n",
    "                                    'score': comment.score,\n",
    "                                    'month': comment_month,\n",
    "                                    'date': comment_date,\n",
    "                                    'has_location': len(comment_locations) > 0\n",
    "                                }\n",
    "                                comments_data.append(comment_data)\n",
    "                                \n",
    "                                # Process comments that mention locations\n",
    "                                if comment_locations:\n",
    "                                    category = categorize_text(comment.body)\n",
    "                                    sentiment = analyze_sentiment(comment.body)\n",
    "                                    \n",
    "                                    for location in comment_locations:\n",
    "                                        location_mention = {\n",
    "                                            'source_id': comment.id,\n",
    "                                            'source_type': 'comment',\n",
    "                                            'location': location,\n",
    "                                            'sentiment_score': sentiment['compound'],\n",
    "                                            'pos_score': sentiment['pos'],\n",
    "                                            'neg_score': sentiment['neg'],\n",
    "                                            'neu_score': sentiment['neu'],\n",
    "                                            'category': category,\n",
    "                                            'month': comment_month,\n",
    "                                            'date': comment_date,\n",
    "                                            'text': comment.body[:300] + ('...' if len(comment.body) > 300 else '')  # Store sample text\n",
    "                                        }\n",
    "                                        location_mentions.append(location_mention)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing comments for post {post.id}: {e}\")\n",
    "                    \n",
    "                    # Print status update occasionally\n",
    "                    if total_posts_checked % 100 == 0:\n",
    "                        print(f\"Checked {total_posts_checked} posts, found {location_posts_count} with location mentions\")\n",
    "                    \n",
    "                    # Check if we've reached our targets\n",
    "                    if location_posts_count >= location_posts_per_subreddit or total_posts_checked >= max_posts_to_check:\n",
    "                        break\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Error getting posts with time filter '{time_filter}': {e}\")\n",
    "        \n",
    "        # Report final stats for this subreddit\n",
    "        print(f\"Checked {total_posts_checked} posts from r/{subreddit_name}\")\n",
    "        print(f\"Found {location_posts_count} posts with location mentions\")\n",
    "    \n",
    "    # Convert lists to DataFrames\n",
    "    posts_df = pd.DataFrame(posts_data) if posts_data else pd.DataFrame(columns=['id', 'title', 'body', 'author', 'created_utc', 'score', 'subreddit', 'url', 'month', 'date', 'has_location'])\n",
    "    comments_df = pd.DataFrame(comments_data) if comments_data else pd.DataFrame(columns=['id', 'post_id', 'body', 'author', 'created_utc', 'score', 'month', 'date', 'has_location'])\n",
    "    location_mentions_df = pd.DataFrame(location_mentions) if location_mentions else pd.DataFrame(columns=['source_id', 'source_type', 'location', 'sentiment_score', 'pos_score', 'neg_score', 'neu_score', 'category', 'month', 'date', 'text'])\n",
    "    \n",
    "    # Print summary\n",
    "    posts_with_locations = posts_df[posts_df['has_location'] == True].shape[0]\n",
    "    comments_with_locations = comments_df[comments_df['has_location'] == True].shape[0]\n",
    "    \n",
    "    print(f\"\\nSCRAPING SUMMARY:\")\n",
    "    print(f\"Total posts checked: {len(posts_df)}\")\n",
    "    print(f\"Posts with location mentions: {posts_with_locations} ({posts_with_locations/len(posts_df)*100:.1f}%)\")\n",
    "    print(f\"Total comments collected: {len(comments_df)}\")\n",
    "    print(f\"Comments with location mentions: {comments_with_locations} ({comments_with_locations/len(comments_df)*100:.1f}% if comments exist)\")\n",
    "    print(f\"Total location mentions found: {len(location_mentions_df)}\")\n",
    "    \n",
    "    # Location distributions\n",
    "    if len(location_mentions_df) > 0:\n",
    "        location_counts = location_mentions_df['location'].value_counts().head(10)\n",
    "        print(\"\\nTop 10 mentioned locations:\")\n",
    "        for location, count in location_counts.items():\n",
    "            print(f\"  - {location}: {count} mentions\")\n",
    "    \n",
    "    return posts_df, comments_df, location_mentions_df\n",
    "\n",
    "# Update the run_analysis function to use the new scraper\n",
    "def run_location_focused_analysis(start_year=2017, subreddits=None, location_posts_target=1000, max_posts_to_check=5000, top_comments_per_post=10, save_data=True):\n",
    "    \"\"\"\n",
    "    Run the full Reddit scraping and sentiment analysis workflow, focusing on location mentions.\n",
    "    \n",
    "    Args:\n",
    "        start_year (int): Only include posts from this year onwards\n",
    "        subreddits (list, optional): List of subreddit names to scrape\n",
    "        location_posts_target (int): Target number of posts with location mentions to collect\n",
    "        max_posts_to_check (int): Maximum number of posts to check per subreddit before stopping\n",
    "        top_comments_per_post (int): Number of top comments to analyze per post\n",
    "        save_data (bool): Whether to save the data to CSV files\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (posts_df, comments_df, location_mentions_df) DataFrames with scraped data\n",
    "    \"\"\"\n",
    "    if subreddits is None:\n",
    "        subreddits = [\"askSingapore\", \"Singapore\", \"SingaporeFI\"]\n",
    "    \n",
    "    try:\n",
    "        # 1. Scrape Reddit data\n",
    "        print(f\"Scraping data from {subreddits} from {start_year} onwards...\")\n",
    "        print(f\"Targeting {location_posts_target} posts with location mentions (max {max_posts_to_check} posts per subreddit)\")\n",
    "        \n",
    "        posts_df, comments_df, location_mentions_df = scrape_reddit_location_focused(\n",
    "            subreddits, \n",
    "            location_posts_target=location_posts_target,\n",
    "            max_posts_to_check=max_posts_to_check,\n",
    "            top_comments_per_post=top_comments_per_post,\n",
    "            start_year=start_year\n",
    "        )\n",
    "        \n",
    "        if save_data and len(posts_df) > 0:\n",
    "            timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            posts_df.to_csv(f'reddit_posts_{timestamp}.csv', index=False)\n",
    "            comments_df.to_csv(f'reddit_comments_{timestamp}.csv', index=False)\n",
    "            location_mentions_df.to_csv(f'location_mentions_{timestamp}.csv', index=False)\n",
    "            print(f\"Data saved with timestamp {timestamp}\")\n",
    "\n",
    "        # Check if we have any data to analyze\n",
    "        if len(location_mentions_df) == 0:\n",
    "            print(\"\\nNo location mentions found in the data. Cannot proceed with analysis.\")\n",
    "            print(\"Try using different subreddits or adjusting the parameters.\")\n",
    "            return posts_df, comments_df, location_mentions_df\n",
    "            \n",
    "        # 2. Display sample data for quality check\n",
    "        print(\"\\nSampling data for quality check...\")\n",
    "        \n",
    "        # Display sample of most positive sentiments\n",
    "        print(\"\\n=== MOST POSITIVE SENTIMENT SAMPLES ===\")\n",
    "        display_sentiment_samples(location_mentions_df, n_samples=3, min_score=0.5)\n",
    "        \n",
    "        # Display sample of most negative sentiments\n",
    "        print(\"\\n=== MOST NEGATIVE SENTIMENT SAMPLES ===\")\n",
    "        display_sentiment_samples(location_mentions_df, n_samples=3, max_score=-0.5)\n",
    "        \n",
    "        # 3. Analyze popular locations\n",
    "        print(\"\\nAnalyzing popular and unpopular locations...\")\n",
    "        popular, unpopular = get_popular_locations(location_mentions_df, top_n=min(5, len(location_mentions_df['location'].unique())))\n",
    "        \n",
    "        print(\"\\nMost Popular Locations:\")\n",
    "        print(popular)\n",
    "        \n",
    "        print(\"\\nLeast Popular Locations:\")\n",
    "        print(unpopular)\n",
    "        \n",
    "        # 4. Plot sentiment analysis\n",
    "        print(\"\\nGenerating sentiment visualizations...\")\n",
    "        \n",
    "        # Overall sentiment\n",
    "        plot_location_sentiment(location_mentions_df, top_n=min(5, len(location_mentions_df['location'].unique())))\n",
    "        \n",
    "        # By category\n",
    "        for category in ['amenities', 'convenience', 'property', 'general']:\n",
    "            if category in location_mentions_df['category'].unique():\n",
    "                category_data = location_mentions_df[location_mentions_df['category'] == category]\n",
    "                if len(category_data['location'].unique()) >= 2:  # Need at least 2 locations for comparison\n",
    "                    plot_location_sentiment(location_mentions_df, top_n=min(3, len(category_data['location'].unique())), category=category)\n",
    "        \n",
    "        # 5. Plot monthly trends for top locations if we have enough months\n",
    "        if len(location_mentions_df['month'].unique()) > 1:\n",
    "            print(\"\\nGenerating monthly trend visualization...\")\n",
    "            top_locations = popular['location'].tolist()[:min(3, len(popular))]\n",
    "            if len(top_locations) > 0:\n",
    "                plot_monthly_trends(location_mentions_df, locations=top_locations)\n",
    "            else:\n",
    "                print(\"Not enough location data for monthly trend visualization.\")\n",
    "        else:\n",
    "            print(\"\\nNot enough months of data for trend visualization.\")\n",
    "        \n",
    "        # 6. Create heatmap of monthly sentiment if we have enough data\n",
    "        if len(location_mentions_df['month'].unique()) > 1 and len(location_mentions_df['location'].unique()) > 1:\n",
    "            print(\"\\nGenerating sentiment heatmap by month...\")\n",
    "            plot_sentiment_heatmap(location_mentions_df, top_locations=min(5, len(location_mentions_df['location'].unique())))\n",
    "        else:\n",
    "            print(\"\\nNot enough data for sentiment heatmap.\")\n",
    "        \n",
    "        # 7. Analyze top locations in detail\n",
    "        if len(popular) > 0:\n",
    "            top_locations = popular['location'].tolist()[:min(3, len(popular))]\n",
    "            print(f\"\\nAnalyzing top locations in detail: {', '.join(top_locations)}\")\n",
    "            analyze_location_sentiment(location_mentions_df, top_locations)\n",
    "        else:\n",
    "            print(\"\\nNo locations to analyze in detail.\")\n",
    "        \n",
    "        # 8. Plot category distribution if we have enough categories\n",
    "        if len(location_mentions_df['category'].unique()) > 1:\n",
    "            print(\"\\nGenerating category distribution visualization...\")\n",
    "            plot_category_distribution(location_mentions_df)\n",
    "        \n",
    "        # 9. Plot sentiment distribution\n",
    "        print(\"\\nGenerating sentiment distribution...\")\n",
    "        plot_sentiment_distribution(location_mentions_df)\n",
    "        \n",
    "        # 10. Merge with property data if available\n",
    "        try:\n",
    "            print(\"\\nAttempting to merge with property data...\")\n",
    "            merged_df = merge_with_property_data(location_mentions_df, output_file='merged_data_3.csv')\n",
    "            \n",
    "            # Analyze correlations if merge was successful\n",
    "            if merged_df is not None and not merged_df.empty:\n",
    "                print(\"\\nAnalyzing correlations between sentiment and property data...\")\n",
    "                \n",
    "                # Select only numeric columns\n",
    "                numeric_cols = merged_df.select_dtypes(include=[np.number]).columns\n",
    "                \n",
    "                if len(numeric_cols) > 1:  # Need at least 2 numeric columns for correlation\n",
    "                    # Calculate correlations\n",
    "                    corr = merged_df[numeric_cols].corr()\n",
    "                    \n",
    "                    # Plot correlation matrix\n",
    "                    plt.figure(figsize=(14, 12))\n",
    "                    sns.heatmap(corr, annot=True, cmap='coolwarm', center=0, linewidths=0.5)\n",
    "                    plt.title('Correlation Matrix between Property Data and Sentiment Scores')\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                    \n",
    "                    # Plot relationship between sentiment and resale prices\n",
    "                    if 'sentiment_general' in merged_df.columns and 'resale_price' in merged_df.columns:\n",
    "                        plt.figure(figsize=(12, 8))\n",
    "                        sns.scatterplot(x='sentiment_general', y='resale_price', \n",
    "                                       hue='town', data=merged_df, alpha=0.6)\n",
    "                        plt.title('Relationship between General Sentiment and Resale Prices')\n",
    "                        plt.xlabel('General Sentiment Score')\n",
    "                        plt.ylabel('Resale Price')\n",
    "                        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "                        plt.tight_layout()\n",
    "                        plt.show()\n",
    "                else:\n",
    "                    print(\"Not enough numeric columns for correlation analysis.\")\n",
    "            else:\n",
    "                print(\"No property data available or merge unsuccessful.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in property data analysis: {e}\")\n",
    "            \n",
    "        return posts_df, comments_df, location_mentions_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(f\"\\nError in analysis: {e}\")\n",
    "        print(\"\\nTo run this analysis, you'll need to set up Reddit API credentials in a .env file.\")\n",
    "        print(\"Reddit API credentials can be obtained by creating an app at https://www.reddit.com/prefs/apps\")\n",
    "        print(\"Then set the following environment variables:\")\n",
    "        print(\"REDDIT_CLIENT_ID=your_client_id\")\n",
    "        print(\"REDDIT_CLIENT_SECRET=your_client_secret\")\n",
    "        print(\"REDDIT_USER_AGENT='python:sg-sentiment-analysis:v1.0 (by u/your_username)'\")\n",
    "        return None, None, None\n",
    "        \n",
    "        # Display sample of most negative sentiments\n",
    "        print(\"\\n=== MOST NEGATIVE SENTIMENT SAMPLES ===\")\n",
    "        display_sentiment_samples(location_mentions_df, n_samples=3, max_score=-0.5)\n",
    "        \n",
    "        # 3. Analyze popular locations\n",
    "        print(\"\\nAnalyzing popular and unpopular locations...\")\n",
    "        popular, unpopular = get_popular_locations(location_mentions_df, top_n=min(5, len(location_mentions_df['location'].unique())))\n",
    "        \n",
    "        print(\"\\nMost Popular Locations:\")\n",
    "        print(popular)\n",
    "        \n",
    "        print(\"\\nLeast Popular Locations:\")\n",
    "        print(unpopular)\n",
    "        \n",
    "        # 4. Plot sentiment analysis\n",
    "        print(\"\\nGenerating sentiment visualizations...\")\n",
    "        \n",
    "        # Overall sentiment\n",
    "        plot_location_sentiment(location_mentions_df, top_n=min(5, len(location_mentions_df['location'].unique())))\n",
    "        \n",
    "        # By category\n",
    "        for category in ['amenities', 'convenience', 'property', 'general']:\n",
    "            if category in location_mentions_df['category'].unique():\n",
    "                category_data = location_mentions_df[location_mentions_df['category'] == category]\n",
    "                if len(category_data['location'].unique()) >= 2:  # Need at least 2 locations for comparison\n",
    "                    plot_location_sentiment(location_mentions_df, top_n=min(3, len(category_data['location'].unique())), category=category)\n",
    "        \n",
    "        # 5. Plot monthly trends for top locations if we have enough months\n",
    "        if len(location_mentions_df['month'].unique()) > 1:\n",
    "            print(\"\\nGenerating monthly trend visualization...\")\n",
    "            top_locations = popular['location'].tolist()[:min(3, len(popular))]\n",
    "            if len(top_locations) > 0:\n",
    "                plot_monthly_trends(location_mentions_df, locations=top_locations)\n",
    "            else:\n",
    "                print(\"Not enough location data for monthly trend visualization.\")\n",
    "        else:\n",
    "            print(\"\\nNot enough months of data for trend visualization.\")\n",
    "        \n",
    "        # 6. Create heatmap of monthly sentiment if we have enough data\n",
    "        if len(location_mentions_df['month'].unique()) > 1 and len(location_mentions_df['location'].unique()) > 1:\n",
    "            print(\"\\nGenerating sentiment heatmap by month...\")\n",
    "            plot_sentiment_heatmap(location_mentions_df, top_locations=min(5, len(location_mentions_df['location'].unique())))\n",
    "        else:\n",
    "            print(\"\\nNot enough data for sentiment heatmap.\")\n",
    "        \n",
    "        # 7. Analyze top locations in detail\n",
    "        if len(popular) > 0:\n",
    "            top_locations = popular['location'].tolist()[:min(3, len(popular))]\n",
    "            print(f\"\\nAnalyzing top locations in detail: {', '.join(top_locations)}\")\n",
    "            analyze_location_sentiment(location_mentions_df, top_locations)\n",
    "        else:\n",
    "            print(\"\\nNo locations to analyze in detail.\")\n",
    "        \n",
    "        # 8. Plot category distribution if we have enough categories\n",
    "        if len(location_mentions_df['category'].unique()) > 1:\n",
    "            print(\"\\nGenerating category distribution visualization...\")\n",
    "            plot_category_distribution(location_mentions_df)\n",
    "        \n",
    "        # 9. Plot sentiment distribution\n",
    "        print(\"\\nGenerating sentiment distribution...\")\n",
    "        plot_sentiment_distribution(location_mentions_df)\n",
    "        \n",
    "        # 10. Merge with property data if available\n",
    "        try:\n",
    "            print(\"\\nAttempting to merge with property data...\")\n",
    "            merged_df = merge_with_property_data(location_mentions_df, output_file = 'merged_data_3.csv')\n",
    "            \n",
    "            # Analyze correlations if merge was successful\n",
    "            if merged_df is not None and not merged_df.empty:\n",
    "                print(\"\\nAnalyzing correlations between sentiment and property data...\")\n",
    "                \n",
    "                # Select only numeric columns\n",
    "                numeric_cols = merged_df.select_dtypes(include=[np.number]).columns\n",
    "                \n",
    "                if len(numeric_cols) > 1:  # Need at least 2 numeric columns for correlation\n",
    "                    # Calculate correlations\n",
    "                    corr = merged_df[numeric_cols].corr()\n",
    "                    \n",
    "                    # Plot correlation matrix\n",
    "                    plt.figure(figsize=(14, 12))\n",
    "                    sns.heatmap(corr, annot=True, cmap='coolwarm', center=0, linewidths=0.5)\n",
    "                    plt.title('Correlation Matrix between Property Data and Sentiment Scores')\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                    \n",
    "                    # Plot relationship between sentiment and resale prices\n",
    "                    if 'sentiment_general' in merged_df.columns and 'resale_price' in merged_df.columns:\n",
    "                        plt.figure(figsize=(12, 8))\n",
    "                        sns.scatterplot(x='sentiment_general', y='resale_price', \n",
    "                                       hue='town', data=merged_df, alpha=0.6)\n",
    "                        plt.title('Relationship between General Sentiment and Resale Prices')\n",
    "                        plt.xlabel('General Sentiment Score')\n",
    "                        plt.ylabel('Resale Price')\n",
    "                        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "                        plt.tight_layout()\n",
    "                        plt.show()\n",
    "                else:\n",
    "                    print(\"Not enough numeric columns for correlation analysis.\")\n",
    "            else:\n",
    "                print(\"No property data available or merge unsuccessful.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in property data analysis: {e}\")\n",
    "            \n",
    "        return posts_df, comments_df, location_mentions_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(f\"\\nError in analysis: {e}\")\n",
    "        print(\"\\nTo run this analysis, you'll need to set up Reddit API credentials in a .env file.\")\n",
    "        print(\"Reddit API credentials can be obtained by creating an app at https://www.reddit.com/prefs/apps\")\n",
    "        print(\"Then set the following environment variables:\")\n",
    "        print(\"REDDIT_CLIENT_ID=your_client_id\")\n",
    "        print(\"REDDIT_CLIENT_SECRET=your_client_secret\")\n",
    "        print(\"REDDIT_USER_AGENT='python:sg-sentiment-analysis:v1.0 (by u/your_username)'\")\n",
    "        return None, None, None\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the location-focused analysis\n",
    "    run_location_focused_analysis(\n",
    "        start_year=2017,\n",
    "        subreddits=[\"askSingapore\", \"Singapore\", \"SingaporeFI\"],\n",
    "        location_posts_target=10000,  # Target 1000 posts that mention locations\n",
    "        max_posts_to_check=100000,    # Check up to 10,000 posts per subreddit\n",
    "        top_comments_per_post=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd87140-4e57-432e-af31-0bcf2c2131ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed month OHE and other parameters with merged_data_4.csv \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import re\n",
    "\n",
    "def load_and_preprocess_data(filepath, reddit_sentiment_filepath=None):\n",
    "    \"\"\"\n",
    "    Load and preprocess the housing data\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the housing dataset CSV\n",
    "        reddit_sentiment_filepath: Optional path to the Reddit sentiment scores CSV\n",
    "        \n",
    "    Returns:\n",
    "        Preprocessed DataFrame\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Display info about the dataset\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(\"\\nData types:\")\n",
    "    print(df.dtypes)\n",
    "    print(\"\\nMissing values:\")\n",
    "    print(df.isnull().sum())\n",
    "    \n",
    "    # Basic preprocessing\n",
    "    \n",
    "    # Extract numeric value from storey_range (e.g., '01 TO 03' -> 2 as the middle value)\n",
    "    df['storey_median'] = df['storey_range'].apply(\n",
    "        lambda x: np.mean([int(i) for i in re.findall(r'\\d+', x)])\n",
    "    )\n",
    "    \n",
    "    # Calculate remaining lease more precisely (if not already available)\n",
    "    if 'remaining_lease' not in df.columns or df['remaining_lease'].isnull().any():\n",
    "        # Check the format of remaining_lease (if it exists)\n",
    "        if 'remaining_lease' in df.columns:\n",
    "            # Check a sample value to see if it needs conversion\n",
    "            sample = df['remaining_lease'].iloc[0] if not df['remaining_lease'].isnull().all() else None\n",
    "            if sample and isinstance(sample, str) and 'years' in sample:\n",
    "                # Extract numeric value from strings like \"61 years 06 months\"\n",
    "                df['remaining_lease'] = df['remaining_lease'].apply(\n",
    "                    lambda x: float(x.split('years')[0].strip()) if isinstance(x, str) else x\n",
    "                )\n",
    "        else:\n",
    "            current_year = 2025  # Update this as needed\n",
    "            df['remaining_lease'] = df['lease_commence_date'].apply(\n",
    "                lambda x: (x + 99) - current_year if pd.notna(x) else np.nan\n",
    "            )\n",
    "    \n",
    "    # Convert sentiment columns if they exist\n",
    "    sentiment_cols = [col for col in df.columns if 'sentiment' in col]\n",
    "    for col in sentiment_cols:\n",
    "        if col in df.columns:\n",
    "            # Ensure sentiment scores are numeric\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Create proximity features from MRT data if available\n",
    "    if 'mrt_distance_km' in df.columns:\n",
    "        df['mrt_accessibility'] = 1 / (1 + df['mrt_distance_km'])  # Higher value means closer to MRT\n",
    "    \n",
    "    # Normalize walking time with distance to create a walking convenience score\n",
    "    if 'walking_time_min' in df.columns and 'walking_distance_m' in df.columns:\n",
    "        # Lower values are better (less time per distance)\n",
    "        df['walking_convenience'] = df['walking_distance_m'] / (df['walking_time_min'] * 60)\n",
    "        \n",
    "    # Age of the flat might be important\n",
    "    df['flat_age'] = 2025 - df['lease_commence_date']  # Update the year as needed\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_features(df, target='resale_price'):\n",
    "    \"\"\"\n",
    "    Prepare features for modeling\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        target: Target variable name\n",
    "        \n",
    "    Returns:\n",
    "        X, y, and feature information\n",
    "    \"\"\"\n",
    "    # Separate features and target\n",
    "    y = df[target]\n",
    "    X = df.drop(target, axis=1)\n",
    "    \n",
    "    # Identify categorical and numerical features\n",
    "    categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "    numerical_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    \n",
    "    # Remove any ID columns or other non-predictive features\n",
    "    features_to_drop = ['street_name', 'storey_range', 'month', 'flat_type', 'block']  # Add others as needed\n",
    "    for feature in features_to_drop:\n",
    "        if feature in numerical_features:\n",
    "            numerical_features.remove(feature)\n",
    "        if feature in categorical_features:\n",
    "            categorical_features.remove(feature)\n",
    "    \n",
    "    # Make sure longitude and latitude are in numerical features if they exist\n",
    "    for feature in ['longitude', 'latitude']:\n",
    "        if feature in X.columns and feature not in numerical_features:\n",
    "            numerical_features.append(feature)\n",
    "    \n",
    "    # Drop the features from X\n",
    "    X = X.drop(features_to_drop, axis=1)\n",
    "    \n",
    "    # Update categorical and numerical features lists\n",
    "    categorical_features = [f for f in categorical_features if f in X.columns]\n",
    "    numerical_features = [f for f in numerical_features if f in X.columns]\n",
    "    \n",
    "    return X, y, categorical_features, numerical_features\n",
    "\n",
    "def create_preprocessing_pipeline(categorical_features, numerical_features, apply_pca=False, pca_components=0.95):\n",
    "    \"\"\"\n",
    "    Create a preprocessing pipeline with optional PCA\n",
    "    \n",
    "    Args:\n",
    "        categorical_features: List of categorical feature names\n",
    "        numerical_features: List of numerical feature names\n",
    "        apply_pca: Whether to apply PCA for dimensionality reduction\n",
    "        pca_components: Number of components or variance ratio to keep\n",
    "        \n",
    "    Returns:\n",
    "        Scikit-learn preprocessing pipeline\n",
    "    \"\"\"\n",
    "    # Use ordinal encoding for high-cardinality features (more than 10 categories)\n",
    "    high_cardinality_features = []\n",
    "    low_cardinality_features = []\n",
    "    \n",
    "    for feature in categorical_features:\n",
    "        # This will be computed at runtime based on actual data\n",
    "        # We're just initializing the lists here\n",
    "        high_cardinality_features.append(feature)\n",
    "    \n",
    "    # Create lists for the transformers\n",
    "    transformers = []\n",
    "    \n",
    "    # Categorical features processing - auto determine encoding based on cardinality\n",
    "    if categorical_features:\n",
    "        # We'll need to check cardinality for each feature\n",
    "        # One-hot encoding for low-cardinality features\n",
    "        if low_cardinality_features:\n",
    "            transformers.append(\n",
    "                ('ohe', Pipeline([\n",
    "                    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                    ('encoder', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
    "                ]), low_cardinality_features)\n",
    "            )\n",
    "    \n",
    "        # Ordinal encoding for high-cardinality features\n",
    "        if high_cardinality_features:\n",
    "            transformers.append(\n",
    "                ('ordinal', Pipeline([\n",
    "                    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                    ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "                ]), high_cardinality_features)\n",
    "            )\n",
    "    \n",
    "    # Standard scaling for numerical features\n",
    "    if numerical_features:\n",
    "        num_pipeline = [\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]\n",
    "        \n",
    "        # Add PCA if requested\n",
    "        if apply_pca:\n",
    "            num_pipeline.append(('pca', PCA(n_components=pca_components)))\n",
    "            \n",
    "        transformers.append(\n",
    "            ('num', Pipeline(num_pipeline), numerical_features)\n",
    "        )\n",
    "    \n",
    "    # Create the column transformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=transformers,\n",
    "        remainder='drop'  # Drop any columns not specified\n",
    "    )\n",
    "    \n",
    "    return preprocessor\n",
    "\n",
    "def train_linear_regression(X, y, categorical_features, numerical_features, do_hyper_tuning=True, apply_pca=True):\n",
    "    \"\"\"\n",
    "    Train a multivariate linear regression model with hyperparameter tuning\n",
    "    \n",
    "    Args:\n",
    "        X: Feature DataFrame\n",
    "        y: Target series\n",
    "        categorical_features: List of categorical feature names\n",
    "        numerical_features: List of numerical feature names\n",
    "        do_hyper_tuning: Whether to perform hyperparameter tuning\n",
    "        apply_pca: Whether to apply PCA for dimensionality reduction\n",
    "        \n",
    "    Returns:\n",
    "        Trained model, preprocessor, and metrics\n",
    "    \"\"\"\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create preprocessing pipeline\n",
    "    preprocessor = create_preprocessing_pipeline(\n",
    "        categorical_features, \n",
    "        numerical_features,\n",
    "        apply_pca=apply_pca\n",
    "    )\n",
    "    \n",
    "    if do_hyper_tuning:\n",
    "        # Create pipeline with multiple regression models\n",
    "        param_grid = [\n",
    "            {\n",
    "                'preprocessor__num__pca__n_components': [0.85, 0.9, 0.95] if apply_pca else [None],\n",
    "                'regressor': [LinearRegression()],\n",
    "                'regressor__fit_intercept': [True, False]\n",
    "            },\n",
    "            {\n",
    "                'preprocessor__num__pca__n_components': [0.85, 0.9, 0.95] if apply_pca else [None],\n",
    "                'regressor': [Ridge()],\n",
    "                'regressor__alpha': [0.01, 0.1, 1.0, 10.0],\n",
    "                'regressor__fit_intercept': [True, False]\n",
    "            },\n",
    "            {\n",
    "                'preprocessor__num__pca__n_components': [0.85, 0.9, 0.95] if apply_pca else [None],\n",
    "                'regressor': [Lasso()],\n",
    "                'regressor__alpha': [0.001, 0.01, 0.1, 1.0],\n",
    "                'regressor__fit_intercept': [True, False]\n",
    "            },\n",
    "            {\n",
    "                'preprocessor__num__pca__n_components': [0.85, 0.9, 0.95] if apply_pca else [None],\n",
    "                'regressor': [ElasticNet()],\n",
    "                'regressor__alpha': [0.001, 0.01, 0.1, 1.0],\n",
    "                'regressor__l1_ratio': [0.1, 0.5, 0.7, 0.9],\n",
    "                'regressor__fit_intercept': [True, False]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Create pipeline\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', LinearRegression())\n",
    "        ])\n",
    "        \n",
    "        # Use RandomizedSearchCV for more efficient hyperparameter search\n",
    "        grid_search = RandomizedSearchCV(\n",
    "            pipeline,\n",
    "            param_grid,\n",
    "            cv=5,\n",
    "            scoring='neg_mean_squared_error',\n",
    "            n_iter=15,  # Number of parameter settings sampled\n",
    "            n_jobs=-1,\n",
    "            verbose=1,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        print(\"\\nPerforming hyperparameter tuning for Linear Model...\")\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        best_model = grid_search.best_estimator_\n",
    "        print(f\"Best model: {grid_search.best_params_}\")\n",
    "        print(f\"Best cross-validation score: {-grid_search.best_score_:.2f} MSE\")\n",
    "        \n",
    "    else:\n",
    "        # Use standard Linear Regression without tuning\n",
    "        best_model = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', LinearRegression())\n",
    "        ])\n",
    "        best_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    metrics = {\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2\n",
    "    }\n",
    "    \n",
    "    print(\"\\nLinear Regression Metrics:\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"R2 Score: {r2:.4f}\")\n",
    "    \n",
    "    # Visualize actual vs predicted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')\n",
    "    plt.xlabel('Actual Prices')\n",
    "    plt.ylabel('Predicted Prices')\n",
    "    plt.title('Linear Regression: Actual vs Predicted Prices')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('linear_regression_results.png')\n",
    "    \n",
    "    return best_model, preprocessor, metrics\n",
    "\n",
    "def train_xgboost(X, y, categorical_features, numerical_features, do_hyper_tuning=True, apply_pca=False):\n",
    "    \"\"\"\n",
    "    Train an XGBoost gradient boosting model with hyperparameter tuning\n",
    "    \n",
    "    Args:\n",
    "        X: Feature DataFrame\n",
    "        y: Target series\n",
    "        categorical_features: List of categorical feature names\n",
    "        numerical_features: List of numerical feature names\n",
    "        do_hyper_tuning: Whether to perform hyperparameter tuning\n",
    "        apply_pca: Whether to apply PCA for dimensionality reduction\n",
    "        \n",
    "    Returns:\n",
    "        Trained model, preprocessor, and metrics\n",
    "    \"\"\"\n",
    "    # Create preprocessing pipeline\n",
    "    preprocessor = create_preprocessing_pipeline(\n",
    "        categorical_features, \n",
    "        numerical_features,\n",
    "        apply_pca=apply_pca\n",
    "    )\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Preprocess the data\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "    \n",
    "    if do_hyper_tuning:\n",
    "        # Define parameter grid for hyperparameter tuning\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [3, 5, 7, 9],\n",
    "            'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "            'subsample': [0.6, 0.8, 1.0],\n",
    "            'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "            'min_child_weight': [1, 3, 5],\n",
    "            'gamma': [0, 0.1, 0.2],\n",
    "            'reg_alpha': [0, 0.1, 1],\n",
    "            'reg_lambda': [1, 1.5, 2]\n",
    "        }\n",
    "        \n",
    "        # Use RandomizedSearchCV for more efficient hyperparameter search\n",
    "        random_search = RandomizedSearchCV(\n",
    "            xgb.XGBRegressor(\n",
    "                objective='reg:squarederror',\n",
    "                random_state=42\n",
    "            ),\n",
    "            param_distributions=param_grid,\n",
    "            n_iter=20,  # Number of parameter settings sampled\n",
    "            scoring='neg_mean_squared_error',\n",
    "            cv=5,\n",
    "            verbose=1,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        print(\"\\nPerforming hyperparameter tuning for XGBoost...\")\n",
    "        random_search.fit(X_train_processed, y_train)\n",
    "        \n",
    "        # Get best parameters\n",
    "        best_params = random_search.best_params_\n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "        print(f\"Best cross-validation score: {-random_search.best_score_:.2f} MSE\")\n",
    "        \n",
    "        # Create and train the XGBoost model with best parameters\n",
    "        xgb_model = xgb.XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            random_state=42,\n",
    "            **best_params\n",
    "        )\n",
    "    else:\n",
    "        # Define XGBoost parameters\n",
    "        xgb_params = {\n",
    "            'n_estimators': 200,\n",
    "            'max_depth': 6,\n",
    "            'learning_rate': 0.1,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'objective': 'reg:squarederror',\n",
    "            'random_state': 42\n",
    "        }\n",
    "        \n",
    "        # Create the XGBoost model\n",
    "        xgb_model = xgb.XGBRegressor(**xgb_params)\n",
    "    \n",
    "    # Train the model for XGBoost 2.1.4\n",
    "    eval_set = [(X_test_processed, y_test)]\n",
    "    \n",
    "    try:\n",
    "        # For XGBoost 2.1.4, we need to use callbacks instead of direct parameters\n",
    "        from xgboost.callback import EarlyStopping\n",
    "        \n",
    "        early_stopping = EarlyStopping(\n",
    "            rounds=20,\n",
    "            metric_name='rmse',\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        xgb_model.fit(\n",
    "            X_train_processed, y_train,\n",
    "            eval_set=eval_set,\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=True\n",
    "        )\n",
    "    except (ImportError, TypeError) as e:\n",
    "        # Fall back to basic method if needed\n",
    "        print(f\"Using basic XGBoost training method due to: {e}\")\n",
    "        xgb_model.fit(X_train_processed, y_train)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred = xgb_model.predict(X_test_processed)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    metrics = {\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2\n",
    "    }\n",
    "    \n",
    "    print(\"\\nXGBoost Metrics:\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"R2 Score: {r2:.4f}\")\n",
    "    \n",
    "    # Visualize actual vs predicted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')\n",
    "    plt.xlabel('Actual Prices')\n",
    "    plt.ylabel('Predicted Prices')\n",
    "    plt.title('XGBoost: Actual vs Predicted Prices')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('xgboost_results.png')\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    xgb.plot_importance(xgb_model, max_num_features=20)\n",
    "    plt.title('XGBoost Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('xgboost_feature_importance.png')\n",
    "    \n",
    "    return xgb_model, preprocessor, metrics\n",
    "\n",
    "def create_dnn_model(input_dim, config):\n",
    "    \"\"\"\n",
    "    Create a DNN model based on configuration parameters\n",
    "    \n",
    "    Args:\n",
    "        input_dim: Number of input features\n",
    "        config: Dictionary with model configuration\n",
    "        \n",
    "    Returns:\n",
    "        Compiled Keras model\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First hidden layer\n",
    "    model.add(Dense(\n",
    "        config.get('first_layer_units', 128), \n",
    "        activation=config.get('activation', 'relu'), \n",
    "        input_dim=input_dim\n",
    "    ))\n",
    "    \n",
    "    if config.get('use_batch_norm', True):\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "    if config.get('dropout_rate', 0.3) > 0:\n",
    "        model.add(Dropout(config.get('dropout_rate', 0.3)))\n",
    "    \n",
    "    # Hidden layers\n",
    "    for units in config.get('hidden_layers', [64, 32]):\n",
    "        model.add(Dense(units, activation=config.get('activation', 'relu')))\n",
    "        \n",
    "        if config.get('use_batch_norm', True):\n",
    "            model.add(BatchNormalization())\n",
    "            \n",
    "        if config.get('dropout_rate', 0.3) > 0:\n",
    "            model.add(Dropout(config.get('dropout_rate', 0.3) * 0.7))  # Reduce dropout in deeper layers\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=config.get('optimizer', 'adam'),\n",
    "        loss=config.get('loss', 'mse'),\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def tune_dnn_hyperparameters(X_train, y_train, input_dim):\n",
    "    \"\"\"\n",
    "    Tune hyperparameters for the DNN model\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        y_train: Training target\n",
    "        input_dim: Number of input features\n",
    "        \n",
    "    Returns:\n",
    "        Best hyperparameter configuration\n",
    "    \"\"\"\n",
    "    # Define hyperparameter configurations to try\n",
    "    configs = [\n",
    "        # Configuration 1\n",
    "        {\n",
    "            'first_layer_units': 128,\n",
    "            'hidden_layers': [64, 32],\n",
    "            'activation': 'relu',\n",
    "            'optimizer': 'adam',\n",
    "            'learning_rate': 0.001,\n",
    "            'batch_size': 32,\n",
    "            'dropout_rate': 0.3,\n",
    "            'use_batch_norm': True\n",
    "        },\n",
    "        # Configuration 2\n",
    "        {\n",
    "            'first_layer_units': 256,\n",
    "            'hidden_layers': [128, 64],\n",
    "            'activation': 'relu',\n",
    "            'optimizer': 'adam',\n",
    "            'learning_rate': 0.001,\n",
    "            'batch_size': 64,\n",
    "            'dropout_rate': 0.4,\n",
    "            'use_batch_norm': True\n",
    "        },\n",
    "        # Configuration 3\n",
    "        {\n",
    "            'first_layer_units': 128,\n",
    "            'hidden_layers': [128, 64, 32],\n",
    "            'activation': 'elu',\n",
    "            'optimizer': 'adam',\n",
    "            'learning_rate': 0.0005,\n",
    "            'batch_size': 32,\n",
    "            'dropout_rate': 0.3,\n",
    "            'use_batch_norm': True\n",
    "        },\n",
    "        # Configuration 4\n",
    "        {\n",
    "            'first_layer_units': 64,\n",
    "            'hidden_layers': [32, 16],\n",
    "            'activation': 'relu',\n",
    "            'optimizer': 'adam',\n",
    "            'learning_rate': 0.001,\n",
    "            'batch_size': 16,\n",
    "            'dropout_rate': 0.2,\n",
    "            'use_batch_norm': True\n",
    "        },\n",
    "        # Configuration 5\n",
    "        {\n",
    "            'first_layer_units': 128,\n",
    "            'hidden_layers': [64],\n",
    "            'activation': 'relu',\n",
    "            'optimizer': 'adam',\n",
    "            'learning_rate': 0.001,\n",
    "            'batch_size': 32,\n",
    "            'dropout_rate': 0.3,\n",
    "            'use_batch_norm': False\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Validation split\n",
    "    validation_split = 0.2\n",
    "    val_size = int(len(X_train) * validation_split)\n",
    "    X_train_val, X_val = X_train[:-val_size], X_train[-val_size:]\n",
    "    y_train_val, y_val = y_train[:-val_size], y_train[-val_size:]\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_config = None\n",
    "    \n",
    "    # Define early stopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\nPerforming hyperparameter tuning for DNN...\")\n",
    "    for i, config in enumerate(configs):\n",
    "        print(f\"\\nTrying configuration {i+1}/{len(configs)}\")\n",
    "        \n",
    "        # Create model\n",
    "        model = create_dnn_model(input_dim, config)\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            X_train_val, y_train_val,\n",
    "            epochs=100,\n",
    "            batch_size=config['batch_size'],\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        val_loss = min(history.history['val_loss'])\n",
    "        print(f\"Configuration {i+1} validation loss: {val_loss:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_config = config\n",
    "            print(f\"New best configuration found!\")\n",
    "    \n",
    "    print(f\"\\nBest DNN configuration: {best_config}\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    return best_config\n",
    "\n",
    "def train_dnn(X, y, categorical_features, numerical_features, do_hyper_tuning=True, apply_pca=False):\n",
    "    \"\"\"\n",
    "    Train a Deep Neural Network model with hyperparameter tuning\n",
    "    \n",
    "    Args:\n",
    "        X: Feature DataFrame\n",
    "        y: Target series\n",
    "        categorical_features: List of categorical feature names\n",
    "        numerical_features: List of numerical feature names\n",
    "        do_hyper_tuning: Whether to perform hyperparameter tuning\n",
    "        apply_pca: Whether to apply PCA for dimensionality reduction\n",
    "        \n",
    "    Returns:\n",
    "        Trained model, preprocessor, history, and metrics\n",
    "    \"\"\"\n",
    "    # Create preprocessing pipeline\n",
    "    preprocessor = create_preprocessing_pipeline(\n",
    "        categorical_features, \n",
    "        numerical_features,\n",
    "        apply_pca=apply_pca\n",
    "    )\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Preprocess the data\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "    \n",
    "    # Standardize the target variable for better neural network training\n",
    "    y_scaler = StandardScaler()\n",
    "    y_train_scaled = y_scaler.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
    "    y_test_scaled = y_scaler.transform(y_test.values.reshape(-1, 1)).ravel()\n",
    "    \n",
    "    # Get the number of features after preprocessing\n",
    "    input_dim = X_train_processed.shape[1]\n",
    "    \n",
    "    if do_hyper_tuning:\n",
    "        # Tune hyperparameters\n",
    "        best_config = tune_dnn_hyperparameters(X_train_processed, y_train_scaled, input_dim)\n",
    "        \n",
    "        # Create and compile the model with best configuration\n",
    "        dnn_model = create_dnn_model(input_dim, best_config)\n",
    "        batch_size = best_config['batch_size']\n",
    "    else:\n",
    "        # Default configuration\n",
    "        default_config = {\n",
    "            'first_layer_units': 128,\n",
    "            'hidden_layers': [64, 32],\n",
    "            'activation': 'relu',\n",
    "            'optimizer': 'adam',\n",
    "            'batch_size': 32,\n",
    "            'dropout_rate': 0.3,\n",
    "            'use_batch_norm': True\n",
    "        }\n",
    "        \n",
    "        # Create and compile the model with default configuration\n",
    "        dnn_model = create_dnn_model(input_dim, default_config)\n",
    "        batch_size = default_config['batch_size']\n",
    "    \n",
    "    # Define early stopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=20,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = dnn_model.fit(\n",
    "        X_train_processed, y_train_scaled,\n",
    "        epochs=200,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_scaled = dnn_model.predict(X_test_processed)\n",
    "    \n",
    "    # Inverse transform to get actual price predictions\n",
    "    y_pred = y_scaler.inverse_transform(y_pred_scaled).ravel()\n",
    "    \n",
    "    # Evaluate the model\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    metrics = {\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2\n",
    "    }\n",
    "    \n",
    "    print(\"\\nDNN Metrics:\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"R2 Score: {r2:.4f}\")\n",
    "    \n",
    "    # Plot training & validation loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('DNN Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('dnn_training_history.png')\n",
    "    \n",
    "    # Visualize actual vs predicted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')\n",
    "    plt.xlabel('Actual Prices')\n",
    "    plt.ylabel('Predicted Prices')\n",
    "    plt.title('DNN: Actual vs Predicted Prices')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('dnn_results.png')\n",
    "    \n",
    "    return dnn_model, preprocessor, history, metrics\n",
    "\n",
    "def predict_with_sentiment(model, preprocessor, X, town_data):\n",
    "    \"\"\"\n",
    "    Make predictions with a trained model, using sentiment data that's already in X\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        preprocessor: Fitted preprocessor\n",
    "        X: Feature DataFrame (with sentiment columns)\n",
    "        town_data: Town names for each instance\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with predictions\n",
    "    \"\"\"\n",
    "    # Create a copy of X to avoid modifying the original\n",
    "    X_pred = X.copy()\n",
    "    \n",
    "    # Fill missing sentiment scores with the mean (if any)\n",
    "    sentiment_cols = [col for col in X_pred.columns if 'sentiment' in col]\n",
    "    for col in sentiment_cols:\n",
    "        if col in X_pred.columns and X_pred[col].isnull().any():\n",
    "            mean_value = X_pred[col].mean()\n",
    "            X_pred[col].fillna(mean_value, inplace=True)\n",
    "    \n",
    "    # Preprocess the data\n",
    "    X_processed = preprocessor.transform(X_pred)\n",
    "    \n",
    "    # Make predictions\n",
    "    if hasattr(model, 'predict'):  # sklearn, xgboost\n",
    "        predictions = model.predict(X_processed)\n",
    "    else:  # keras (TensorFlow)\n",
    "        predictions = model.predict(X_processed).flatten()\n",
    "    \n",
    "    # Create a DataFrame with predictions\n",
    "    results = pd.DataFrame({\n",
    "        'town': town_data,\n",
    "        'predicted_resale_price': predictions\n",
    "    })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def compare_models(metrics_dict):\n",
    "    \"\"\"\n",
    "    Compare the performance of different models\n",
    "    \n",
    "    Args:\n",
    "        metrics_dict: Dictionary with model names as keys and their metrics as values\n",
    "    \"\"\"\n",
    "    models = list(metrics_dict.keys())\n",
    "    rmse_values = [metrics_dict[model]['rmse'] for model in models]\n",
    "    mae_values = [metrics_dict[model]['mae'] for model in models]\n",
    "    r2_values = [metrics_dict[model]['r2'] for model in models]\n",
    "    \n",
    "    # Plot RMSE comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.bar(models, rmse_values)\n",
    "    plt.title('RMSE Comparison')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.bar(models, mae_values)\n",
    "    plt.title('MAE Comparison')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.bar(models, r2_values)\n",
    "    plt.title('R Score Comparison')\n",
    "    plt.ylabel('R')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_comparison.png')\n",
    "    \n",
    "    # Print comparison table\n",
    "    print(\"\\nModel Comparison:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Model':<15} {'RMSE':>10} {'MAE':>10} {'R':>10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for model in models:\n",
    "        print(f\"{model:<15} {metrics_dict[model]['rmse']:>10.2f} {metrics_dict[model]['mae']:>10.2f} {metrics_dict[model]['r2']:>10.4f}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the entire modeling pipeline\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    df = load_and_preprocess_data('merged_data_4.csv')\n",
    "    \n",
    "    # Check for sentiment data\n",
    "    sentiment_cols = [col for col in df.columns if 'sentiment' in col]\n",
    "    sentiment_available = len(sentiment_cols) > 0\n",
    "    \n",
    "    if not sentiment_available:\n",
    "        print(\"No sentiment columns found in the dataset. Some analyses will be skipped.\")\n",
    "    else:\n",
    "        print(f\"Found sentiment columns: {sentiment_cols}\")\n",
    "    \n",
    "    # Prepare features\n",
    "    X, y, categorical_features, numerical_features = prepare_features(df)\n",
    "    \n",
    "    # Set hyperparameter tuning and PCA flags\n",
    "    do_hyper_tuning = True\n",
    "    apply_pca_linear = True      # PCA often helps linear models\n",
    "    apply_pca_xgboost = False    # Tree-based models like XGBoost can handle high-dimensionality\n",
    "    apply_pca_dnn = False        # DNNs can learn representations from raw features\n",
    "    \n",
    "    print(\"\\n========== Training Linear Regression Model ==========\")\n",
    "    linear_model, linear_preprocessor, linear_metrics = train_linear_regression(\n",
    "        X, y, categorical_features, numerical_features, \n",
    "        do_hyper_tuning=do_hyper_tuning, \n",
    "        apply_pca=apply_pca_linear\n",
    "    )\n",
    "    \n",
    "    print(\"\\n========== Training XGBoost Model ==========\")\n",
    "    xgb_model, xgb_preprocessor, xgb_metrics = train_xgboost(\n",
    "        X, y, categorical_features, numerical_features, \n",
    "        do_hyper_tuning=do_hyper_tuning, \n",
    "        apply_pca=apply_pca_xgboost\n",
    "    )\n",
    "    \n",
    "    print(\"\\n========== Training DNN Model ==========\")\n",
    "    dnn_model, dnn_preprocessor, dnn_history, dnn_metrics = train_dnn(\n",
    "        X, y, categorical_features, numerical_features, \n",
    "        do_hyper_tuning=do_hyper_tuning, \n",
    "        apply_pca=apply_pca_dnn\n",
    "    )\n",
    "    \n",
    "    # Compare models\n",
    "    all_metrics = {\n",
    "        'Linear Regression': linear_metrics,\n",
    "        'XGBoost': xgb_metrics,\n",
    "        'DNN': dnn_metrics\n",
    "    }\n",
    "    \n",
    "    compare_models(all_metrics)\n",
    "    \n",
    "    # Get best model based on R score\n",
    "    best_model_name = max(all_metrics, key=lambda x: all_metrics[x]['r2'])\n",
    "    print(f\"\\nBest model based on R score: {best_model_name}\")\n",
    "    \n",
    "    # Save the best model for future use\n",
    "    if best_model_name == 'Linear Regression':\n",
    "        best_model = linear_model\n",
    "        best_preprocessor = linear_preprocessor\n",
    "    elif best_model_name == 'XGBoost':\n",
    "        best_model = xgb_model\n",
    "        best_preprocessor = xgb_preprocessor\n",
    "    else:  # DNN\n",
    "        best_model = dnn_model\n",
    "        best_preprocessor = dnn_preprocessor\n",
    "    \n",
    "    # If sentiment data is available, show example of prediction and analyze sentiment impact\n",
    "    if sentiment_available:\n",
    "        print(\"\\nExample of prediction with sentiment data:\")\n",
    "        # Create a sample DataFrame for prediction\n",
    "        sample_X = X.loc[X.index[:5]].copy()\n",
    "        \n",
    "        # Make predictions\n",
    "        sample_predictions = predict_with_sentiment(\n",
    "            best_model, best_preprocessor, sample_X, df['town'].iloc[:5]\n",
    "        )\n",
    "        \n",
    "        print(sample_predictions)\n",
    "        \n",
    "        # Analyze sentiment impact\n",
    "        print(\"\\nExploring impact of sentiment on predictions:\")\n",
    "        # Create copies with modified sentiment values\n",
    "        high_sentiment_X = sample_X.copy()\n",
    "        low_sentiment_X = sample_X.copy()\n",
    "        \n",
    "        # Modify sentiment values (increase/decrease by 0.2)\n",
    "        for col in sentiment_cols:\n",
    "            if col in high_sentiment_X.columns:\n",
    "                high_sentiment_X[col] = high_sentiment_X[col].apply(lambda x: min(x + 0.2, 1.0))\n",
    "                low_sentiment_X[col] = low_sentiment_X[col].apply(lambda x: max(x - 0.2, -1.0))\n",
    "        \n",
    "        # Make predictions with modified sentiment\n",
    "        high_predictions = predict_with_sentiment(\n",
    "            best_model, best_preprocessor, high_sentiment_X, df['town'].iloc[:5]\n",
    "        )\n",
    "        \n",
    "        low_predictions = predict_with_sentiment(\n",
    "            best_model, best_preprocessor, low_sentiment_X, df['town'].iloc[:5]\n",
    "        )\n",
    "        \n",
    "        # Combine results to show sentiment impact\n",
    "        sentiment_impact = pd.DataFrame({\n",
    "            'town': df['town'].iloc[:5],\n",
    "            'original_price': sample_predictions['predicted_resale_price'],\n",
    "            'high_sentiment_price': high_predictions['predicted_resale_price'],\n",
    "            'low_sentiment_price': low_predictions['predicted_resale_price']\n",
    "        })\n",
    "        \n",
    "        sentiment_impact['high_sentiment_diff_pct'] = (\n",
    "            (sentiment_impact['high_sentiment_price'] - sentiment_impact['original_price']) / \n",
    "            sentiment_impact['original_price'] * 100\n",
    "        ).round(2)\n",
    "        \n",
    "        sentiment_impact['low_sentiment_diff_pct'] = (\n",
    "            (sentiment_impact['low_sentiment_price'] - sentiment_impact['original_price']) / \n",
    "            sentiment_impact['original_price'] * 100\n",
    "        ).round(2)\n",
    "        \n",
    "        print(\"\\nImpact of Sentiment Analysis on Predictions:\")\n",
    "        print(sentiment_impact[['town', 'high_sentiment_diff_pct', 'low_sentiment_diff_pct']])\n",
    "        \n",
    "        # Visualize sentiment impact\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Create positions for bars\n",
    "        towns = sentiment_impact['town'].tolist()\n",
    "        x = np.arange(len(towns))\n",
    "        width = 0.25\n",
    "        \n",
    "        # Create bars\n",
    "        plt.bar(x - width, sentiment_impact['low_sentiment_diff_pct'], width, label='Negative Sentiment Impact')\n",
    "        plt.bar(x + width, sentiment_impact['high_sentiment_diff_pct'], width, label='Positive Sentiment Impact')\n",
    "        \n",
    "        # Customize chart\n",
    "        plt.xlabel('Town')\n",
    "        plt.ylabel('Price Change (%)')\n",
    "        plt.title('Impact of Sentiment Analysis on Predicted Resale Prices')\n",
    "        plt.xticks(x, towns, rotation=45)\n",
    "        plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('sentiment_impact.png')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f2f368-c522-49be-a10a-830d0c45fc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning for XGBoost\n",
    "\n",
    "def train_xgboost(X, y, categorical_features, numerical_features, do_hyper_tuning=True, apply_pca=False, top_n_configs=5):\n",
    "    \"\"\"\n",
    "    Train an XGBoost gradient boosting model with comprehensive hyperparameter tuning and configuration comparison\n",
    "    \n",
    "    Args:\n",
    "        X: Feature DataFrame\n",
    "        y: Target series\n",
    "        categorical_features: List of categorical feature names\n",
    "        numerical_features: List of numerical feature names\n",
    "        do_hyper_tuning: Whether to perform hyperparameter tuning\n",
    "        apply_pca: Whether to apply PCA for dimensionality reduction\n",
    "        top_n_configs: Number of top configurations to compare\n",
    "        \n",
    "    Returns:\n",
    "        Trained model, preprocessor, and metrics\n",
    "    \"\"\"\n",
    "    # Create preprocessing pipeline\n",
    "    preprocessor = create_preprocessing_pipeline(\n",
    "        categorical_features, \n",
    "        numerical_features,\n",
    "        apply_pca=apply_pca\n",
    "    )\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Preprocess the data\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "    \n",
    "    if do_hyper_tuning:\n",
    "        # Expanded parameter grid with additional XGBoost parameters\n",
    "        param_grid = {\n",
    "            # Basic parameters\n",
    "            'n_estimators': [100, 200, 300, 400],\n",
    "            'max_depth': [3, 5, 7, 9, 11],\n",
    "            'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "            'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "            'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "            'min_child_weight': [1, 2, 3, 4, 5],\n",
    "            'gamma': [0, 0.1, 0.2, 0.3, 0.4],\n",
    "            'reg_alpha': [0, 0.1, 0.5, 1, 2],\n",
    "            'reg_lambda': [0.1, 0.5, 1, 1.5, 2],\n",
    "            \n",
    "            # Additional parameters\n",
    "            'scale_pos_weight': [0.5, 1.0, 1.5],\n",
    "            'max_delta_step': [0, 0.1, 0.5, 1],\n",
    "            \n",
    "            # Tree construction parameters\n",
    "            'grow_policy': ['depthwise', 'lossguide'],\n",
    "            'max_leaves': [0, 16, 32, 64],\n",
    "            'max_bin': [128, 256, 512],\n",
    "            \n",
    "            # Sampling methods\n",
    "            'sampling_method': ['uniform', 'gradient_based']\n",
    "        }\n",
    "        \n",
    "        # Use RandomizedSearchCV for efficient hyperparameter search\n",
    "        random_search = RandomizedSearchCV(\n",
    "            xgb.XGBRegressor(\n",
    "                objective='reg:squarederror',\n",
    "                random_state=42,\n",
    "                tree_method='hist'  # Efficient histogram-based method\n",
    "            ),\n",
    "            param_distributions=param_grid,\n",
    "            n_iter=50,  # Number of parameter settings sampled\n",
    "            scoring='neg_mean_squared_error',\n",
    "            cv=5,\n",
    "            verbose=1,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        print(\"\\nPerforming extended hyperparameter tuning for XGBoost...\")\n",
    "        random_search.fit(X_train_processed, y_train)\n",
    "        \n",
    "        # Get top configurations\n",
    "        top_configs = sorted(\n",
    "            zip(\n",
    "                random_search.cv_results_['mean_test_score'], \n",
    "                random_search.cv_results_['params']\n",
    "            ), \n",
    "            key=lambda x: x[0], \n",
    "            reverse=True\n",
    "        )[:top_n_configs]\n",
    "        \n",
    "        print(\"\\nTop XGBoost Configurations:\")\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"{'Rank':<6} {'CV Score':>12} {'Configuration'}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Store results for each top configuration\n",
    "        results = {}\n",
    "        for rank, (cv_score, params) in enumerate(top_configs, 1):\n",
    "            # Create model with current configuration\n",
    "            xgb_model = xgb.XGBRegressor(\n",
    "                objective='reg:squarederror',\n",
    "                random_state=42,\n",
    "                **params\n",
    "            )\n",
    "            \n",
    "            # Train the model\n",
    "            xgb_model.fit(X_train_processed, y_train)\n",
    "            \n",
    "            # Evaluate the model\n",
    "            y_pred = xgb_model.predict(X_test_processed)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            \n",
    "            # Print configuration details\n",
    "            print(f\"{rank:<6} {-cv_score:>12.4f} {params}\")\n",
    "            \n",
    "            # Store results\n",
    "            results[f'Config_{rank}'] = {\n",
    "                'model': xgb_model,\n",
    "                'params': params,\n",
    "                'cv_score': -cv_score,\n",
    "                'metrics': {\n",
    "                    'rmse': rmse,\n",
    "                    'mae': mae,\n",
    "                    'r2': r2\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Visualize actual vs predicted values\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "            plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')\n",
    "            plt.xlabel('Actual Prices')\n",
    "            plt.ylabel('Predicted Prices')\n",
    "            plt.title(f'Config {rank} XGBoost: Actual vs Predicted Prices')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'xgboost_config_{rank}_results.png')\n",
    "            plt.close()\n",
    "            \n",
    "            # Plot feature importance\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            xgb.plot_importance(xgb_model, max_num_features=20)\n",
    "            plt.title(f'Config {rank} XGBoost Feature Importance')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'xgboost_config_{rank}_feature_importance.png')\n",
    "            plt.close()\n",
    "        \n",
    "        # Print detailed comparison\n",
    "        print(\"\\nXGBoost Configuration Comparison:\")\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"{'Configuration':<15} {'RMSE':>10} {'MAE':>10} {'R':>10}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for name, result in results.items():\n",
    "            metrics = result['metrics']\n",
    "            print(f\"{name:<15} {metrics['rmse']:>10.2f} {metrics['mae']:>10.2f} {metrics['r2']:>10.4f}\")\n",
    "        \n",
    "        # Select the best model based on test set performance\n",
    "        best_config_name = max(results, key=lambda x: results[x]['metrics']['r2'])\n",
    "        best_model = results[best_config_name]['model']\n",
    "        best_params = results[best_config_name]['params']\n",
    "        \n",
    "        print(f\"\\nBest configuration: {best_config_name}\")\n",
    "        print(\"Best parameters:\")\n",
    "        for param, value in best_params.items():\n",
    "            print(f\"{param}: {value}\")\n",
    "        \n",
    "        return best_model, preprocessor, results\n",
    "    \n",
    "    else:\n",
    "        # Default configuration if no tuning\n",
    "        default_params = {\n",
    "            'n_estimators': 300,\n",
    "            'max_depth': 6,\n",
    "            'learning_rate': 0.1,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'objective': 'reg:squarederror',\n",
    "            'random_state': 42\n",
    "        }\n",
    "        \n",
    "        # Create and train the XGBoost model\n",
    "        xgb_model = xgb.XGBRegressor(**default_params)\n",
    "        xgb_model.fit(X_train_processed, y_train)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        y_pred = xgb_model.predict(X_test_processed)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        metrics = {\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'r2': r2\n",
    "        }\n",
    "        \n",
    "        return xgb_model, preprocessor, metrics\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the entire modeling pipeline\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    df = load_and_preprocess_data('merged_data_4.csv')\n",
    "    \n",
    "    # Check for sentiment data\n",
    "    sentiment_cols = [col for col in df.columns if 'sentiment' in col]\n",
    "    sentiment_available = len(sentiment_cols) > 0\n",
    "    \n",
    "    if not sentiment_available:\n",
    "        print(\"No sentiment columns found in the dataset. Some analyses will be skipped.\")\n",
    "    else:\n",
    "        print(f\"Found sentiment columns: {sentiment_cols}\")\n",
    "    \n",
    "    # Prepare features\n",
    "    X, y, categorical_features, numerical_features = prepare_features(df)\n",
    "    \n",
    "    # Set hyperparameter tuning and PCA flags\n",
    "    do_hyper_tuning = True\n",
    "    apply_pca_xgboost = False    # Tree-based models like XGBoost can handle high-dimensionality\n",
    "    \n",
    "    # Train XGBoost with random search and top configuration comparison\n",
    "    xgb_model, xgb_preprocessor, xgb_results = train_xgboost(\n",
    "        X, y, categorical_features, numerical_features, \n",
    "        do_hyper_tuning=do_hyper_tuning, \n",
    "        apply_pca=apply_pca_xgboost\n",
    "    )\n",
    "    \n",
    "    return xgb_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d05212-19de-49e9-8ba4-6d28e2bf8c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "Top XGBoost Configurations Results:\n",
    "------------------------------------------------------------\n",
    "Rank       CV Score Configuration\n",
    "------------------------------------------------------------\n",
    "1      676114736.5612 {'subsample': 1.0, 'scale_pos_weight': 1.5, 'sampling_method': 'uniform', 'reg_lambda': 1, 'reg_alpha': 1, 'n_estimators': 350, 'min_child_weight': 1, 'max_leaves': 0, 'max_depth': 10, 'max_delta_step': 0, 'max_bin': 512, 'learning_rate': 0.075, 'grow_policy': 'lossguide', 'gamma': 0, 'colsample_bytree': 0.55}\n",
    "2      679837518.6431 {'subsample': 1.0, 'scale_pos_weight': 1.5, 'sampling_method': 'uniform', 'reg_lambda': 1.25, 'reg_alpha': 1, 'n_estimators': 300, 'min_child_weight': 1, 'max_leaves': 0, 'max_depth': 10, 'max_delta_step': 0, 'max_bin': 256, 'learning_rate': 0.075, 'grow_policy': 'lossguide', 'gamma': 0, 'colsample_bytree': 0.6}\n",
    "3      681004025.3180 {'subsample': 1.0, 'scale_pos_weight': 0.5, 'sampling_method': 'uniform', 'reg_lambda': 1, 'reg_alpha': 1, 'n_estimators': 350, 'min_child_weight': 1, 'max_leaves': 0, 'max_depth': 9, 'max_delta_step': 0, 'max_bin': 512, 'learning_rate': 0.1, 'grow_policy': 'depthwise', 'gamma': 0, 'colsample_bytree': 0.55}\n",
    "4      682258765.8153 {'subsample': 1.0, 'scale_pos_weight': 1.5, 'sampling_method': 'uniform', 'reg_lambda': 1.25, 'reg_alpha': 1.5, 'n_estimators': 300, 'min_child_weight': 1, 'max_leaves': 0, 'max_depth': 10, 'max_delta_step': 0, 'max_bin': 512, 'learning_rate': 0.1, 'grow_policy': 'lossguide', 'gamma': 0, 'colsample_bytree': 0.6}\n",
    "5      683345554.8013 {'subsample': 1.0, 'scale_pos_weight': 0.5, 'sampling_method': 'uniform', 'reg_lambda': 1.25, 'reg_alpha': 0.75, 'n_estimators': 300, 'min_child_weight': 1, 'max_leaves': 0, 'max_depth': 9, 'max_delta_step': 0, 'max_bin': 256, 'learning_rate': 0.1, 'grow_policy': 'lossguide', 'gamma': 0, 'colsample_bytree': 0.6}\n",
    "\n",
    "XGBoost Configuration Comparison:\n",
    "------------------------------------------------------------\n",
    "Configuration         RMSE        MAE         R\n",
    "------------------------------------------------------------\n",
    "Config_1          25612.60   18231.61     0.9794\n",
    "Config_2          25718.57   18297.42     0.9792\n",
    "Config_3          25777.55   18404.43     0.9791\n",
    "Config_4          25763.58   18354.98     0.9791\n",
    "Config_5          25983.51   18523.94     0.9788\n",
    "\n",
    "Best configuration: Config_1\n",
    "Best parameters:\n",
    "subsample: 1.0\n",
    "scale_pos_weight: 1.5\n",
    "sampling_method: uniform\n",
    "reg_lambda: 1\n",
    "reg_alpha: 1\n",
    "n_estimators: 350\n",
    "min_child_weight: 1\n",
    "max_leaves: 0\n",
    "max_depth: 10\n",
    "max_delta_step: 0\n",
    "max_bin: 512\n",
    "learning_rate: 0.075\n",
    "grow_policy: lossguide\n",
    "gamma: 0\n",
    "colsample_bytree: 0.55"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2c6e29-e110-47c1-b382-8f57c72f109f",
   "metadata": {},
   "source": [
    "# Annex of Mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c68c50-2e15-4202-829c-95996691910f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training (Before Full Data Fix merged_data_4.csv)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "def load_and_preprocess_data(filepath, reddit_sentiment_filepath=None):\n",
    "    \"\"\"\n",
    "    Load and preprocess the housing data\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the housing dataset CSV\n",
    "        reddit_sentiment_filepath: Optional path to the Reddit sentiment scores CSV\n",
    "        \n",
    "    Returns:\n",
    "        Preprocessed DataFrame\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Display info about the dataset\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(\"\\nData types:\")\n",
    "    print(df.dtypes)\n",
    "    print(\"\\nMissing values:\")\n",
    "    print(df.isnull().sum())\n",
    "    \n",
    "    # Specialized handler for 'YYYY-MM' formatted columns\n",
    "    def parse_yearmonth_column(df, column_name):\n",
    "        \"\"\"\n",
    "        Parse YYYY-MM formatted columns into cyclic time features\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame containing the column\n",
    "            column_name: Name of the column to parse\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with additional time-based features\n",
    "        \"\"\"\n",
    "        # Ensure the column is in string format\n",
    "        df[column_name] = df[column_name].astype(str)\n",
    "        \n",
    "        # Parse year and month\n",
    "        df['year'] = df[column_name].str[:4].astype(int)\n",
    "        df['month'] = df[column_name].str[5:].astype(int)\n",
    "        \n",
    "        # Find the earliest date to calculate months from start\n",
    "        earliest_year = df['year'].min()\n",
    "        earliest_month = df.loc[df['year'] == earliest_year, 'month'].min()\n",
    "        \n",
    "        # Calculate months from start\n",
    "        df['months_from_start'] = ((df['year'] - earliest_year) * 12 + \n",
    "                                   (df['month'] - earliest_month))\n",
    "        \n",
    "        # Create cyclic month features\n",
    "        df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "        df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    # Check for 'YYYY-MM' formatted columns\n",
    "    yearmonth_columns = [col for col in df.columns if \n",
    "        df[col].dtype == 'object' and \n",
    "        df[col].dropna().apply(lambda x: isinstance(x, str) and len(x) == 7 and x[4] == '-').all()]\n",
    "    \n",
    "    if yearmonth_columns:\n",
    "        print(f\"\\nDetected YYYY-MM formatted column(s): {yearmonth_columns}\")\n",
    "        for col in yearmonth_columns:\n",
    "            df = parse_yearmonth_column(df, col)\n",
    "    \n",
    "    # Basic preprocessing\n",
    "    \n",
    "    # Extract numeric value from storey_range (e.g., '01 TO 03' -> 2 as the middle value)\n",
    "    if 'storey_range' in df.columns:\n",
    "        df['storey_median'] = df['storey_range'].apply(\n",
    "            lambda x: np.mean([int(i) for i in re.findall(r'\\d+', x)])\n",
    "        )\n",
    "    \n",
    "    # Calculate remaining lease more precisely (if not already available)\n",
    "    if 'remaining_lease' not in df.columns or df['remaining_lease'].isnull().any():\n",
    "        # Check the format of remaining_lease (if it exists)\n",
    "        if 'remaining_lease' in df.columns:\n",
    "            # Check a sample value to see if it needs conversion\n",
    "            sample = df['remaining_lease'].iloc[0] if not df['remaining_lease'].isnull().all() else None\n",
    "            if sample and isinstance(sample, str) and 'years' in sample:\n",
    "                # Extract numeric value from strings like \"61 years 06 months\"\n",
    "                df['remaining_lease'] = df['remaining_lease'].apply(\n",
    "                    lambda x: float(x.split('years')[0].strip()) if isinstance(x, str) else x\n",
    "                )\n",
    "        elif 'lease_commence_date' in df.columns:\n",
    "            current_year = 2025  # Update this as needed\n",
    "            df['remaining_lease'] = df['lease_commence_date'].apply(\n",
    "                lambda x: (x + 99) - current_year if pd.notna(x) else np.nan\n",
    "            )\n",
    "    \n",
    "    # Convert sentiment columns if they exist\n",
    "    sentiment_cols = [col for col in df.columns if 'sentiment' in col]\n",
    "    for col in sentiment_cols:\n",
    "        if col in df.columns:\n",
    "            # Ensure sentiment scores are numeric\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Create proximity features from MRT data if available\n",
    "    if 'mrt_distance_km' in df.columns:\n",
    "        df['mrt_accessibility'] = 1 / (1.0 + df['mrt_distance_km'])  # Higher value means closer to MRT\n",
    "    \n",
    "    # Normalize walking time with distance to create a walking convenience score\n",
    "    if 'walking_time_min' in df.columns and 'walking_distance_m' in df.columns:\n",
    "        # Lower values are better (less time per distance)\n",
    "        df['walking_convenience'] = df['walking_distance_m'] / (df['walking_time_min'] * 60 + 0.1)  # Added 0.1 to avoid division by zero\n",
    "        \n",
    "    # Age of the flat might be important\n",
    "    if 'lease_commence_date' in df.columns:\n",
    "        df['flat_age'] = 2025 - df['lease_commence_date']  # Update the year as needed\n",
    "    \n",
    "    # Make sure to reuse existing time-based features if created earlier\n",
    "    if 'month' in df.columns and 'year' in df.columns:\n",
    "        # Create a reference date to measure months from start if not already created\n",
    "        if 'months_from_start' not in df.columns:\n",
    "            earliest_year = df['year'].min()\n",
    "            earliest_month = df.loc[df['year'] == earliest_year, 'month'].min()\n",
    "            \n",
    "            # Calculate months from start (for inflation trends)\n",
    "            df['months_from_start'] = (df['year'] - earliest_year) * 12 + (df['month'] - earliest_month)\n",
    "        \n",
    "        # Extract month for seasonal trends\n",
    "        df['month_in_year'] = df['month']\n",
    "    elif 'transaction_date' in df.columns:\n",
    "        # Parse transaction dates\n",
    "        df['transaction_date'] = pd.to_datetime(df['transaction_date'])\n",
    "        \n",
    "        # Find the earliest date\n",
    "        earliest_date = df['transaction_date'].min()\n",
    "        \n",
    "        # Calculate months from start (capture inflation trends)\n",
    "        df['months_from_start'] = ((df['transaction_date'].dt.year - earliest_date.year) * 12 + \n",
    "                                  (df['transaction_date'].dt.month - earliest_date.month))\n",
    "        \n",
    "        # Extract month for seasonal trends\n",
    "        df['month_in_year'] = df['transaction_date'].dt.month\n",
    "    elif 'resale_registration_date' in df.columns:\n",
    "        # Try using registration date if available\n",
    "        try:\n",
    "            df['resale_registration_date'] = pd.to_datetime(df['resale_registration_date'])\n",
    "            \n",
    "            # Find the earliest date\n",
    "            earliest_date = df['resale_registration_date'].min()\n",
    "            \n",
    "            # Calculate months from start\n",
    "            df['months_from_start'] = ((df['resale_registration_date'].dt.year - earliest_date.year) * 12 + \n",
    "                                      (df['resale_registration_date'].dt.month - earliest_date.month))\n",
    "            \n",
    "            # Extract month for seasonal trends\n",
    "            df['month_in_year'] = df['resale_registration_date'].dt.month\n",
    "        except:\n",
    "            print(\"Could not convert resale_registration_date. Creating artificial time features.\")\n",
    "            # If conversion fails, create artificial time features\n",
    "            df['months_from_start'] = range(len(df))  # Just an incremental counter as proxy\n",
    "            df['month_in_year'] = np.random.randint(1, 13, size=len(df))  # Random month as placeholder\n",
    "    else:\n",
    "        # If no date columns found, create artificial features\n",
    "        print(\"No date columns found. Creating artificial time features.\")\n",
    "        df['months_from_start'] = range(len(df))  # Just an incremental counter as proxy\n",
    "        df['month_in_year'] = np.random.randint(1, 13, size=len(df))  # Random month as placeholder\n",
    "        \n",
    "    # Check if time-based features were successfully created\n",
    "    if 'months_from_start' in df.columns and 'month_in_year' in df.columns:\n",
    "        print(f\"Created time-based features: months_from_start and month_in_year\")\n",
    "    else:\n",
    "        print(f\"Warning: Failed to create time-based features\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_features(df, target='resale_price'):\n",
    "    \"\"\"\n",
    "    Prepare features for modeling\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        target: Target variable name\n",
    "        \n",
    "    Returns:\n",
    "        X, y, and feature information\n",
    "    \"\"\"\n",
    "    # Separate features and target\n",
    "    y = df[target]\n",
    "    X = df.drop(target, axis=1)\n",
    "    \n",
    "    # Identify categorical and numerical features\n",
    "    categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "    numerical_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    \n",
    "    # Remove any ID columns or other non-predictive features\n",
    "    features_to_drop = ['street_name', 'storey_range']  # Add others as needed\n",
    "    \n",
    "    # Also drop date columns as we've already extracted their information\n",
    "    date_cols = [col for col in X.columns if 'date' in col.lower()]\n",
    "    features_to_drop.extend(date_cols)\n",
    "    \n",
    "    for feature in features_to_drop:\n",
    "        if feature in numerical_features:\n",
    "            numerical_features.remove(feature)\n",
    "        if feature in categorical_features:\n",
    "            categorical_features.remove(feature)\n",
    "    \n",
    "    # Make sure longitude and latitude are in numerical features if they exist\n",
    "    for feature in ['longitude', 'latitude']:\n",
    "        if feature in X.columns and feature not in numerical_features:\n",
    "            numerical_features.append(feature)\n",
    "    \n",
    "    # Ensure months_from_start and month_in_year are handled correctly\n",
    "    if 'month_in_year' in numerical_features:\n",
    "        numerical_features.remove('month_in_year')\n",
    "        # We'll handle this specially in the preprocessing pipeline\n",
    "    \n",
    "    # Drop the features from X\n",
    "    X = X.drop([f for f in features_to_drop if f in X.columns], axis=1)\n",
    "    \n",
    "    # Update categorical and numerical features lists\n",
    "    categorical_features = [f for f in categorical_features if f in X.columns]\n",
    "    numerical_features = [f for f in numerical_features if f in X.columns]\n",
    "    \n",
    "    return X, y, categorical_features, numerical_features\n",
    "\n",
    "def create_preprocessing_pipeline(categorical_features, numerical_features, X_train=None, apply_pca=False, pca_components=0.95):\n",
    "    \"\"\"\n",
    "    Create a preprocessing pipeline with optional PCA\n",
    "    \n",
    "    Args:\n",
    "        categorical_features: List of categorical feature names\n",
    "        numerical_features: List of numerical feature names\n",
    "        X_train: Training data to determine cardinality\n",
    "        apply_pca: Whether to apply PCA for dimensionality reduction\n",
    "        pca_components: Number of components or variance ratio to keep\n",
    "        \n",
    "    Returns:\n",
    "        Scikit-learn preprocessing pipeline\n",
    "    \"\"\"\n",
    "    # Import necessary libraries\n",
    "    from sklearn.preprocessing import FunctionTransformer\n",
    "    \n",
    "    # MODIFICATION 1: Split categorical features by cardinality\n",
    "    low_cardinality_features = []\n",
    "    high_cardinality_features = []\n",
    "    \n",
    "    if X_train is not None:\n",
    "        for feature in categorical_features:\n",
    "            if feature in X_train.columns:\n",
    "                unique_values = X_train[feature].nunique()\n",
    "                if unique_values <= 10:  # Only one-hot encode if 10 or fewer categories\n",
    "                    low_cardinality_features.append(feature)\n",
    "                else:\n",
    "                    high_cardinality_features.append(feature)\n",
    "    else:\n",
    "        # If no training data provided, treat all as high cardinality for safety\n",
    "        high_cardinality_features = categorical_features\n",
    "    \n",
    "    # Create column transformer with appropriate encoders\n",
    "    transformers = []\n",
    "    \n",
    "    # One-hot encoding for low-cardinality features\n",
    "    if low_cardinality_features:\n",
    "        transformers.append(\n",
    "            ('ohe', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                ('encoder', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
    "            ]), low_cardinality_features)\n",
    "        )\n",
    "    \n",
    "    # Ordinal encoding for high-cardinality features\n",
    "    if high_cardinality_features:\n",
    "        transformers.append(\n",
    "            ('ordinal', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "            ]), high_cardinality_features)\n",
    "        )\n",
    "    \n",
    "    # Special handling for date-formatted columns like '2017-01'\n",
    "    month_column = None\n",
    "    \n",
    "    # Check if we have a date-formatted column in YYYY-MM format\n",
    "    date_format_columns = []\n",
    "    if X_train is not None:\n",
    "        for col in X_train.columns:\n",
    "            # Check a few samples to see if they match the YYYY-MM pattern\n",
    "            if col in X_train.columns:\n",
    "                sample_vals = X_train[col].dropna().head(5)\n",
    "                if all(isinstance(val, str) and len(val) == 7 and val[4] == '-' for val in sample_vals):\n",
    "                    date_format_columns.append(col)\n",
    "    \n",
    "    if date_format_columns:\n",
    "        # Handle date-formatted columns like '2017-01'\n",
    "        def parse_yearmonth(X):\n",
    "            \"\"\"Extract month from YYYY-MM format and convert to cyclic features.\"\"\"\n",
    "            import pandas as pd\n",
    "            \n",
    "            # Convert to DataFrame if it's a numpy array\n",
    "            if isinstance(X, np.ndarray):\n",
    "                X_df = pd.DataFrame(X, columns=['yearmonth'])\n",
    "            else:\n",
    "                X_df = pd.DataFrame({'yearmonth': X})\n",
    "            \n",
    "            # Extract month and convert to numeric\n",
    "            months = X_df['yearmonth'].str.split('-').str[1].astype(int)\n",
    "            \n",
    "            # Create cyclic features\n",
    "            sin_values = np.sin(2 * np.pi * months / 12)\n",
    "            cos_values = np.cos(2 * np.pi * months / 12)\n",
    "            \n",
    "            return np.column_stack([sin_values, cos_values])\n",
    "        \n",
    "        for col in date_format_columns:\n",
    "            transformers.append(\n",
    "                (f'cyclic_{col}', Pipeline([\n",
    "                    ('imputer', SimpleImputer(strategy='constant', fill_value='2020-01')),  # Default value if missing\n",
    "                    ('cyclic_encoder', FunctionTransformer(parse_yearmonth))\n",
    "                ]), [col])\n",
    "            )\n",
    "    \n",
    "    # Also keep the original handling for numerical month_in_year if it exists\n",
    "    elif 'month_in_year' in numerical_features or (X_train is not None and 'month_in_year' in X_train.columns):\n",
    "        # Create a function to transform month values to cyclic features\n",
    "        def create_cyclic_features(X):\n",
    "            \"\"\"Transform month values to cyclic sin/cos features.\"\"\"\n",
    "            # Handle 1D or 2D arrays\n",
    "            X_reshaped = X.reshape(-1) if X.ndim > 1 else X\n",
    "            sin_values = np.sin(2 * np.pi * X_reshaped / 12)\n",
    "            cos_values = np.cos(2 * np.pi * X_reshaped / 12)\n",
    "            return np.column_stack([sin_values, cos_values])\n",
    "            \n",
    "        transformers.append(\n",
    "            ('cyclic', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='median')),\n",
    "                ('cyclic_encoder', FunctionTransformer(create_cyclic_features))\n",
    "            ]), ['month_in_year'])\n",
    "        )\n",
    "    \n",
    "    # Standard scaling for numerical features (except for month_in_year)\n",
    "    numerical_features_filtered = [f for f in numerical_features if f != 'month_in_year']\n",
    "    if numerical_features_filtered:\n",
    "        num_pipeline = [\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]\n",
    "        \n",
    "        # Add PCA if requested\n",
    "        if apply_pca:\n",
    "            num_pipeline.append(('pca', PCA(n_components=pca_components)))\n",
    "            \n",
    "        transformers.append(\n",
    "            ('num', Pipeline(num_pipeline), numerical_features_filtered)\n",
    "        )\n",
    "    \n",
    "    # Create the column transformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=transformers,\n",
    "        remainder='drop'  # Drop any columns not specified\n",
    "    )\n",
    "    \n",
    "    return preprocessor\n",
    "\n",
    "def train_linear_regression(X, y, categorical_features, numerical_features, do_hyper_tuning=True, apply_pca=True):\n",
    "    \"\"\"\n",
    "    Train a multivariate linear regression model with hyperparameter tuning\n",
    "    \n",
    "    Args:\n",
    "        X: Feature DataFrame\n",
    "        y: Target series\n",
    "        categorical_features: List of categorical feature names\n",
    "        numerical_features: List of numerical feature names\n",
    "        do_hyper_tuning: Whether to perform hyperparameter tuning\n",
    "        apply_pca: Whether to apply PCA for dimensionality reduction\n",
    "        \n",
    "    Returns:\n",
    "        Trained model, preprocessor, and metrics\n",
    "    \"\"\"\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # MODIFICATION 1: Split categorical features by cardinality\n",
    "    low_cardinality_features = []\n",
    "    high_cardinality_features = []\n",
    "    \n",
    "    for feature in categorical_features:\n",
    "        if feature in X_train.columns:\n",
    "            unique_values = X_train[feature].nunique()\n",
    "            if unique_values <= 10:  # Only one-hot encode if 10 or fewer categories\n",
    "                low_cardinality_features.append(feature)\n",
    "            else:\n",
    "                high_cardinality_features.append(feature)\n",
    "    \n",
    "    print(f\"\\nLow cardinality features (one-hot encoded): {low_cardinality_features}\")\n",
    "    print(f\"High cardinality features (ordinal encoded): {high_cardinality_features}\")\n",
    "    \n",
    "    # Create preprocessor with cardinality-based encoding\n",
    "    preprocessor = create_preprocessing_pipeline(\n",
    "        categorical_features, \n",
    "        numerical_features,\n",
    "        X_train=X_train,\n",
    "        apply_pca=apply_pca\n",
    "    )\n",
    "    \n",
    "    if do_hyper_tuning:\n",
    "        # Create pipeline with multiple regression models\n",
    "        param_grid = [\n",
    "            {\n",
    "                'regressor': [LinearRegression()],\n",
    "                'regressor__fit_intercept': [True, False]\n",
    "            },\n",
    "            {\n",
    "                'regressor': [Ridge()],\n",
    "                'regressor__alpha': [0.01, 0.1, 1.0, 10.0],\n",
    "                'regressor__fit_intercept': [True, False]\n",
    "            },\n",
    "            {\n",
    "                'regressor': [Lasso()],\n",
    "                'regressor__alpha': [0.001, 0.01, 0.1, 1.0],\n",
    "                'regressor__fit_intercept': [True, False]\n",
    "            },\n",
    "            {\n",
    "                'regressor': [ElasticNet()],\n",
    "                'regressor__alpha': [0.001, 0.01, 0.1, 1.0],\n",
    "                'regressor__l1_ratio': [0.1, 0.5, 0.7, 0.9],\n",
    "                'regressor__fit_intercept': [True, False]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Create pipeline\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', LinearRegression())\n",
    "        ])\n",
    "        \n",
    "        # Use RandomizedSearchCV for more efficient hyperparameter search\n",
    "        grid_search = RandomizedSearchCV(\n",
    "            pipeline,\n",
    "            param_grid,\n",
    "            cv=5,\n",
    "            scoring='neg_mean_squared_error',\n",
    "            n_iter=15,  # Number of parameter settings sampled\n",
    "            n_jobs=-1,\n",
    "            verbose=1,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        print(\"\\nPerforming hyperparameter tuning for Linear Model...\")\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        best_model = grid_search.best_estimator_\n",
    "        print(f\"Best model: {grid_search.best_params_}\")\n",
    "        print(f\"Best cross-validation score: {-grid_search.best_score_:.2f} MSE\")\n",
    "        \n",
    "    else:\n",
    "        # Use standard Linear Regression without tuning\n",
    "        best_model = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', LinearRegression())\n",
    "        ])\n",
    "        best_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    metrics = {\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2\n",
    "    }\n",
    "    \n",
    "    print(\"\\nLinear Regression Metrics:\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"R2 Score: {r2:.4f}\")\n",
    "    \n",
    "    # Visualize actual vs predicted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')\n",
    "    plt.xlabel('Actual Prices')\n",
    "    plt.ylabel('Predicted Prices')\n",
    "    plt.title('Linear Regression: Actual vs Predicted Prices')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('linear_regression_results.png')\n",
    "    \n",
    "    return best_model, preprocessor, metrics\n",
    "\n",
    "def train_xgboost(X, y, categorical_features, numerical_features, do_hyper_tuning=True, apply_pca=False):\n",
    "    \"\"\"\n",
    "    Train an XGBoost gradient boosting model with hyperparameter tuning\n",
    "    \n",
    "    Args:\n",
    "        X: Feature DataFrame\n",
    "        y: Target series\n",
    "        categorical_features: List of categorical feature names\n",
    "        numerical_features: List of numerical feature names\n",
    "        do_hyper_tuning: Whether to perform hyperparameter tuning\n",
    "        apply_pca: Whether to apply PCA for dimensionality reduction\n",
    "        \n",
    "    Returns:\n",
    "        Trained model, preprocessor, and metrics\n",
    "    \"\"\"\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create preprocessor with cardinality-based encoding\n",
    "    preprocessor = create_preprocessing_pipeline(\n",
    "        categorical_features, \n",
    "        numerical_features,\n",
    "        X_train=X_train,\n",
    "        apply_pca=apply_pca\n",
    "    )\n",
    "    \n",
    "    # Preprocess the data\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "    \n",
    "    if do_hyper_tuning:\n",
    "        # Define parameter grid for hyperparameter tuning\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [3, 5, 7, 9],\n",
    "            'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "            'subsample': [0.6, 0.8, 1.0],\n",
    "            'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "            'min_child_weight': [1, 3, 5],\n",
    "            'gamma': [0, 0.1, 0.2],\n",
    "            'reg_alpha': [0, 0.1, 1],\n",
    "            'reg_lambda': [1, 1.5, 2]\n",
    "        }\n",
    "        \n",
    "        # Use RandomizedSearchCV for more efficient hyperparameter search\n",
    "        random_search = RandomizedSearchCV(\n",
    "            xgb.XGBRegressor(\n",
    "                objective='reg:squarederror',\n",
    "                random_state=42\n",
    "            ),\n",
    "            param_distributions=param_grid,\n",
    "            n_iter=20,  # Number of parameter settings sampled\n",
    "            scoring='neg_mean_squared_error',\n",
    "            cv=5,\n",
    "            verbose=1,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        print(\"\\nPerforming hyperparameter tuning for XGBoost...\")\n",
    "        random_search.fit(X_train_processed, y_train)\n",
    "        \n",
    "        # Get best parameters\n",
    "        best_params = random_search.best_params_\n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "        print(f\"Best cross-validation score: {-random_search.best_score_:.2f} MSE\")\n",
    "        \n",
    "        # Create and train the XGBoost model with best parameters\n",
    "        xgb_model = xgb.XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            random_state=42,\n",
    "            **best_params\n",
    "        )\n",
    "    else:\n",
    "        # Define XGBoost parameters\n",
    "        xgb_params = {\n",
    "            'n_estimators': 200,\n",
    "            'max_depth': 6,\n",
    "            'learning_rate': 0.1,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'objective': 'reg:squarederror',\n",
    "            'random_state': 42\n",
    "        }\n",
    "        \n",
    "        # Create the XGBoost model\n",
    "        xgb_model = xgb.XGBRegressor(**xgb_params)\n",
    "    \n",
    "    # Train the model for XGBoost 2.1.4\n",
    "    eval_set = [(X_test_processed, y_test)]\n",
    "    \n",
    "    try:\n",
    "        # For XGBoost 2.1.4, we need to use callbacks instead of direct parameters\n",
    "        from xgboost.callback import EarlyStopping\n",
    "        \n",
    "        early_stopping = EarlyStopping(\n",
    "            rounds=20,\n",
    "            metric_name='rmse',\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        xgb_model.fit(\n",
    "            X_train_processed, y_train,\n",
    "            eval_set=eval_set,\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=True\n",
    "        )\n",
    "    except (ImportError, TypeError) as e:\n",
    "        # Fall back to basic method if needed\n",
    "        print(f\"Using basic XGBoost training method due to: {e}\")\n",
    "        xgb_model.fit(X_train_processed, y_train)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred = xgb_model.predict(X_test_processed)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    metrics = {\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2\n",
    "    }\n",
    "    \n",
    "    print(\"\\nXGBoost Metrics:\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"R2 Score: {r2:.4f}\")\n",
    "    \n",
    "    # Visualize actual vs predicted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')\n",
    "    plt.xlabel('Actual Prices')\n",
    "    plt.ylabel('Predicted Prices')\n",
    "    plt.title('XGBoost: Actual vs Predicted Prices')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('xgboost_results.png')\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    xgb.plot_importance(xgb_model, max_num_features=20)\n",
    "    plt.title('XGBoost Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('xgboost_feature_importance.png')\n",
    "    \n",
    "    return xgb_model, preprocessor, metrics\n",
    "\n",
    "def create_dnn_model(input_dim, config):\n",
    "    \"\"\"\n",
    "    Create a DNN model based on configuration parameters\n",
    "    \n",
    "    Args:\n",
    "        input_dim: Number of input features\n",
    "        config: Dictionary with model configuration\n",
    "        \n",
    "    Returns:\n",
    "        Compiled Keras model\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First hidden layer\n",
    "    model.add(Dense(\n",
    "        config.get('first_layer_units', 128), \n",
    "        activation=config.get('activation', 'relu'), \n",
    "        input_dim=input_dim\n",
    "    ))\n",
    "    \n",
    "    if config.get('use_batch_norm', True):\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "    if config.get('dropout_rate', 0.3) > 0:\n",
    "        model.add(Dropout(config.get('dropout_rate', 0.3)))\n",
    "    \n",
    "    # Hidden layers\n",
    "    for units in config.get('hidden_layers', [64, 32]):\n",
    "        model.add(Dense(units, activation=config.get('activation', 'relu')))\n",
    "        \n",
    "        if config.get('use_batch_norm', True):\n",
    "            model.add(BatchNormalization())\n",
    "            \n",
    "        if config.get('dropout_rate', 0.3) > 0:\n",
    "            model.add(Dropout(config.get('dropout_rate', 0.3) * 0.7))  # Reduce dropout in deeper layers\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=config.get('optimizer', 'adam'),\n",
    "        loss=config.get('loss', 'mse'),\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def tune_dnn_hyperparameters(X_train, y_train, input_dim):\n",
    "    \"\"\"\n",
    "    Tune hyperparameters for the DNN model\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        y_train: Training target\n",
    "        input_dim: Number of input features\n",
    "        \n",
    "    Returns:\n",
    "        Best hyperparameter configuration\n",
    "    \"\"\"\n",
    "    # Define hyperparameter configurations to try\n",
    "    configs = [\n",
    "        # Configuration 1\n",
    "        {\n",
    "            'first_layer_units': 128,\n",
    "            'hidden_layers': [64, 32],\n",
    "            'activation': 'relu',\n",
    "            'optimizer': 'adam',\n",
    "            'learning_rate': 0.001,\n",
    "            'batch_size': 32,\n",
    "            'dropout_rate': 0.3,\n",
    "            'use_batch_norm': True\n",
    "        },\n",
    "        # Configuration 2\n",
    "        {\n",
    "            'first_layer_units': 256,\n",
    "            'hidden_layers': [128, 64],\n",
    "            'activation': 'relu',\n",
    "            'optimizer': 'adam',\n",
    "            'learning_rate': 0.001,\n",
    "            'batch_size': 64,\n",
    "            'dropout_rate': 0.4,\n",
    "            'use_batch_norm': True\n",
    "        },\n",
    "        # Configuration 3\n",
    "        {\n",
    "            'first_layer_units': 128,\n",
    "            'hidden_layers': [128, 64, 32],\n",
    "            'activation': 'elu',\n",
    "            'optimizer': 'adam',\n",
    "            'learning_rate': 0.0005,\n",
    "            'batch_size': 32,\n",
    "            'dropout_rate': 0.3,\n",
    "            'use_batch_norm': True\n",
    "        },\n",
    "        # Configuration 4\n",
    "        {\n",
    "            'first_layer_units': 64,\n",
    "            'hidden_layers': [32, 16],\n",
    "            'activation': 'relu',\n",
    "            'optimizer': 'adam',\n",
    "            'learning_rate': 0.001,\n",
    "            'batch_size': 16,\n",
    "            'dropout_rate': 0.2,\n",
    "            'use_batch_norm': True\n",
    "        },\n",
    "        # Configuration 5\n",
    "        {\n",
    "            'first_layer_units': 128,\n",
    "            'hidden_layers': [64],\n",
    "            'activation': 'relu',\n",
    "            'optimizer': 'adam',\n",
    "            'learning_rate': 0.001,\n",
    "            'batch_size': 32,\n",
    "            'dropout_rate': 0.3,\n",
    "            'use_batch_norm': False\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Validation split\n",
    "    validation_split = 0.2\n",
    "    val_size = int(len(X_train) * validation_split)\n",
    "    X_train_val, X_val = X_train[:-val_size], X_train[-val_size:]\n",
    "    y_train_val, y_val = y_train[:-val_size], y_train[-val_size:]\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_config = None\n",
    "    \n",
    "    # Define early stopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\nPerforming hyperparameter tuning for DNN...\")\n",
    "    for i, config in enumerate(configs):\n",
    "        print(f\"\\nTrying configuration {i+1}/{len(configs)}\")\n",
    "        \n",
    "        # Create model\n",
    "        model = create_dnn_model(input_dim, config)\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            X_train_val, y_train_val,\n",
    "            epochs=100,\n",
    "            batch_size=config['batch_size'],\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        val_loss = min(history.history['val_loss'])\n",
    "        print(f\"Configuration {i+1} validation loss: {val_loss:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_config = config\n",
    "            print(f\"New best configuration found!\")\n",
    "    \n",
    "    print(f\"\\nBest DNN configuration: {best_config}\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    return best_config\n",
    "\n",
    "def train_dnn(X, y, categorical_features, numerical_features, do_hyper_tuning=True, apply_pca=False):\n",
    "    \"\"\"\n",
    "    Train a Deep Neural Network model with hyperparameter tuning\n",
    "    \n",
    "    Args:\n",
    "        X: Feature DataFrame\n",
    "        y: Target series\n",
    "        categorical_features: List of categorical feature names\n",
    "        numerical_features: List of numerical feature names\n",
    "        do_hyper_tuning: Whether to perform hyperparameter tuning\n",
    "        apply_pca: Whether to apply PCA for dimensionality reduction\n",
    "        \n",
    "    Returns:\n",
    "        Trained model, preprocessor, history, and metrics\n",
    "    \"\"\"\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create preprocessor with cardinality-based encoding\n",
    "    preprocessor = create_preprocessing_pipeline(\n",
    "        categorical_features, \n",
    "        numerical_features,\n",
    "        X_train=X_train,  \n",
    "        apply_pca=apply_pca\n",
    "    )\n",
    "    \n",
    "    # Preprocess the data\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "    \n",
    "    # Standardize the target variable for better neural network training\n",
    "    y_scaler = StandardScaler()\n",
    "    y_train_scaled = y_scaler.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
    "    y_test_scaled = y_scaler.transform(y_test.values.reshape(-1, 1)).ravel()\n",
    "    \n",
    "    # Get the number of features after preprocessing\n",
    "    input_dim = X_train_processed.shape[1]\n",
    "    \n",
    "    if do_hyper_tuning:\n",
    "        # Tune hyperparameters\n",
    "        best_config = tune_dnn_hyperparameters(X_train_processed, y_train_scaled, input_dim)\n",
    "        \n",
    "        # Create and compile the model with best configuration\n",
    "        dnn_model = create_dnn_model(input_dim, best_config)\n",
    "        batch_size = best_config['batch_size']\n",
    "    else:\n",
    "        # Default configuration\n",
    "        default_config = {\n",
    "            'first_layer_units': 128,\n",
    "            'hidden_layers': [64, 32],\n",
    "            'activation': 'relu',\n",
    "            'optimizer': 'adam',\n",
    "            'batch_size': 32,\n",
    "            'dropout_rate': 0.3,\n",
    "            'use_batch_norm': True\n",
    "        }\n",
    "        \n",
    "        # Create and compile the model with default configuration\n",
    "        dnn_model = create_dnn_model(input_dim, default_config)\n",
    "        batch_size = default_config['batch_size']\n",
    "    \n",
    "    # Define early stopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=20,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = dnn_model.fit(\n",
    "        X_train_processed, y_train_scaled,\n",
    "        epochs=200,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_scaled = dnn_model.predict(X_test_processed)\n",
    "    \n",
    "    # Inverse transform to get actual price predictions\n",
    "    y_pred = y_scaler.inverse_transform(y_pred_scaled).ravel()\n",
    "    \n",
    "    # Evaluate the model\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    metrics = {\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2\n",
    "    }\n",
    "    \n",
    "    print(\"\\nDNN Metrics:\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"R2 Score: {r2:.4f}\")\n",
    "    \n",
    "    # Plot training & validation loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('DNN Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('dnn_training_history.png')\n",
    "    \n",
    "    # Visualize actual vs predicted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')\n",
    "    plt.xlabel('Actual Prices')\n",
    "    plt.ylabel('Predicted Prices')\n",
    "    plt.title('DNN: Actual vs Predicted Prices')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('dnn_results.png')\n",
    "    \n",
    "    return dnn_model, preprocessor, history, metrics\n",
    "\n",
    "def analyze_time_features_impact(best_model, preprocessor, X, y):\n",
    "    \"\"\"\n",
    "    Analyze the impact of time-based features on predictions\n",
    "    \n",
    "    Args:\n",
    "        best_model: Trained model\n",
    "        preprocessor: Fitted preprocessor\n",
    "        X: Feature DataFrame\n",
    "        y: Target values\n",
    "    \"\"\"\n",
    "    # Check if time features exist\n",
    "    if 'months_from_start' not in X.columns or 'month_in_year' not in X.columns:\n",
    "        print(\"Cannot analyze time features impact: features not found in the dataset\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nAnalyzing impact of time-based features on predictions...\")\n",
    "    \n",
    "    # Copy the data to avoid modifying the original\n",
    "    X_copy = X.copy()\n",
    "    \n",
    "    # Create versions without time features\n",
    "    X_no_inflation = X_copy.copy()\n",
    "    X_no_inflation['months_from_start'] = X_no_inflation['months_from_start'].median()  # Neutralize the feature\n",
    "    \n",
    "    X_no_seasonal = X_copy.copy()\n",
    "    X_no_seasonal['month_in_year'] = X_no_seasonal['month_in_year'].median()  # Neutralize the feature\n",
    "    \n",
    "    X_no_time = X_copy.copy()\n",
    "    X_no_time['months_from_start'] = X_no_time['months_from_start'].median()\n",
    "    X_no_time['month_in_year'] = X_no_time['month_in_year'].median()\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_original = best_model.predict(preprocessor.transform(X_copy))\n",
    "    y_pred_no_inflation = best_model.predict(preprocessor.transform(X_no_inflation))\n",
    "    y_pred_no_seasonal = best_model.predict(preprocessor.transform(X_no_seasonal))\n",
    "    y_pred_no_time = best_model.predict(preprocessor.transform(X_no_time))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    rmse_original = np.sqrt(mean_squared_error(y, y_pred_original))\n",
    "    rmse_no_inflation = np.sqrt(mean_squared_error(y, y_pred_no_inflation))\n",
    "    rmse_no_seasonal = np.sqrt(mean_squared_error(y, y_pred_no_seasonal))\n",
    "    rmse_no_time = np.sqrt(mean_squared_error(y, y_pred_no_time))\n",
    "    \n",
    "    # Calculate the impact as percentage difference in RMSE\n",
    "    inflation_impact = (rmse_no_inflation - rmse_original) / rmse_original * 100\n",
    "    seasonal_impact = (rmse_no_seasonal - rmse_original) / rmse_original * 100\n",
    "    time_impact = (rmse_no_time - rmse_original) / rmse_original * 100\n",
    "    \n",
    "    print(f\"RMSE with all features: {rmse_original:.2f}\")\n",
    "    print(f\"RMSE without inflation trend: {rmse_no_inflation:.2f} (Impact: {inflation_impact:.2f}%)\")\n",
    "    print(f\"RMSE without seasonal pattern: {rmse_no_seasonal:.2f} (Impact: {seasonal_impact:.2f}%)\")\n",
    "    print(f\"RMSE without any time features: {rmse_no_time:.2f} (Impact: {time_impact:.2f}%)\")\n",
    "    \n",
    "    # Create visualization of price predictions by month\n",
    "    if len(X['month_in_year'].unique()) > 1:\n",
    "        # Group by month and calculate average predicted price\n",
    "        monthly_avg = pd.DataFrame({\n",
    "            'month': X['month_in_year'],\n",
    "            'actual': y,\n",
    "            'predicted': y_pred_original\n",
    "        }).groupby('month').mean().reset_index()\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(monthly_avg['month'], monthly_avg['actual'], 'b-', label='Actual Prices')\n",
    "        plt.plot(monthly_avg['month'], monthly_avg['predicted'], 'r--', label='Predicted Prices')\n",
    "        plt.xlabel('Month')\n",
    "        plt.ylabel('Average Price')\n",
    "        plt.title('Seasonal Pattern: Average Price by Month')\n",
    "        plt.legend()\n",
    "        plt.xticks(range(1, 13))\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('seasonal_pattern_analysis.png')\n",
    "        \n",
    "    # Analyze inflation trend\n",
    "    if len(X['months_from_start'].unique()) > 10:\n",
    "        # Group by time periods and calculate average prices\n",
    "        time_bins = pd.cut(X['months_from_start'], bins=10)\n",
    "        time_trend = pd.DataFrame({\n",
    "            'time_period': time_bins,\n",
    "            'actual': y,\n",
    "            'predicted': y_pred_original\n",
    "        }).groupby('time_period').mean().reset_index()\n",
    "        \n",
    "        # Create x-axis labels from bin intervals\n",
    "        time_labels = [f\"{int(interval.left)}-{int(interval.right)}\" for interval in time_trend['time_period']]\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(range(len(time_labels)), time_trend['actual'], 'b-', marker='o', label='Actual Prices')\n",
    "        plt.plot(range(len(time_labels)), time_trend['predicted'], 'r--', marker='x', label='Predicted Prices')\n",
    "        plt.xlabel('Months from Start (binned)')\n",
    "        plt.ylabel('Average Price')\n",
    "        plt.title('Inflation Trend: Average Price Over Time')\n",
    "        plt.legend()\n",
    "        plt.xticks(range(len(time_labels)), time_labels, rotation=45)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('inflation_trend_analysis.png')\n",
    "        \n",
    "    return {\n",
    "        'inflation_impact': inflation_impact,\n",
    "        'seasonal_impact': seasonal_impact,\n",
    "        'time_impact': time_impact\n",
    "    }\n",
    "\n",
    "def predict_with_sentiment(model, preprocessor, X, town_data):\n",
    "    \"\"\"\n",
    "    Make predictions with a trained model, using sentiment data that's already in X\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        preprocessor: Fitted preprocessor\n",
    "        X: Feature DataFrame (with sentiment columns)\n",
    "        town_data: Town names for each instance\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with predictions\n",
    "    \"\"\"\n",
    "    # Create a copy of X to avoid modifying the original\n",
    "    X_pred = X.copy()\n",
    "    \n",
    "    # Fill missing sentiment scores with the mean (if any)\n",
    "    sentiment_cols = [col for col in X_pred.columns if 'sentiment' in col]\n",
    "    for col in sentiment_cols:\n",
    "        if col in X_pred.columns and X_pred[col].isnull().any():\n",
    "            mean_value = X_pred[col].mean()\n",
    "            X_pred[col].fillna(mean_value, inplace=True)\n",
    "    \n",
    "    # Preprocess the data\n",
    "    X_processed = preprocessor.transform(X_pred)\n",
    "    \n",
    "    # Make predictions\n",
    "    if hasattr(model, 'predict'):  # sklearn, xgboost\n",
    "        predictions = model.predict(X_processed)\n",
    "    else:  # keras (TensorFlow)\n",
    "        predictions = model.predict(X_processed).flatten()\n",
    "    \n",
    "    # Create a DataFrame with predictions\n",
    "    results = pd.DataFrame({\n",
    "        'town': town_data,\n",
    "        'predicted_resale_price': predictions\n",
    "    })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def compare_models(metrics_dict):\n",
    "    \"\"\"\n",
    "    Compare the performance of different models\n",
    "    \n",
    "    Args:\n",
    "        metrics_dict: Dictionary with model names as keys and their metrics as values\n",
    "    \"\"\"\n",
    "    models = list(metrics_dict.keys())\n",
    "    rmse_values = [metrics_dict[model]['rmse'] for model in models]\n",
    "    mae_values = [metrics_dict[model]['mae'] for model in models]\n",
    "    r2_values = [metrics_dict[model]['r2'] for model in models]\n",
    "    \n",
    "    # Plot RMSE comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.bar(models, rmse_values)\n",
    "    plt.title('RMSE Comparison')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.bar(models, mae_values)\n",
    "    plt.title('MAE Comparison')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.bar(models, r2_values)\n",
    "    plt.title('R Score Comparison')\n",
    "    plt.ylabel('R')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_comparison.png')\n",
    "    \n",
    "    # Print comparison table\n",
    "    print(\"\\nModel Comparison:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Model':<15} {'RMSE':>10} {'MAE':>10} {'R':>10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for model in models:\n",
    "        print(f\"{model:<15} {metrics_dict[model]['rmse']:>10.2f} {metrics_dict[model]['mae']:>10.2f} {metrics_dict[model]['r2']:>10.4f}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the entire modeling pipeline\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    df = load_and_preprocess_data('merged_data_3.csv')\n",
    "    \n",
    "    # Check for time-based features\n",
    "    if 'months_from_start' in df.columns and 'month_in_year' in df.columns:\n",
    "        print(\"\\nSuccessfully created time-based features:\")\n",
    "        print(f\"- months_from_start: Captures inflation trends over time\")\n",
    "        print(f\"- month_in_year: Captures seasonal pricing patterns\")\n",
    "        \n",
    "        # Visualize months_from_start vs prices if possible\n",
    "        if 'resale_price' in df.columns:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            \n",
    "            # Grouped boxplot for month_in_year vs price\n",
    "            plt.subplot(1, 2, 1)\n",
    "            monthly_prices = df.groupby('month_in_year')['resale_price'].mean().reset_index()\n",
    "            plt.bar(monthly_prices['month_in_year'], monthly_prices['resale_price'])\n",
    "            plt.title('Average Price by Month')\n",
    "            plt.xlabel('Month in Year')\n",
    "            plt.ylabel('Average Resale Price')\n",
    "            plt.xticks(range(1, 13))\n",
    "            \n",
    "            # Scatterplot for months_from_start vs price trend\n",
    "            plt.subplot(1, 2, 2)\n",
    "            # Sample points to avoid overcrowding the plot\n",
    "            sample_size = min(2000, len(df))\n",
    "            sample_df = df.sample(sample_size, random_state=42)\n",
    "            plt.scatter(sample_df['months_from_start'], sample_df['resale_price'], alpha=0.3)\n",
    "            \n",
    "            # Add trend line\n",
    "            month_groups = df.groupby('months_from_start')['resale_price'].mean()\n",
    "            if len(month_groups) > 1:  # Only add trend line if there are multiple groups\n",
    "                plt.plot(month_groups.index, month_groups.values, 'r-', linewidth=2)\n",
    "            \n",
    "            plt.title('Price Trend Over Time')\n",
    "            plt.xlabel('Months from Start')\n",
    "            plt.ylabel('Resale Price')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig('time_features_analysis.png')\n",
    "    else:\n",
    "        print(\"\\nWarning: Time-based features were not created. Check your data format.\")\n",
    "    \n",
    "    # Check for sentiment data\n",
    "    sentiment_cols = [col for col in df.columns if 'sentiment' in col]\n",
    "    sentiment_available = len(sentiment_cols) > 0\n",
    "    \n",
    "    if not sentiment_available:\n",
    "        print(\"No sentiment columns found in the dataset. Some analyses will be skipped.\")\n",
    "    else:\n",
    "        print(f\"Found sentiment columns: {sentiment_cols}\")\n",
    "    \n",
    "    # Prepare features\n",
    "    X, y, categorical_features, numerical_features = prepare_features(df)\n",
    "    \n",
    "    # Set hyperparameter tuning and PCA flags\n",
    "    do_hyper_tuning = True\n",
    "    apply_pca_linear = True      # PCA often helps linear models\n",
    "    apply_pca_xgboost = False    # Tree-based models like XGBoost can handle high-dimensionality\n",
    "    apply_pca_dnn = False        # DNNs can learn representations from raw features\n",
    "    \n",
    "    print(\"\\n========== Training Linear Regression Model ==========\")\n",
    "    linear_model, linear_preprocessor, linear_metrics = train_linear_regression(\n",
    "        X, y, categorical_features, numerical_features, \n",
    "        do_hyper_tuning=do_hyper_tuning, \n",
    "        apply_pca=apply_pca_linear\n",
    "    )\n",
    "    \n",
    "    print(\"\\n========== Training XGBoost Model ==========\")\n",
    "    xgb_model, xgb_preprocessor, xgb_metrics = train_xgboost(\n",
    "        X, y, categorical_features, numerical_features, \n",
    "        do_hyper_tuning=do_hyper_tuning, \n",
    "        apply_pca=apply_pca_xgboost\n",
    "    )\n",
    "    \n",
    "    print(\"\\n========== Training DNN Model ==========\")\n",
    "    dnn_model, dnn_preprocessor, dnn_history, dnn_metrics = train_dnn(\n",
    "        X, y, categorical_features, numerical_features, \n",
    "        do_hyper_tuning=do_hyper_tuning, \n",
    "        apply_pca=apply_pca_dnn\n",
    "    )\n",
    "    \n",
    "    # Compare models\n",
    "    all_metrics = {\n",
    "        'Linear Regression': linear_metrics,\n",
    "        'XGBoost': xgb_metrics,\n",
    "        'DNN': dnn_metrics\n",
    "    }\n",
    "    \n",
    "    compare_models(all_metrics)\n",
    "    \n",
    "    # Get best model based on R score\n",
    "    best_model_name = max(all_metrics, key=lambda x: all_metrics[x]['r2'])\n",
    "    print(f\"\\nBest model based on R score: {best_model_name}\")\n",
    "    \n",
    "    # Save the best model for future use\n",
    "    if best_model_name == 'Linear Regression':\n",
    "        best_model = linear_model\n",
    "        best_preprocessor = linear_preprocessor\n",
    "    elif best_model_name == 'XGBoost':\n",
    "        best_model = xgb_model\n",
    "        best_preprocessor = xgb_preprocessor\n",
    "    else:  # DNN\n",
    "        best_model = dnn_model\n",
    "        best_preprocessor = dnn_preprocessor\n",
    "        \n",
    "    # Analyze the impact of time-based features\n",
    "    print(\"\\n========== Analyzing Time Features Impact ==========\")\n",
    "    time_impact = analyze_time_features_impact(best_model, best_preprocessor, X, y)\n",
    "    \n",
    "    # If sentiment data is available, show example of prediction and analyze sentiment impact\n",
    "    if sentiment_available:\n",
    "        print(\"\\nExample of prediction with sentiment data:\")\n",
    "        # Create a sample DataFrame for prediction\n",
    "        sample_X = X.loc[X.index[:5]].copy()\n",
    "        \n",
    "        # Make predictions\n",
    "        sample_predictions = predict_with_sentiment(\n",
    "            best_model, best_preprocessor, sample_X, df['town'].iloc[:5]\n",
    "        )\n",
    "        \n",
    "        print(sample_predictions)\n",
    "        \n",
    "        # Analyze sentiment impact\n",
    "        print(\"\\nExploring impact of sentiment on predictions:\")\n",
    "        # Create copies with modified sentiment values\n",
    "        high_sentiment_X = sample_X.copy()\n",
    "        low_sentiment_X = sample_X.copy()\n",
    "        \n",
    "        # Modify sentiment values (increase/decrease by 0.2)\n",
    "        for col in sentiment_cols:\n",
    "            if col in high_sentiment_X.columns:\n",
    "                high_sentiment_X[col] = high_sentiment_X[col].apply(lambda x: min(x + 0.2, 1.0))\n",
    "                low_sentiment_X[col] = low_sentiment_X[col].apply(lambda x: max(x - 0.2, -1.0))\n",
    "        \n",
    "        # Make predictions with modified sentiment\n",
    "        high_predictions = predict_with_sentiment(\n",
    "            best_model, best_preprocessor, high_sentiment_X, df['town'].iloc[:5]\n",
    "        )\n",
    "        \n",
    "        low_predictions = predict_with_sentiment(\n",
    "            best_model, best_preprocessor, low_sentiment_X, df['town'].iloc[:5]\n",
    "        )\n",
    "        \n",
    "        # Combine results to show sentiment impact\n",
    "        sentiment_impact = pd.DataFrame({\n",
    "            'town': df['town'].iloc[:5],\n",
    "            'original_price': sample_predictions['predicted_resale_price'],\n",
    "            'high_sentiment_price': high_predictions['predicted_resale_price'],\n",
    "            'low_sentiment_price': low_predictions['predicted_resale_price']\n",
    "        })\n",
    "        \n",
    "        sentiment_impact['high_sentiment_diff_pct'] = (\n",
    "            (sentiment_impact['high_sentiment_price'] - sentiment_impact['original_price']) / \n",
    "            sentiment_impact['original_price'] * 100\n",
    "        ).round(2)\n",
    "        \n",
    "        sentiment_impact['low_sentiment_diff_pct'] = (\n",
    "            (sentiment_impact['low_sentiment_price'] - sentiment_impact['original_price']) / \n",
    "            sentiment_impact['original_price'] * 100\n",
    "        ).round(2)\n",
    "        \n",
    "        print(\"\\nImpact of Sentiment Analysis on Predictions:\")\n",
    "        print(sentiment_impact[['town', 'high_sentiment_diff_pct', 'low_sentiment_diff_pct']])\n",
    "        \n",
    "        # Visualize sentiment impact\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Create positions for bars\n",
    "        towns = sentiment_impact['town'].tolist()\n",
    "        x = np.arange(len(towns))\n",
    "        width = 0.25\n",
    "        \n",
    "        # Create bars\n",
    "        plt.bar(x - width, sentiment_impact['low_sentiment_diff_pct'], width, label='Negative Sentiment Impact')\n",
    "        plt.bar(x + width, sentiment_impact['high_sentiment_diff_pct'], width, label='Positive Sentiment Impact')\n",
    "        \n",
    "        # Customize chart\n",
    "        plt.xlabel('Town')\n",
    "        plt.ylabel('Price Change (%)')\n",
    "        plt.title('Impact of Sentiment Analysis on Predicted Resale Prices')\n",
    "        plt.xticks(x, towns, rotation=45)\n",
    "        plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('sentiment_impact.png')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Print modifications info before running\n",
    "    print(\"\\n==========================================================\")\n",
    "    print(\"MODIFICATIONS IMPLEMENTED:\")\n",
    "    print(\"==========================================================\")\n",
    "    print(\"1. Cardinality-Based Encoding:\")\n",
    "    print(\"   - Features with  10 categories: One-hot encoded\")\n",
    "    print(\"   - Features with > 10 categories: Ordinal encoded\") \n",
    "    print(\"   - Expected to significantly reduce memory requirements\")\n",
    "    print(\"\\n2. Time-Based Features:\")\n",
    "    print(\"   - months_from_start: Captures inflation/long-term price trends\")\n",
    "    print(\"   - month_in_year: Captures seasonal price patterns\")\n",
    "    print(\"   - Both features transformed for optimal model usage\")\n",
    "    print(\"==========================================================\\n\")\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988898a7-bc5b-4166-b0d3-c86405f63624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Boosting (Before Data Fix on merged_data_4.csv)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import xgboost as xgb\n",
    "from joblib import dump, load\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def optimize_xgboost_final(X, y, categorical_features, numerical_features, base_metrics, use_cv=True):\n",
    "    \"\"\"\n",
    "    Optimized XGBoost implementation with stronger regularization and no PCA\n",
    "    \n",
    "    Args:\n",
    "        X: Feature DataFrame\n",
    "        y: Target series\n",
    "        categorical_features: List of categorical feature names\n",
    "        numerical_features: List of numerical feature names\n",
    "        base_metrics: Metrics from the base XGBoost model for comparison\n",
    "        use_cv: Whether to use cross-validation for evaluation\n",
    "        \n",
    "    Returns:\n",
    "        Trained model, preprocessor, metrics, and feature importances\n",
    "    \"\"\"\n",
    "    print(\"\\n========== Training Regularized XGBoost Model ==========\")\n",
    "    \n",
    "    # Create preprocessing pipeline\n",
    "    preprocessor = create_preprocessing_pipeline(categorical_features, numerical_features)\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Preprocess the data\n",
    "    print(\"Preprocessing training data...\")\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    print(\"Preprocessing test data...\")\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "    \n",
    "    # Check processed data size and shape\n",
    "    print(f\"Processed training data shape: {X_train_processed.shape}\")\n",
    "    print(f\"Processed test data shape: {X_test_processed.shape}\")\n",
    "    \n",
    "    # Parameter combinations with stronger regularization\n",
    "    param_combinations = [\n",
    "        # Base regularized model\n",
    "        {\n",
    "            'n_estimators': 300, \n",
    "            'max_depth': 6, \n",
    "            'learning_rate': 0.05, \n",
    "            'subsample': 0.7, \n",
    "            'colsample_bytree': 0.7,\n",
    "            'reg_alpha': 1.0, \n",
    "            'reg_lambda': 3.0,\n",
    "            'min_child_weight': 5,\n",
    "            'gamma': 0.1\n",
    "        },\n",
    "        \n",
    "        # Stronger regularization\n",
    "        {\n",
    "            'n_estimators': 500, \n",
    "            'max_depth': 5, \n",
    "            'learning_rate': 0.03, \n",
    "            'subsample': 0.6, \n",
    "            'colsample_bytree': 0.6,\n",
    "            'reg_alpha': 2.0, \n",
    "            'reg_lambda': 5.0,\n",
    "            'min_child_weight': 7,\n",
    "            'gamma': 0.5\n",
    "        },\n",
    "        \n",
    "        # Heavy regularization with more trees\n",
    "        {\n",
    "            'n_estimators': 800, \n",
    "            'max_depth': 4, \n",
    "            'learning_rate': 0.01, \n",
    "            'subsample': 0.6, \n",
    "            'colsample_bytree': 0.6,\n",
    "            'reg_alpha': 3.0, \n",
    "            'reg_lambda': 8.0,\n",
    "            'min_child_weight': 10,\n",
    "            'gamma': 1.0\n",
    "        },\n",
    "        \n",
    "        # Balanced approach\n",
    "        {\n",
    "            'n_estimators': 400, \n",
    "            'max_depth': 5, \n",
    "            'learning_rate': 0.05, \n",
    "            'subsample': 0.65, \n",
    "            'colsample_bytree': 0.65,\n",
    "            'reg_alpha': 1.5, \n",
    "            'reg_lambda': 4.0,\n",
    "            'min_child_weight': 6,\n",
    "            'gamma': 0.2\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    best_model = None\n",
    "    best_score = float('inf')  # For RMSE (lower is better)\n",
    "    best_params = {}\n",
    "    best_cv_score = float('inf')\n",
    "    \n",
    "    # Test each combination\n",
    "    for i, params in enumerate(param_combinations):\n",
    "        print(f\"\\nTrying combination {i+1}/{len(param_combinations)}:\")\n",
    "        print(params)\n",
    "        \n",
    "        # Create model with current parameters\n",
    "        model = xgb.XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            random_state=42,\n",
    "            tree_method='hist',  # Memory efficient algorithm\n",
    "            **params\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Use cross-validation if enabled\n",
    "            if use_cv:\n",
    "                cv_scores = cross_val_score(\n",
    "                    model, \n",
    "                    X_train_processed, \n",
    "                    y_train, \n",
    "                    cv=5, \n",
    "                    scoring='neg_root_mean_squared_error',\n",
    "                    n_jobs=-1\n",
    "                )\n",
    "                cv_rmse = -np.mean(cv_scores)\n",
    "                print(f\"Cross-validation RMSE: {cv_rmse:.2f} ({np.std(-cv_scores):.2f})\")\n",
    "                \n",
    "                # Record if this is the best CV score\n",
    "                if cv_rmse < best_cv_score:\n",
    "                    best_cv_score = cv_rmse\n",
    "                    print(f\"New best CV score! RMSE: {cv_rmse:.2f}\")\n",
    "            \n",
    "            # For early stopping, we need to create a validation set\n",
    "            X_train_fit, X_val, y_train_fit, y_val = train_test_split(\n",
    "                X_train_processed, y_train, test_size=0.2, random_state=42\n",
    "            )\n",
    "            \n",
    "            # Create DMatrix objects for early stopping\n",
    "            dtrain = xgb.DMatrix(X_train_fit, label=y_train_fit)\n",
    "            dval = xgb.DMatrix(X_val, label=y_val)\n",
    "            \n",
    "            # Create parameter dictionary from regressor parameters\n",
    "            param_dict = model.get_params()\n",
    "            \n",
    "            # Remove scikit-learn specific parameters\n",
    "            for key in ['n_estimators', 'n_jobs', 'verbosity']:\n",
    "                if key in param_dict:\n",
    "                    del param_dict[key]\n",
    "            \n",
    "            # Convert learning_rate to eta for XGBoost API\n",
    "            if 'learning_rate' in param_dict:\n",
    "                param_dict['eta'] = param_dict['learning_rate']\n",
    "                del param_dict['learning_rate']\n",
    "                \n",
    "            # Use XGBoost's native API for training with early stopping\n",
    "            evallist = [(dtrain, 'train'), (dval, 'validation')]\n",
    "            \n",
    "            # Train using the native XGBoost API with early stopping\n",
    "            num_round = params['n_estimators']\n",
    "            bst = xgb.train(\n",
    "                params=param_dict,\n",
    "                dtrain=dtrain,\n",
    "                num_boost_round=num_round,\n",
    "                evals=evallist,\n",
    "                early_stopping_rounds=50,\n",
    "                verbose_eval=100\n",
    "            )\n",
    "            \n",
    "            # Get the best iteration\n",
    "            best_iteration = bst.best_iteration if hasattr(bst, 'best_iteration') else num_round\n",
    "            print(f\"Best iteration: {best_iteration}\")\n",
    "            \n",
    "            # Create a new model with the best iteration\n",
    "            best_iteration_model = xgb.XGBRegressor(\n",
    "                objective='reg:squarederror',\n",
    "                random_state=42,\n",
    "                tree_method='hist',\n",
    "                n_estimators=best_iteration,\n",
    "                **{k: v for k, v in params.items() if k != 'n_estimators'}\n",
    "            )\n",
    "            \n",
    "            # Fit on the full training data\n",
    "            best_iteration_model.fit(X_train_processed, y_train)\n",
    "            model = best_iteration_model\n",
    "            \n",
    "            # Evaluate\n",
    "            y_pred = model.predict(X_test_processed)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            \n",
    "            print(f\"Test RMSE: {rmse:.2f}\")\n",
    "            \n",
    "            # Check if this is the best model so far (based on test RMSE)\n",
    "            if rmse < best_score:\n",
    "                best_score = rmse\n",
    "                best_model = model\n",
    "                best_params = params.copy()\n",
    "                best_params['n_estimators'] = best_iteration\n",
    "                print(f\"New best test score! RMSE: {rmse:.2f}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error training model: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # If no successful models, use a default configuration with basic fit\n",
    "    if best_model is None:\n",
    "        print(\"No successful models were trained. Using a default model as fallback.\")\n",
    "        default_params = {\n",
    "            'n_estimators': 300,\n",
    "            'max_depth': 5,\n",
    "            'learning_rate': 0.05,\n",
    "            'subsample': 0.7,\n",
    "            'colsample_bytree': 0.7,\n",
    "            'reg_alpha': 1.0,\n",
    "            'reg_lambda': 3.0,\n",
    "            'tree_method': 'hist',\n",
    "            'objective': 'reg:squarederror',\n",
    "            'random_state': 42\n",
    "        }\n",
    "        best_model = xgb.XGBRegressor(**default_params)\n",
    "        # Fit without early stopping\n",
    "        best_model.fit(X_train_processed, y_train)\n",
    "        best_params = default_params\n",
    "    \n",
    "    # Final evaluation\n",
    "    y_pred = best_model.predict(X_test_processed)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    metrics = {\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2\n",
    "    }\n",
    "    \n",
    "    print(\"\\nRegularized XGBoost Final Metrics:\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"R2 Score: {r2:.4f}\")\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    \n",
    "    # Compare with base model\n",
    "    improvement_rmse = ((base_metrics['rmse'] - rmse) / base_metrics['rmse']) * 100\n",
    "    improvement_mae = ((base_metrics['mae'] - mae) / base_metrics['mae']) * 100\n",
    "    improvement_r2 = ((r2 - base_metrics['r2']) / base_metrics['r2']) * 100\n",
    "    \n",
    "    print(\"\\nImprovement over base XGBoost model:\")\n",
    "    print(f\"RMSE: {improvement_rmse:.2f}%\")\n",
    "    print(f\"MAE: {improvement_mae:.2f}%\")\n",
    "    print(f\"R2: {improvement_r2:.2f}%\")\n",
    "    \n",
    "    # Try to get feature names\n",
    "    try:\n",
    "        feature_names = preprocessor.get_feature_names_out()\n",
    "    except:\n",
    "        feature_names = [f\"feature_{i}\" for i in range(X_train_processed.shape[1])]\n",
    "    \n",
    "    # Handle potential mismatch in feature dimensions\n",
    "    if len(feature_names) != len(best_model.feature_importances_):\n",
    "        print(f\"Warning: Feature names count ({len(feature_names)}) doesn't match model features count ({len(best_model.feature_importances_)})\")\n",
    "        feature_names = [f\"feature_{i}\" for i in range(len(best_model.feature_importances_))]\n",
    "    \n",
    "    # Get feature importance\n",
    "    feature_importance = analyze_feature_importance(best_model, feature_names)\n",
    "    \n",
    "    # Try feature selection with the best model\n",
    "    print(\"\\nTrying feature selection to further improve model...\")\n",
    "    try:\n",
    "        # Create a feature selector based on the best model\n",
    "        selector = SelectFromModel(best_model, threshold='median', prefit=True)\n",
    "        \n",
    "        # Apply feature selection\n",
    "        X_train_selected = selector.transform(X_train_processed)\n",
    "        X_test_selected = selector.transform(X_test_processed)\n",
    "        \n",
    "        print(f\"Selected {X_train_selected.shape[1]} out of {X_train_processed.shape[1]} features\")\n",
    "        \n",
    "        # Train a new model on selected features\n",
    "        selected_model = xgb.XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            random_state=42,\n",
    "            tree_method='hist',\n",
    "            **{k: v for k, v in best_params.items() if k != 'n_estimators'}\n",
    "        )\n",
    "        \n",
    "        # For early stopping with feature selection, use the native API again\n",
    "        X_train_fit_sel, X_val_sel, y_train_fit_sel, y_val_sel = train_test_split(\n",
    "            X_train_selected, y_train, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Create DMatrix objects\n",
    "        dtrain_sel = xgb.DMatrix(X_train_fit_sel, label=y_train_fit_sel)\n",
    "        dval_sel = xgb.DMatrix(X_val_sel, label=y_val_sel)\n",
    "        \n",
    "        # Create parameter dictionary\n",
    "        param_dict_sel = selected_model.get_params()\n",
    "        \n",
    "        # Remove scikit-learn specific parameters\n",
    "        for key in ['n_estimators', 'n_jobs', 'verbosity']:\n",
    "            if key in param_dict_sel:\n",
    "                del param_dict_sel[key]\n",
    "        \n",
    "        # Convert learning_rate to eta\n",
    "        if 'learning_rate' in param_dict_sel:\n",
    "            param_dict_sel['eta'] = param_dict_sel['learning_rate']\n",
    "            del param_dict_sel['learning_rate']\n",
    "        \n",
    "        # Evaluation list\n",
    "        evallist_sel = [(dtrain_sel, 'train'), (dval_sel, 'validation')]\n",
    "        \n",
    "        # Train with early stopping\n",
    "        try:\n",
    "            num_round_sel = best_params['n_estimators']\n",
    "            bst_sel = xgb.train(\n",
    "                params=param_dict_sel,\n",
    "                dtrain=dtrain_sel,\n",
    "                num_boost_round=num_round_sel,\n",
    "                evals=evallist_sel,\n",
    "                early_stopping_rounds=50,\n",
    "                verbose_eval=100\n",
    "            )\n",
    "            \n",
    "            # Get best iteration\n",
    "            best_iteration_sel = bst_sel.best_iteration if hasattr(bst_sel, 'best_iteration') else num_round_sel\n",
    "            \n",
    "            # Create final model with best iteration\n",
    "            selected_model = xgb.XGBRegressor(\n",
    "                objective='reg:squarederror',\n",
    "                random_state=42,\n",
    "                tree_method='hist',\n",
    "                n_estimators=best_iteration_sel,\n",
    "                **{k: v for k, v in best_params.items() if k != 'n_estimators'}\n",
    "            )\n",
    "            \n",
    "            # Fit on all training data\n",
    "            selected_model.fit(X_train_selected, y_train)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during feature selection with early stopping: {e}\")\n",
    "            print(\"Falling back to basic fit without early stopping\")\n",
    "            selected_model.fit(X_train_selected, y_train)\n",
    "        \n",
    "        # Evaluate selected model\n",
    "        y_pred_selected = selected_model.predict(X_test_selected)\n",
    "        rmse_selected = np.sqrt(mean_squared_error(y_test, y_pred_selected))\n",
    "        mae_selected = mean_absolute_error(y_test, y_pred_selected)\n",
    "        r2_selected = r2_score(y_test, y_pred_selected)\n",
    "        \n",
    "        print(\"\\nFeature-Selected Model Metrics:\")\n",
    "        print(f\"RMSE: {rmse_selected:.2f}\")\n",
    "        print(f\"MAE: {mae_selected:.2f}\")\n",
    "        print(f\"R2 Score: {r2_selected:.4f}\")\n",
    "        \n",
    "        # Compare with non-feature-selected model\n",
    "        if rmse_selected < rmse:\n",
    "            print(\"Feature selection improved performance!\")\n",
    "            best_model = selected_model\n",
    "            metrics = {\n",
    "                'rmse': rmse_selected,\n",
    "                'mae': mae_selected,\n",
    "                'r2': r2_selected\n",
    "            }\n",
    "            # Updated improvement metrics\n",
    "            improvement_rmse = ((base_metrics['rmse'] - rmse_selected) / base_metrics['rmse']) * 100\n",
    "            improvement_mae = ((base_metrics['mae'] - mae_selected) / base_metrics['mae']) * 100\n",
    "            improvement_r2 = ((r2_selected - base_metrics['r2']) / base_metrics['r2']) * 100\n",
    "            \n",
    "            print(\"\\nImprovement over base XGBoost model (with feature selection):\")\n",
    "            print(f\"RMSE: {improvement_rmse:.2f}%\")\n",
    "            print(f\"MAE: {improvement_mae:.2f}%\")\n",
    "            print(f\"R2: {improvement_r2:.2f}%\")\n",
    "        else:\n",
    "            print(\"Feature selection did not improve performance.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during feature selection: {e}\")\n",
    "    \n",
    "    # Sample data points for plotting to avoid memory issues\n",
    "    sample_size = min(5000, len(y_test))\n",
    "    indices = np.random.choice(len(y_test), sample_size, replace=False)\n",
    "    \n",
    "    # Plot actual vs predicted\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test.iloc[indices], y_pred[indices], alpha=0.5)\n",
    "    plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')\n",
    "    plt.xlabel('Actual Prices')\n",
    "    plt.ylabel('Predicted Prices')\n",
    "    plt.title('Regularized XGBoost: Actual vs Predicted Prices')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('xgboost_regularized_results.png')\n",
    "    \n",
    "    return best_model, preprocessor, metrics, feature_importance\n",
    "\n",
    "\n",
    "def create_preprocessing_pipeline(categorical_features, numerical_features):\n",
    "    \"\"\"\n",
    "    Create a preprocessing pipeline with memory efficiency in mind\n",
    "    \n",
    "    Args:\n",
    "        categorical_features: List of categorical feature names\n",
    "        numerical_features: List of numerical feature names\n",
    "        \n",
    "    Returns:\n",
    "        Scikit-learn preprocessing pipeline\n",
    "    \"\"\"\n",
    "    # Handle high cardinality features differently to save memory\n",
    "    high_cardinality_features = []\n",
    "    low_cardinality_features = []\n",
    "    \n",
    "    # Manually identify high cardinality features\n",
    "    high_cardinality_candidates = ['town', 'street_name', 'flat_model', 'nearest_mrt']\n",
    "    \n",
    "    for feature in categorical_features:\n",
    "        if feature in high_cardinality_candidates:\n",
    "            high_cardinality_features.append(feature)\n",
    "        else:\n",
    "            low_cardinality_features.append(feature)\n",
    "    \n",
    "    print(f\"High cardinality features: {high_cardinality_features}\")\n",
    "    print(f\"Low cardinality features: {low_cardinality_features}\")\n",
    "    \n",
    "    # Create transformers list\n",
    "    transformers = []\n",
    "    \n",
    "    # One-hot encoding for low-cardinality features\n",
    "    if low_cardinality_features:\n",
    "        transformers.append(\n",
    "            ('ohe', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                ('encoder', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
    "            ]), low_cardinality_features)\n",
    "        )\n",
    "    \n",
    "    # Ordinal encoding for high-cardinality features\n",
    "    if high_cardinality_features:\n",
    "        transformers.append(\n",
    "            ('ordinal', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "            ]), high_cardinality_features)\n",
    "        )\n",
    "    \n",
    "    # Standard scaling for numerical features\n",
    "    if numerical_features:\n",
    "        transformers.append(\n",
    "            ('num', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='median')),\n",
    "                ('scaler', StandardScaler())\n",
    "            ]), numerical_features)\n",
    "        )\n",
    "    \n",
    "    # Create the column transformer\n",
    "    try:\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=transformers,\n",
    "            remainder='drop',\n",
    "            verbose_feature_names_out=True\n",
    "        )\n",
    "    except TypeError:\n",
    "        # For older sklearn versions\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=transformers,\n",
    "            remainder='drop'\n",
    "        )\n",
    "    \n",
    "    return preprocessor\n",
    "\n",
    "def analyze_feature_importance(model, feature_names):\n",
    "    \"\"\"\n",
    "    Analyze feature importance from the XGBoost model\n",
    "    \n",
    "    Args:\n",
    "        model: Trained XGBoost model\n",
    "        feature_names: List of feature names\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with feature importance\n",
    "    \"\"\"\n",
    "    # Get feature importance\n",
    "    importance = model.feature_importances_\n",
    "    \n",
    "    # Create DataFrame with feature names\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importance\n",
    "    })\n",
    "    \n",
    "    # Sort by importance\n",
    "    feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Plot top 20 features (or fewer if less are available)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    top_n = min(20, len(feature_importance))\n",
    "    top_features = feature_importance.head(top_n)\n",
    "    sns.barplot(x='importance', y='feature', data=top_features)\n",
    "    plt.title(f'Regularized XGBoost Feature Importance (Top {top_n})')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('xgboost_regularized_feature_importance.png')\n",
    "    \n",
    "    # Print top important features\n",
    "    print(f\"\\nTop {top_n} important features:\")\n",
    "    for index, row in top_features.iterrows():\n",
    "        print(f\"{row['feature']}: {row['importance']:.4f}\")\n",
    "    \n",
    "    return feature_importance\n",
    "\n",
    "def compare_model_versions(metrics_dict):\n",
    "    \"\"\"\n",
    "    Compare the performance of different model versions\n",
    "    \n",
    "    Args:\n",
    "        metrics_dict: Dictionary with model names as keys and their metrics as values\n",
    "    \"\"\"\n",
    "    models = list(metrics_dict.keys())\n",
    "    rmse_values = [metrics_dict[model]['rmse'] for model in models]\n",
    "    mae_values = [metrics_dict[model]['mae'] for model in models]\n",
    "    r2_values = [metrics_dict[model]['r2'] for model in models]\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    \n",
    "    # For RMSE (lower is better)\n",
    "    plt.subplot(1, 3, 1)\n",
    "    ax = sns.barplot(x=models, y=rmse_values)\n",
    "    plt.title('RMSE Comparison (lower is better)')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, p in enumerate(ax.patches):\n",
    "        ax.annotate(f'{p.get_height():.0f}', \n",
    "                   (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                   ha = 'center', va = 'bottom',\n",
    "                   xytext = (0, 5), textcoords = 'offset points')\n",
    "    \n",
    "    # For MAE (lower is better)\n",
    "    plt.subplot(1, 3, 2)\n",
    "    ax = sns.barplot(x=models, y=mae_values)\n",
    "    plt.title('MAE Comparison (lower is better)')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, p in enumerate(ax.patches):\n",
    "        ax.annotate(f'{p.get_height():.0f}', \n",
    "                   (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                   ha = 'center', va = 'bottom',\n",
    "                   xytext = (0, 5), textcoords = 'offset points')\n",
    "    \n",
    "    # For R (higher is better)\n",
    "    plt.subplot(1, 3, 3)\n",
    "    ax = sns.barplot(x=models, y=r2_values)\n",
    "    plt.title('R Score Comparison (higher is better)')\n",
    "    plt.ylabel('R')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, p in enumerate(ax.patches):\n",
    "        ax.annotate(f'{p.get_height():.4f}', \n",
    "                   (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                   ha = 'center', va = 'bottom',\n",
    "                   xytext = (0, 5), textcoords = 'offset points')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_version_comparison.png')\n",
    "    \n",
    "    # Print comparison table\n",
    "    print(\"\\nModel Version Comparison:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Model':<20} {'RMSE':>10} {'MAE':>10} {'R':>10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for model in models:\n",
    "        print(f\"{model:<20} {metrics_dict[model]['rmse']:>10.2f} {metrics_dict[model]['mae']:>10.2f} \"\n",
    "              f\"{metrics_dict[model]['r2']:>10.4f}\")\n",
    "    \n",
    "    # Calculate improvements over baseline (Linear Regression)\n",
    "    baseline_model = 'Linear Regression'\n",
    "    if baseline_model in metrics_dict:\n",
    "        print(\"\\nImprovement over Linear Regression baseline:\")\n",
    "        baseline_rmse = metrics_dict[baseline_model]['rmse']\n",
    "        baseline_mae = metrics_dict[baseline_model]['mae']\n",
    "        baseline_r2 = metrics_dict[baseline_model]['r2']\n",
    "        \n",
    "        print(\"-\" * 80)\n",
    "        print(f\"{'Model':<20} {'RMSE Imp%':>15} {'MAE Imp%':>15} {'R Imp%':>15}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for model in models:\n",
    "            if model == baseline_model:\n",
    "                continue\n",
    "                \n",
    "            rmse_imp = ((baseline_rmse - metrics_dict[model]['rmse']) / baseline_rmse) * 100\n",
    "            mae_imp = ((baseline_mae - metrics_dict[model]['mae']) / baseline_mae) * 100\n",
    "            r2_imp = ((metrics_dict[model]['r2'] - baseline_r2) / baseline_r2) * 100\n",
    "            \n",
    "            print(f\"{model:<20} {rmse_imp:>15.2f}% {mae_imp:>15.2f}% {r2_imp:>15.2f}%\")\n",
    "\n",
    "def save_model(model, preprocessor, feature_importance, metrics, filename_prefix='xgboost_regularized'):\n",
    "    \"\"\"\n",
    "    Save model artifacts for future use\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        preprocessor: Fitted preprocessor\n",
    "        feature_importance: DataFrame with feature importance\n",
    "        metrics: Dictionary with performance metrics\n",
    "        filename_prefix: Prefix for saved files\n",
    "    \"\"\"\n",
    "    # Save model\n",
    "    dump(model, f'{filename_prefix}_model.joblib')\n",
    "    \n",
    "    # Save preprocessor\n",
    "    dump(preprocessor, f'{filename_prefix}_preprocessor.joblib')\n",
    "    \n",
    "    # Save feature importance\n",
    "    feature_importance.to_csv(f'{filename_prefix}_feature_importance.csv', index=False)\n",
    "    \n",
    "    # Save metrics\n",
    "    with open(f'{filename_prefix}_metrics.txt', 'w') as f:\n",
    "        f.write(\"Performance Metrics:\\n\")\n",
    "        f.write(f\"RMSE: {metrics['rmse']:.2f}\\n\")\n",
    "        f.write(f\"MAE: {metrics['mae']:.2f}\\n\")\n",
    "        f.write(f\"R2: {metrics['r2']:.4f}\\n\")\n",
    "    \n",
    "    print(f\"\\nModel and artifacts saved with prefix '{filename_prefix}'\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the improved XGBoost training and evaluation\n",
    "    \"\"\"\n",
    "    # Assuming we have the base XGBoost metrics from your previous run\n",
    "    base_xgboost_metrics = {\n",
    "        'rmse': 25266.07,\n",
    "        'mae': 18066.53,\n",
    "        'r2': 0.9801\n",
    "    }\n",
    "    \n",
    "    # Call functions to load and prepare data (these functions should be defined elsewhere)\n",
    "    df = load_and_preprocess_data('merged_data_3.csv')\n",
    "    X, y, categorical_features, numerical_features = prepare_features(df)\n",
    "    \n",
    "    # Print shape information\n",
    "    print(f\"Input data shape: X: {X.shape}, y: {y.shape}\")\n",
    "    print(f\"Categorical features: {len(categorical_features)}\")\n",
    "    print(f\"Numerical features: {len(numerical_features)}\")\n",
    "    \n",
    "    # Train optimized XGBoost with regularization\n",
    "    xgb_reg_model, xgb_reg_preprocessor, xgb_reg_metrics, feature_importance = optimize_xgboost_final(\n",
    "        X, y, \n",
    "        categorical_features, \n",
    "        numerical_features, \n",
    "        base_xgboost_metrics,\n",
    "        use_cv=True\n",
    "    )\n",
    "    \n",
    "    # Compare model versions\n",
    "    all_metrics = {\n",
    "        'Linear Regression': {'rmse': 86357.11, 'mae': 66020.02, 'r2': 0.7681},\n",
    "        'XGBoost V1': base_xgboost_metrics,\n",
    "        'DNN': {'rmse': 42849.78, 'mae': 31954.07, 'r2': 0.9429},\n",
    "        'XGBoost V2': {'rmse': 38604.96, 'mae': 27393.89, 'r2': 0.9536},\n",
    "        'XGBoost Regularized': xgb_reg_metrics\n",
    "    }\n",
    "    \n",
    "    compare_model_versions(all_metrics)\n",
    "    \n",
    "    # Save the model\n",
    "    save_model(xgb_reg_model, xgb_reg_preprocessor, feature_importance, xgb_reg_metrics)\n",
    "    \n",
    "    # Print final recommendation\n",
    "    print(\"\\nFinal Model Recommendation:\")\n",
    "    best_model = min(all_metrics, key=lambda x: all_metrics[x]['rmse'])\n",
    "    print(f\"Based on RMSE: {best_model}\")\n",
    "    \n",
    "    best_model_r2 = max(all_metrics, key=lambda x: all_metrics[x]['r2'])\n",
    "    print(f\"Based on R: {best_model_r2}\")\n",
    "    \n",
    "    print(\"\\nTop features for predicting housing prices:\")\n",
    "    for index, row in feature_importance.head(10).iterrows():\n",
    "        print(f\"  - {row['feature']}: {row['importance']:.4f}\")\n",
    "    \n",
    "    # Return values for further analysis if needed\n",
    "    return xgb_reg_model, xgb_reg_preprocessor, xgb_reg_metrics, feature_importance\n",
    "\n",
    "# These functions would need to be implemented or imported\n",
    "def load_and_preprocess_data(filepath, reddit_sentiment_filepath=None):\n",
    "    \"\"\"\n",
    "    Load and preprocess the housing data\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the housing dataset CSV\n",
    "        reddit_sentiment_filepath: Optional path to the Reddit sentiment scores CSV\n",
    "        \n",
    "    Returns:\n",
    "        Preprocessed DataFrame\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Display info about the dataset\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(\"\\nData types:\")\n",
    "    print(df.dtypes)\n",
    "    print(\"\\nMissing values:\")\n",
    "    print(df.isnull().sum())\n",
    "    \n",
    "    # Specialized handler for 'YYYY-MM' formatted columns\n",
    "    def parse_yearmonth_column(df, column_name):\n",
    "        \"\"\"\n",
    "        Parse YYYY-MM formatted columns into cyclic time features\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame containing the column\n",
    "            column_name: Name of the column to parse\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with additional time-based features\n",
    "        \"\"\"\n",
    "        # Ensure the column is in string format\n",
    "        df[column_name] = df[column_name].astype(str)\n",
    "        \n",
    "        # Parse year and month\n",
    "        df['year'] = df[column_name].str[:4].astype(int)\n",
    "        df['month'] = df[column_name].str[5:].astype(int)\n",
    "        \n",
    "        # Find the earliest date to calculate months from start\n",
    "        earliest_year = df['year'].min()\n",
    "        earliest_month = df.loc[df['year'] == earliest_year, 'month'].min()\n",
    "        \n",
    "        # Calculate months from start\n",
    "        df['months_from_start'] = ((df['year'] - earliest_year) * 12 + \n",
    "                                   (df['month'] - earliest_month))\n",
    "        \n",
    "        # Create cyclic month features\n",
    "        df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "        df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    # Check for 'YYYY-MM' formatted columns\n",
    "    yearmonth_columns = [col for col in df.columns if \n",
    "        df[col].dtype == 'object' and \n",
    "        df[col].dropna().apply(lambda x: isinstance(x, str) and len(x) == 7 and x[4] == '-').all()]\n",
    "    \n",
    "    if yearmonth_columns:\n",
    "        print(f\"\\nDetected YYYY-MM formatted column(s): {yearmonth_columns}\")\n",
    "        for col in yearmonth_columns:\n",
    "            df = parse_yearmonth_column(df, col)\n",
    "    \n",
    "    # Basic preprocessing\n",
    "    \n",
    "    # Extract numeric value from storey_range (e.g., '01 TO 03' -> 2 as the middle value)\n",
    "    if 'storey_range' in df.columns:\n",
    "        df['storey_median'] = df['storey_range'].apply(\n",
    "            lambda x: np.mean([int(i) for i in re.findall(r'\\d+', x)])\n",
    "        )\n",
    "    \n",
    "    # Calculate remaining lease more precisely (if not already available)\n",
    "    if 'remaining_lease' not in df.columns or df['remaining_lease'].isnull().any():\n",
    "        # Check the format of remaining_lease (if it exists)\n",
    "        if 'remaining_lease' in df.columns:\n",
    "            # Check a sample value to see if it needs conversion\n",
    "            sample = df['remaining_lease'].iloc[0] if not df['remaining_lease'].isnull().all() else None\n",
    "            if sample and isinstance(sample, str) and 'years' in sample:\n",
    "                # Extract numeric value from strings like \"61 years 06 months\"\n",
    "                df['remaining_lease'] = df['remaining_lease'].apply(\n",
    "                    lambda x: float(x.split('years')[0].strip()) if isinstance(x, str) else x\n",
    "                )\n",
    "        elif 'lease_commence_date' in df.columns:\n",
    "            current_year = 2025  # Update this as needed\n",
    "            df['remaining_lease'] = df['lease_commence_date'].apply(\n",
    "                lambda x: (x + 99) - current_year if pd.notna(x) else np.nan\n",
    "            )\n",
    "    \n",
    "    # Convert sentiment columns if they exist\n",
    "    sentiment_cols = [col for col in df.columns if 'sentiment' in col]\n",
    "    for col in sentiment_cols:\n",
    "        if col in df.columns:\n",
    "            # Ensure sentiment scores are numeric\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Create proximity features from MRT data if available\n",
    "    if 'mrt_distance_km' in df.columns:\n",
    "        df['mrt_accessibility'] = 1 / (1.0 + df['mrt_distance_km'])  # Higher value means closer to MRT\n",
    "    \n",
    "    # Normalize walking time with distance to create a walking convenience score\n",
    "    if 'walking_time_min' in df.columns and 'walking_distance_m' in df.columns:\n",
    "        # Lower values are better (less time per distance)\n",
    "        df['walking_convenience'] = df['walking_distance_m'] / (df['walking_time_min'] * 60 + 0.1)  # Added 0.1 to avoid division by zero\n",
    "        \n",
    "    # Age of the flat might be important\n",
    "    if 'lease_commence_date' in df.columns:\n",
    "        df['flat_age'] = 2025 - df['lease_commence_date']  # Update the year as needed\n",
    "    \n",
    "    # Make sure to reuse existing time-based features if created earlier\n",
    "    if 'month' in df.columns and 'year' in df.columns:\n",
    "        # Create a reference date to measure months from start if not already created\n",
    "        if 'months_from_start' not in df.columns:\n",
    "            earliest_year = df['year'].min()\n",
    "            earliest_month = df.loc[df['year'] == earliest_year, 'month'].min()\n",
    "            \n",
    "            # Calculate months from start (for inflation trends)\n",
    "            df['months_from_start'] = (df['year'] - earliest_year) * 12 + (df['month'] - earliest_month)\n",
    "        \n",
    "        # Extract month for seasonal trends\n",
    "        df['month_in_year'] = df['month']\n",
    "    elif 'transaction_date' in df.columns:\n",
    "        # Parse transaction dates\n",
    "        df['transaction_date'] = pd.to_datetime(df['transaction_date'])\n",
    "        \n",
    "        # Find the earliest date\n",
    "        earliest_date = df['transaction_date'].min()\n",
    "        \n",
    "        # Calculate months from start (capture inflation trends)\n",
    "        df['months_from_start'] = ((df['transaction_date'].dt.year - earliest_date.year) * 12 + \n",
    "                                  (df['transaction_date'].dt.month - earliest_date.month))\n",
    "        \n",
    "        # Extract month for seasonal trends\n",
    "        df['month_in_year'] = df['transaction_date'].dt.month\n",
    "    elif 'resale_registration_date' in df.columns:\n",
    "        # Try using registration date if available\n",
    "        try:\n",
    "            df['resale_registration_date'] = pd.to_datetime(df['resale_registration_date'])\n",
    "            \n",
    "            # Find the earliest date\n",
    "            earliest_date = df['resale_registration_date'].min()\n",
    "            \n",
    "            # Calculate months from start\n",
    "            df['months_from_start'] = ((df['resale_registration_date'].dt.year - earliest_date.year) * 12 + \n",
    "                                      (df['resale_registration_date'].dt.month - earliest_date.month))\n",
    "            \n",
    "            # Extract month for seasonal trends\n",
    "            df['month_in_year'] = df['resale_registration_date'].dt.month\n",
    "        except:\n",
    "            print(\"Could not convert resale_registration_date. Creating artificial time features.\")\n",
    "            # If conversion fails, create artificial time features\n",
    "            df['months_from_start'] = range(len(df))  # Just an incremental counter as proxy\n",
    "            df['month_in_year'] = np.random.randint(1, 13, size=len(df))  # Random month as placeholder\n",
    "    else:\n",
    "        # If no date columns found, create artificial features\n",
    "        print(\"No date columns found. Creating artificial time features.\")\n",
    "        df['months_from_start'] = range(len(df))  # Just an incremental counter as proxy\n",
    "        df['month_in_year'] = np.random.randint(1, 13, size=len(df))  # Random month as placeholder\n",
    "        \n",
    "    # Check if time-based features were successfully created\n",
    "    if 'months_from_start' in df.columns and 'month_in_year' in df.columns:\n",
    "        print(f\"Created time-based features: months_from_start and month_in_year\")\n",
    "    else:\n",
    "        print(f\"Warning: Failed to create time-based features\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_features(df, target_col='resale_price'):\n",
    "    \"\"\"\n",
    "    Prepare features and target for modeling\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with raw data\n",
    "        target_col: Name of target column\n",
    "        \n",
    "    Returns:\n",
    "        X, y, categorical_features, numerical_features\n",
    "    \"\"\"\n",
    "    # Separate target\n",
    "    y = df[target_col]\n",
    "    X = df.drop(columns=[target_col])\n",
    "    \n",
    "    # Identify categorical and numerical features\n",
    "    categorical_features = []\n",
    "    numerical_features = []\n",
    "    \n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == 'object' or X[col].dtype == 'category':\n",
    "            categorical_features.append(col)\n",
    "        elif X[col].dtype in ['int64', 'float64']:\n",
    "            numerical_features.append(col)\n",
    "    \n",
    "    print(f\"Target: {target_col}\")\n",
    "    print(f\"Categorical features ({len(categorical_features)}): {categorical_features[:5]}...\")\n",
    "    print(f\"Numerical features ({len(numerical_features)}): {numerical_features[:5]}...\")\n",
    "    \n",
    "    return X, y, categorical_features, numerical_features\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b23cbd7-e454-404a-b81e-91972caf033e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct ML across 3 different models, (1) Multivariate Linear Regression (2) XGBoost Gradient Boosting Algorithm (3) DNN Architectures \n",
    "# Wrong data encoding for blocks and months \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import re\n",
    "\n",
    "def load_and_preprocess_data(filepath, reddit_sentiment_filepath=None):\n",
    "    \"\"\"\n",
    "    Load and preprocess the housing data\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the housing dataset CSV\n",
    "        reddit_sentiment_filepath: Optional path to the Reddit sentiment scores CSV\n",
    "        \n",
    "    Returns:\n",
    "        Preprocessed DataFrame\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Display info about the dataset\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(\"\\nData types:\")\n",
    "    print(df.dtypes)\n",
    "    print(\"\\nMissing values:\")\n",
    "    print(df.isnull().sum())\n",
    "    \n",
    "    # Basic preprocessing\n",
    "    \n",
    "    # Extract numeric value from storey_range (e.g., '01 TO 03' -> 2 as the middle value)\n",
    "    df['storey_median'] = df['storey_range'].apply(\n",
    "        lambda x: np.mean([int(i) for i in re.findall(r'\\d+', x)])\n",
    "    )\n",
    "    \n",
    "    # Calculate remaining lease more precisely (if not already available)\n",
    "    if 'remaining_lease' not in df.columns or df['remaining_lease'].isnull().any():\n",
    "        # Check the format of remaining_lease (if it exists)\n",
    "        if 'remaining_lease' in df.columns:\n",
    "            # Check a sample value to see if it needs conversion\n",
    "            sample = df['remaining_lease'].iloc[0] if not df['remaining_lease'].isnull().all() else None\n",
    "            if sample and isinstance(sample, str) and 'years' in sample:\n",
    "                # Extract numeric value from strings like \"61 years 06 months\"\n",
    "                df['remaining_lease'] = df['remaining_lease'].apply(\n",
    "                    lambda x: float(x.split('years')[0].strip()) if isinstance(x, str) else x\n",
    "                )\n",
    "        else:\n",
    "            current_year = 2025  # Update this as needed\n",
    "            df['remaining_lease'] = df['lease_commence_date'].apply(\n",
    "                lambda x: (x + 99) - current_year if pd.notna(x) else np.nan\n",
    "            )\n",
    "    \n",
    "    # Convert sentiment columns if they exist\n",
    "    sentiment_cols = [col for col in df.columns if 'sentiment' in col]\n",
    "    for col in sentiment_cols:\n",
    "        if col in df.columns:\n",
    "            # Ensure sentiment scores are numeric\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Create proximity features from MRT data if available\n",
    "    if 'mrt_distance_km' in df.columns:\n",
    "        df['mrt_accessibility'] = 1 / (1 + df['mrt_distance_km'])  # Higher value means closer to MRT\n",
    "    \n",
    "    # Normalize walking time with distance to create a walking convenience score\n",
    "    if 'walking_time_min' in df.columns and 'walking_distance_m' in df.columns:\n",
    "        # Lower values are better (less time per distance)\n",
    "        df['walking_convenience'] = df['walking_distance_m'] / (df['walking_time_min'] * 60)\n",
    "        \n",
    "    # Age of the flat might be important\n",
    "    df['flat_age'] = 2025 - df['lease_commence_date']  # Update the year as needed\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_features(df, target='resale_price'):\n",
    "    \"\"\"\n",
    "    Prepare features for modeling\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        target: Target variable name\n",
    "        \n",
    "    Returns:\n",
    "        X, y, and feature information\n",
    "    \"\"\"\n",
    "    # Separate features and target\n",
    "    y = df[target]\n",
    "    X = df.drop(target, axis=1)\n",
    "    \n",
    "    # Identify categorical and numerical features\n",
    "    categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "    numerical_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    \n",
    "    # Remove any ID columns or other non-predictive features\n",
    "    features_to_drop = ['street_name', 'storey_range']  # Add others as needed\n",
    "    for feature in features_to_drop:\n",
    "        if feature in numerical_features:\n",
    "            numerical_features.remove(feature)\n",
    "        if feature in categorical_features:\n",
    "            categorical_features.remove(feature)\n",
    "    \n",
    "    # Make sure longitude and latitude are in numerical features if they exist\n",
    "    for feature in ['longitude', 'latitude']:\n",
    "        if feature in X.columns and feature not in numerical_features:\n",
    "            numerical_features.append(feature)\n",
    "    \n",
    "    # Drop the features from X\n",
    "    X = X.drop(features_to_drop, axis=1)\n",
    "    \n",
    "    # Update categorical and numerical features lists\n",
    "    categorical_features = [f for f in categorical_features if f in X.columns]\n",
    "    numerical_features = [f for f in numerical_features if f in X.columns]\n",
    "    \n",
    "    return X, y, categorical_features, numerical_features\n",
    "\n",
    "def create_preprocessing_pipeline(categorical_features, numerical_features, apply_pca=False, pca_components=0.95):\n",
    "    \"\"\"\n",
    "    Create a preprocessing pipeline with optional PCA\n",
    "    \n",
    "    Args:\n",
    "        categorical_features: List of categorical feature names\n",
    "        numerical_features: List of numerical feature names\n",
    "        apply_pca: Whether to apply PCA for dimensionality reduction\n",
    "        pca_components: Number of components or variance ratio to keep\n",
    "        \n",
    "    Returns:\n",
    "        Scikit-learn preprocessing pipeline\n",
    "    \"\"\"\n",
    "    # Use ordinal encoding for high-cardinality features (more than 10 categories)\n",
    "    high_cardinality_features = []\n",
    "    low_cardinality_features = []\n",
    "    \n",
    "    for feature in categorical_features:\n",
    "        # This will be computed at runtime based on actual data\n",
    "        # We're just initializing the lists here\n",
    "        high_cardinality_features.append(feature)\n",
    "    \n",
    "    # Create column transformer with appropriate encoders\n",
    "    transformers = []\n",
    "    \n",
    "    # One-hot encoding for low-cardinality features\n",
    "    if low_cardinality_features:\n",
    "        transformers.append(\n",
    "            ('ohe', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                ('encoder', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
    "            ]), low_cardinality_features)\n",
    "        )\n",
    "    \n",
    "    # Ordinal encoding for high-cardinality features\n",
    "    if high_cardinality_features:\n",
    "        transformers.append(\n",
    "            ('ordinal', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "            ]), high_cardinality_features)\n",
    "        )\n",
    "    \n",
    "    # Standard scaling for numerical features\n",
    "    if numerical_features:\n",
    "        num_pipeline = [\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]\n",
    "        \n",
    "        # Add PCA if requested\n",
    "        if apply_pca:\n",
    "            num_pipeline.append(('pca', PCA(n_components=pca_components)))\n",
    "            \n",
    "        transformers.append(\n",
    "            ('num', Pipeline(num_pipeline), numerical_features)\n",
    "        )\n",
    "    \n",
    "    # Create the column transformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=transformers,\n",
    "        remainder='drop'  # Drop any columns not specified\n",
    "    )\n",
    "    \n",
    "    return preprocessor\n",
    "\n",
    "def train_linear_regression(X, y, categorical_features, numerical_features, do_hyper_tuning=True, apply_pca=True):\n",
    "    \"\"\"\n",
    "    Train a multivariate linear regression model with hyperparameter tuning\n",
    "    \n",
    "    Args:\n",
    "        X: Feature DataFrame\n",
    "        y: Target series\n",
    "        categorical_features: List of categorical feature names\n",
    "        numerical_features: List of numerical feature names\n",
    "        do_hyper_tuning: Whether to perform hyperparameter tuning\n",
    "        apply_pca: Whether to apply PCA for dimensionality reduction\n",
    "        \n",
    "    Returns:\n",
    "        Trained model, preprocessor, and metrics\n",
    "    \"\"\"\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create preprocessing pipeline\n",
    "    preprocessor = create_preprocessing_pipeline(\n",
    "        categorical_features, \n",
    "        numerical_features,\n",
    "        apply_pca=apply_pca\n",
    "    )\n",
    "    \n",
    "    if do_hyper_tuning:\n",
    "        # Create pipeline with multiple regression models\n",
    "        param_grid = [\n",
    "            {\n",
    "                'preprocessor__num__pca__n_components': [0.85, 0.9, 0.95] if apply_pca else [None],\n",
    "                'regressor': [LinearRegression()],\n",
    "                'regressor__fit_intercept': [True, False]\n",
    "            },\n",
    "            {\n",
    "                'preprocessor__num__pca__n_components': [0.85, 0.9, 0.95] if apply_pca else [None],\n",
    "                'regressor': [Ridge()],\n",
    "                'regressor__alpha': [0.01, 0.1, 1.0, 10.0],\n",
    "                'regressor__fit_intercept': [True, False]\n",
    "            },\n",
    "            {\n",
    "                'preprocessor__num__pca__n_components': [0.85, 0.9, 0.95] if apply_pca else [None],\n",
    "                'regressor': [Lasso()],\n",
    "                'regressor__alpha': [0.001, 0.01, 0.1, 1.0],\n",
    "                'regressor__fit_intercept': [True, False]\n",
    "            },\n",
    "            {\n",
    "                'preprocessor__num__pca__n_components': [0.85, 0.9, 0.95] if apply_pca else [None],\n",
    "                'regressor': [ElasticNet()],\n",
    "                'regressor__alpha': [0.001, 0.01, 0.1, 1.0],\n",
    "                'regressor__l1_ratio': [0.1, 0.5, 0.7, 0.9],\n",
    "                'regressor__fit_intercept': [True, False]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Create pipeline\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', LinearRegression())\n",
    "        ])\n",
    "        \n",
    "        # Use RandomizedSearchCV for more efficient hyperparameter search\n",
    "        grid_search = RandomizedSearchCV(\n",
    "            pipeline,\n",
    "            param_grid,\n",
    "            cv=5,\n",
    "            scoring='neg_mean_squared_error',\n",
    "            n_iter=15,  # Number of parameter settings sampled\n",
    "            n_jobs=-1,\n",
    "            verbose=1,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        print(\"\\nPerforming hyperparameter tuning for Linear Model...\")\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        best_model = grid_search.best_estimator_\n",
    "        print(f\"Best model: {grid_search.best_params_}\")\n",
    "        print(f\"Best cross-validation score: {-grid_search.best_score_:.2f} MSE\")\n",
    "        \n",
    "    else:\n",
    "        # Use standard Linear Regression without tuning\n",
    "        best_model = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', LinearRegression())\n",
    "        ])\n",
    "        best_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    metrics = {\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2\n",
    "    }\n",
    "    \n",
    "    print(\"\\nLinear Regression Metrics:\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"R2 Score: {r2:.4f}\")\n",
    "    \n",
    "    # Visualize actual vs predicted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')\n",
    "    plt.xlabel('Actual Prices')\n",
    "    plt.ylabel('Predicted Prices')\n",
    "    plt.title('Linear Regression: Actual vs Predicted Prices')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('linear_regression_results.png')\n",
    "    \n",
    "    return best_model, preprocessor, metrics\n",
    "\n",
    "def train_xgboost(X, y, categorical_features, numerical_features, do_hyper_tuning=True, apply_pca=False):\n",
    "    \"\"\"\n",
    "    Train an XGBoost gradient boosting model with hyperparameter tuning\n",
    "    \n",
    "    Args:\n",
    "        X: Feature DataFrame\n",
    "        y: Target series\n",
    "        categorical_features: List of categorical feature names\n",
    "        numerical_features: List of numerical feature names\n",
    "        do_hyper_tuning: Whether to perform hyperparameter tuning\n",
    "        apply_pca: Whether to apply PCA for dimensionality reduction\n",
    "        \n",
    "    Returns:\n",
    "        Trained model, preprocessor, and metrics\n",
    "    \"\"\"\n",
    "    # Create preprocessing pipeline\n",
    "    preprocessor = create_preprocessing_pipeline(\n",
    "        categorical_features, \n",
    "        numerical_features,\n",
    "        apply_pca=apply_pca\n",
    "    )\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Preprocess the data\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "    \n",
    "    if do_hyper_tuning:\n",
    "        # Define parameter grid for hyperparameter tuning\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [3, 5, 7, 9],\n",
    "            'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "            'subsample': [0.6, 0.8, 1.0],\n",
    "            'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "            'min_child_weight': [1, 3, 5],\n",
    "            'gamma': [0, 0.1, 0.2],\n",
    "            'reg_alpha': [0, 0.1, 1],\n",
    "            'reg_lambda': [1, 1.5, 2]\n",
    "        }\n",
    "        \n",
    "        # Use RandomizedSearchCV for more efficient hyperparameter search\n",
    "        random_search = RandomizedSearchCV(\n",
    "            xgb.XGBRegressor(\n",
    "                objective='reg:squarederror',\n",
    "                random_state=42\n",
    "            ),\n",
    "            param_distributions=param_grid,\n",
    "            n_iter=20,  # Number of parameter settings sampled\n",
    "            scoring='neg_mean_squared_error',\n",
    "            cv=5,\n",
    "            verbose=1,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        print(\"\\nPerforming hyperparameter tuning for XGBoost...\")\n",
    "        random_search.fit(X_train_processed, y_train)\n",
    "        \n",
    "        # Get best parameters\n",
    "        best_params = random_search.best_params_\n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "        print(f\"Best cross-validation score: {-random_search.best_score_:.2f} MSE\")\n",
    "        \n",
    "        # Create and train the XGBoost model with best parameters\n",
    "        xgb_model = xgb.XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            random_state=42,\n",
    "            **best_params\n",
    "        )\n",
    "    else:\n",
    "        # Define XGBoost parameters\n",
    "        xgb_params = {\n",
    "            'n_estimators': 200,\n",
    "            'max_depth': 6,\n",
    "            'learning_rate': 0.1,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'objective': 'reg:squarederror',\n",
    "            'random_state': 42\n",
    "        }\n",
    "        \n",
    "        # Create the XGBoost model\n",
    "        xgb_model = xgb.XGBRegressor(**xgb_params)\n",
    "    \n",
    "    # Train the model for XGBoost 2.1.4\n",
    "    eval_set = [(X_test_processed, y_test)]\n",
    "    \n",
    "    try:\n",
    "        # For XGBoost 2.1.4, we need to use callbacks instead of direct parameters\n",
    "        from xgboost.callback import EarlyStopping\n",
    "        \n",
    "        early_stopping = EarlyStopping(\n",
    "            rounds=20,\n",
    "            metric_name='rmse',\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        xgb_model.fit(\n",
    "            X_train_processed, y_train,\n",
    "            eval_set=eval_set,\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=True\n",
    "        )\n",
    "    except (ImportError, TypeError) as e:\n",
    "        # Fall back to basic method if needed\n",
    "        print(f\"Using basic XGBoost training method due to: {e}\")\n",
    "        xgb_model.fit(X_train_processed, y_train)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred = xgb_model.predict(X_test_processed)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    metrics = {\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2\n",
    "    }\n",
    "    \n",
    "    print(\"\\nXGBoost Metrics:\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"R2 Score: {r2:.4f}\")\n",
    "    \n",
    "    # Visualize actual vs predicted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')\n",
    "    plt.xlabel('Actual Prices')\n",
    "    plt.ylabel('Predicted Prices')\n",
    "    plt.title('XGBoost: Actual vs Predicted Prices')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('xgboost_results.png')\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    xgb.plot_importance(xgb_model, max_num_features=20)\n",
    "    plt.title('XGBoost Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('xgboost_feature_importance.png')\n",
    "    \n",
    "    return xgb_model, preprocessor, metrics\n",
    "\n",
    "def create_dnn_model(input_dim, config):\n",
    "    \"\"\"\n",
    "    Create a DNN model based on configuration parameters\n",
    "    \n",
    "    Args:\n",
    "        input_dim: Number of input features\n",
    "        config: Dictionary with model configuration\n",
    "        \n",
    "    Returns:\n",
    "        Compiled Keras model\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First hidden layer\n",
    "    model.add(Dense(\n",
    "        config.get('first_layer_units', 128), \n",
    "        activation=config.get('activation', 'relu'), \n",
    "        input_dim=input_dim\n",
    "    ))\n",
    "    \n",
    "    if config.get('use_batch_norm', True):\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "    if config.get('dropout_rate', 0.3) > 0:\n",
    "        model.add(Dropout(config.get('dropout_rate', 0.3)))\n",
    "    \n",
    "    # Hidden layers\n",
    "    for units in config.get('hidden_layers', [64, 32]):\n",
    "        model.add(Dense(units, activation=config.get('activation', 'relu')))\n",
    "        \n",
    "        if config.get('use_batch_norm', True):\n",
    "            model.add(BatchNormalization())\n",
    "            \n",
    "        if config.get('dropout_rate', 0.3) > 0:\n",
    "            model.add(Dropout(config.get('dropout_rate', 0.3) * 0.7))  # Reduce dropout in deeper layers\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=config.get('optimizer', 'adam'),\n",
    "        loss=config.get('loss', 'mse'),\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def tune_dnn_hyperparameters(X_train, y_train, input_dim):\n",
    "    \"\"\"\n",
    "    Tune hyperparameters for the DNN model\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        y_train: Training target\n",
    "        input_dim: Number of input features\n",
    "        \n",
    "    Returns:\n",
    "        Best hyperparameter configuration\n",
    "    \"\"\"\n",
    "    # Define hyperparameter configurations to try\n",
    "    configs = [\n",
    "        # Configuration 1\n",
    "        {\n",
    "            'first_layer_units': 128,\n",
    "            'hidden_layers': [64, 32],\n",
    "            'activation': 'relu',\n",
    "            'optimizer': 'adam',\n",
    "            'learning_rate': 0.001,\n",
    "            'batch_size': 32,\n",
    "            'dropout_rate': 0.3,\n",
    "            'use_batch_norm': True\n",
    "        },\n",
    "        # Configuration 2\n",
    "        {\n",
    "            'first_layer_units': 256,\n",
    "            'hidden_layers': [128, 64],\n",
    "            'activation': 'relu',\n",
    "            'optimizer': 'adam',\n",
    "            'learning_rate': 0.001,\n",
    "            'batch_size': 64,\n",
    "            'dropout_rate': 0.4,\n",
    "            'use_batch_norm': True\n",
    "        },\n",
    "        # Configuration 3\n",
    "        {\n",
    "            'first_layer_units': 128,\n",
    "            'hidden_layers': [128, 64, 32],\n",
    "            'activation': 'elu',\n",
    "            'optimizer': 'adam',\n",
    "            'learning_rate': 0.0005,\n",
    "            'batch_size': 32,\n",
    "            'dropout_rate': 0.3,\n",
    "            'use_batch_norm': True\n",
    "        },\n",
    "        # Configuration 4\n",
    "        {\n",
    "            'first_layer_units': 64,\n",
    "            'hidden_layers': [32, 16],\n",
    "            'activation': 'relu',\n",
    "            'optimizer': 'adam',\n",
    "            'learning_rate': 0.001,\n",
    "            'batch_size': 16,\n",
    "            'dropout_rate': 0.2,\n",
    "            'use_batch_norm': True\n",
    "        },\n",
    "        # Configuration 5\n",
    "        {\n",
    "            'first_layer_units': 128,\n",
    "            'hidden_layers': [64],\n",
    "            'activation': 'relu',\n",
    "            'optimizer': 'adam',\n",
    "            'learning_rate': 0.001,\n",
    "            'batch_size': 32,\n",
    "            'dropout_rate': 0.3,\n",
    "            'use_batch_norm': False\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Validation split\n",
    "    validation_split = 0.2\n",
    "    val_size = int(len(X_train) * validation_split)\n",
    "    X_train_val, X_val = X_train[:-val_size], X_train[-val_size:]\n",
    "    y_train_val, y_val = y_train[:-val_size], y_train[-val_size:]\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_config = None\n",
    "    \n",
    "    # Define early stopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\nPerforming hyperparameter tuning for DNN...\")\n",
    "    for i, config in enumerate(configs):\n",
    "        print(f\"\\nTrying configuration {i+1}/{len(configs)}\")\n",
    "        \n",
    "        # Create model\n",
    "        model = create_dnn_model(input_dim, config)\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            X_train_val, y_train_val,\n",
    "            epochs=100,\n",
    "            batch_size=config['batch_size'],\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        val_loss = min(history.history['val_loss'])\n",
    "        print(f\"Configuration {i+1} validation loss: {val_loss:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_config = config\n",
    "            print(f\"New best configuration found!\")\n",
    "    \n",
    "    print(f\"\\nBest DNN configuration: {best_config}\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    return best_config\n",
    "\n",
    "def train_dnn(X, y, categorical_features, numerical_features, do_hyper_tuning=True, apply_pca=False):\n",
    "    \"\"\"\n",
    "    Train a Deep Neural Network model with hyperparameter tuning\n",
    "    \n",
    "    Args:\n",
    "        X: Feature DataFrame\n",
    "        y: Target series\n",
    "        categorical_features: List of categorical feature names\n",
    "        numerical_features: List of numerical feature names\n",
    "        do_hyper_tuning: Whether to perform hyperparameter tuning\n",
    "        apply_pca: Whether to apply PCA for dimensionality reduction\n",
    "        \n",
    "    Returns:\n",
    "        Trained model, preprocessor, history, and metrics\n",
    "    \"\"\"\n",
    "    # Create preprocessing pipeline\n",
    "    preprocessor = create_preprocessing_pipeline(\n",
    "        categorical_features, \n",
    "        numerical_features,\n",
    "        apply_pca=apply_pca\n",
    "    )\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Preprocess the data\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "    \n",
    "    # Standardize the target variable for better neural network training\n",
    "    y_scaler = StandardScaler()\n",
    "    y_train_scaled = y_scaler.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
    "    y_test_scaled = y_scaler.transform(y_test.values.reshape(-1, 1)).ravel()\n",
    "    \n",
    "    # Get the number of features after preprocessing\n",
    "    input_dim = X_train_processed.shape[1]\n",
    "    \n",
    "    if do_hyper_tuning:\n",
    "        # Tune hyperparameters\n",
    "        best_config = tune_dnn_hyperparameters(X_train_processed, y_train_scaled, input_dim)\n",
    "        \n",
    "        # Create and compile the model with best configuration\n",
    "        dnn_model = create_dnn_model(input_dim, best_config)\n",
    "        batch_size = best_config['batch_size']\n",
    "    else:\n",
    "        # Default configuration\n",
    "        default_config = {\n",
    "            'first_layer_units': 128,\n",
    "            'hidden_layers': [64, 32],\n",
    "            'activation': 'relu',\n",
    "            'optimizer': 'adam',\n",
    "            'batch_size': 32,\n",
    "            'dropout_rate': 0.3,\n",
    "            'use_batch_norm': True\n",
    "        }\n",
    "        \n",
    "        # Create and compile the model with default configuration\n",
    "        dnn_model = create_dnn_model(input_dim, default_config)\n",
    "        batch_size = default_config['batch_size']\n",
    "    \n",
    "    # Define early stopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=20,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = dnn_model.fit(\n",
    "        X_train_processed, y_train_scaled,\n",
    "        epochs=200,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_scaled = dnn_model.predict(X_test_processed)\n",
    "    \n",
    "    # Inverse transform to get actual price predictions\n",
    "    y_pred = y_scaler.inverse_transform(y_pred_scaled).ravel()\n",
    "    \n",
    "    # Evaluate the model\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    metrics = {\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2\n",
    "    }\n",
    "    \n",
    "    print(\"\\nDNN Metrics:\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"R2 Score: {r2:.4f}\")\n",
    "    \n",
    "    # Plot training & validation loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('DNN Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('dnn_training_history.png')\n",
    "    \n",
    "    # Visualize actual vs predicted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')\n",
    "    plt.xlabel('Actual Prices')\n",
    "    plt.ylabel('Predicted Prices')\n",
    "    plt.title('DNN: Actual vs Predicted Prices')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('dnn_results.png')\n",
    "    \n",
    "    return dnn_model, preprocessor, history, metrics\n",
    "\n",
    "def predict_with_sentiment(model, preprocessor, X, town_data):\n",
    "    \"\"\"\n",
    "    Make predictions with a trained model, using sentiment data that's already in X\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        preprocessor: Fitted preprocessor\n",
    "        X: Feature DataFrame (with sentiment columns)\n",
    "        town_data: Town names for each instance\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with predictions\n",
    "    \"\"\"\n",
    "    # Create a copy of X to avoid modifying the original\n",
    "    X_pred = X.copy()\n",
    "    \n",
    "    # Fill missing sentiment scores with the mean (if any)\n",
    "    sentiment_cols = [col for col in X_pred.columns if 'sentiment' in col]\n",
    "    for col in sentiment_cols:\n",
    "        if col in X_pred.columns and X_pred[col].isnull().any():\n",
    "            mean_value = X_pred[col].mean()\n",
    "            X_pred[col].fillna(mean_value, inplace=True)\n",
    "    \n",
    "    # Preprocess the data\n",
    "    X_processed = preprocessor.transform(X_pred)\n",
    "    \n",
    "    # Make predictions\n",
    "    if hasattr(model, 'predict'):  # sklearn, xgboost\n",
    "        predictions = model.predict(X_processed)\n",
    "    else:  # keras (TensorFlow)\n",
    "        predictions = model.predict(X_processed).flatten()\n",
    "    \n",
    "    # Create a DataFrame with predictions\n",
    "    results = pd.DataFrame({\n",
    "        'town': town_data,\n",
    "        'predicted_resale_price': predictions\n",
    "    })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def compare_models(metrics_dict):\n",
    "    \"\"\"\n",
    "    Compare the performance of different models\n",
    "    \n",
    "    Args:\n",
    "        metrics_dict: Dictionary with model names as keys and their metrics as values\n",
    "    \"\"\"\n",
    "    models = list(metrics_dict.keys())\n",
    "    rmse_values = [metrics_dict[model]['rmse'] for model in models]\n",
    "    mae_values = [metrics_dict[model]['mae'] for model in models]\n",
    "    r2_values = [metrics_dict[model]['r2'] for model in models]\n",
    "    \n",
    "    # Plot RMSE comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.bar(models, rmse_values)\n",
    "    plt.title('RMSE Comparison')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.bar(models, mae_values)\n",
    "    plt.title('MAE Comparison')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.bar(models, r2_values)\n",
    "    plt.title('R Score Comparison')\n",
    "    plt.ylabel('R')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_comparison.png')\n",
    "    \n",
    "    # Print comparison table\n",
    "    print(\"\\nModel Comparison:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Model':<15} {'RMSE':>10} {'MAE':>10} {'R':>10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for model in models:\n",
    "        print(f\"{model:<15} {metrics_dict[model]['rmse']:>10.2f} {metrics_dict[model]['mae']:>10.2f} {metrics_dict[model]['r2']:>10.4f}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the entire modeling pipeline\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    df = load_and_preprocess_data('merged_data_3.csv')\n",
    "    \n",
    "    # Check for sentiment data\n",
    "    sentiment_cols = [col for col in df.columns if 'sentiment' in col]\n",
    "    sentiment_available = len(sentiment_cols) > 0\n",
    "    \n",
    "    if not sentiment_available:\n",
    "        print(\"No sentiment columns found in the dataset. Some analyses will be skipped.\")\n",
    "    else:\n",
    "        print(f\"Found sentiment columns: {sentiment_cols}\")\n",
    "    \n",
    "    # Prepare features\n",
    "    X, y, categorical_features, numerical_features = prepare_features(df)\n",
    "    \n",
    "    # Set hyperparameter tuning and PCA flags\n",
    "    do_hyper_tuning = True\n",
    "    apply_pca_linear = True      # PCA often helps linear models\n",
    "    apply_pca_xgboost = False    # Tree-based models like XGBoost can handle high-dimensionality\n",
    "    apply_pca_dnn = False        # DNNs can learn representations from raw features\n",
    "    \n",
    "    print(\"\\n========== Training Linear Regression Model ==========\")\n",
    "    linear_model, linear_preprocessor, linear_metrics = train_linear_regression(\n",
    "        X, y, categorical_features, numerical_features, \n",
    "        do_hyper_tuning=do_hyper_tuning, \n",
    "        apply_pca=apply_pca_linear\n",
    "    )\n",
    "    \n",
    "    print(\"\\n========== Training XGBoost Model ==========\")\n",
    "    xgb_model, xgb_preprocessor, xgb_metrics = train_xgboost(\n",
    "        X, y, categorical_features, numerical_features, \n",
    "        do_hyper_tuning=do_hyper_tuning, \n",
    "        apply_pca=apply_pca_xgboost\n",
    "    )\n",
    "    \n",
    "    print(\"\\n========== Training DNN Model ==========\")\n",
    "    dnn_model, dnn_preprocessor, dnn_history, dnn_metrics = train_dnn(\n",
    "        X, y, categorical_features, numerical_features, \n",
    "        do_hyper_tuning=do_hyper_tuning, \n",
    "        apply_pca=apply_pca_dnn\n",
    "    )\n",
    "    \n",
    "    # Compare models\n",
    "    all_metrics = {\n",
    "        'Linear Regression': linear_metrics,\n",
    "        'XGBoost': xgb_metrics,\n",
    "        'DNN': dnn_metrics\n",
    "    }\n",
    "    \n",
    "    compare_models(all_metrics)\n",
    "    \n",
    "    # Get best model based on R score\n",
    "    best_model_name = max(all_metrics, key=lambda x: all_metrics[x]['r2'])\n",
    "    print(f\"\\nBest model based on R score: {best_model_name}\")\n",
    "    \n",
    "    # Save the best model for future use\n",
    "    if best_model_name == 'Linear Regression':\n",
    "        best_model = linear_model\n",
    "        best_preprocessor = linear_preprocessor\n",
    "    elif best_model_name == 'XGBoost':\n",
    "        best_model = xgb_model\n",
    "        best_preprocessor = xgb_preprocessor\n",
    "    else:  # DNN\n",
    "        best_model = dnn_model\n",
    "        best_preprocessor = dnn_preprocessor\n",
    "    \n",
    "    # If sentiment data is available, show example of prediction and analyze sentiment impact\n",
    "    if sentiment_available:\n",
    "        print(\"\\nExample of prediction with sentiment data:\")\n",
    "        # Create a sample DataFrame for prediction\n",
    "        sample_X = X.loc[X.index[:5]].copy()\n",
    "        \n",
    "        # Make predictions\n",
    "        sample_predictions = predict_with_sentiment(\n",
    "            best_model, best_preprocessor, sample_X, df['town'].iloc[:5]\n",
    "        )\n",
    "        \n",
    "        print(sample_predictions)\n",
    "        \n",
    "        # Analyze sentiment impact\n",
    "        print(\"\\nExploring impact of sentiment on predictions:\")\n",
    "        # Create copies with modified sentiment values\n",
    "        high_sentiment_X = sample_X.copy()\n",
    "        low_sentiment_X = sample_X.copy()\n",
    "        \n",
    "        # Modify sentiment values (increase/decrease by 0.2)\n",
    "        for col in sentiment_cols:\n",
    "            if col in high_sentiment_X.columns:\n",
    "                high_sentiment_X[col] = high_sentiment_X[col].apply(lambda x: min(x + 0.2, 1.0))\n",
    "                low_sentiment_X[col] = low_sentiment_X[col].apply(lambda x: max(x - 0.2, -1.0))\n",
    "        \n",
    "        # Make predictions with modified sentiment\n",
    "        high_predictions = predict_with_sentiment(\n",
    "            best_model, best_preprocessor, high_sentiment_X, df['town'].iloc[:5]\n",
    "        )\n",
    "        \n",
    "        low_predictions = predict_with_sentiment(\n",
    "            best_model, best_preprocessor, low_sentiment_X, df['town'].iloc[:5]\n",
    "        )\n",
    "        \n",
    "        # Combine results to show sentiment impact\n",
    "        sentiment_impact = pd.DataFrame({\n",
    "            'town': df['town'].iloc[:5],\n",
    "            'original_price': sample_predictions['predicted_resale_price'],\n",
    "            'high_sentiment_price': high_predictions['predicted_resale_price'],\n",
    "            'low_sentiment_price': low_predictions['predicted_resale_price']\n",
    "        })\n",
    "        \n",
    "        sentiment_impact['high_sentiment_diff_pct'] = (\n",
    "            (sentiment_impact['high_sentiment_price'] - sentiment_impact['original_price']) / \n",
    "            sentiment_impact['original_price'] * 100\n",
    "        ).round(2)\n",
    "        \n",
    "        sentiment_impact['low_sentiment_diff_pct'] = (\n",
    "            (sentiment_impact['low_sentiment_price'] - sentiment_impact['original_price']) / \n",
    "            sentiment_impact['original_price'] * 100\n",
    "        ).round(2)\n",
    "        \n",
    "        print(\"\\nImpact of Sentiment Analysis on Predictions:\")\n",
    "        print(sentiment_impact[['town', 'high_sentiment_diff_pct', 'low_sentiment_diff_pct']])\n",
    "        \n",
    "        # Visualize sentiment impact\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Create positions for bars\n",
    "        towns = sentiment_impact['town'].tolist()\n",
    "        x = np.arange(len(towns))\n",
    "        width = 0.25\n",
    "        \n",
    "        # Create bars\n",
    "        plt.bar(x - width, sentiment_impact['low_sentiment_diff_pct'], width, label='Negative Sentiment Impact')\n",
    "        plt.bar(x + width, sentiment_impact['high_sentiment_diff_pct'], width, label='Positive Sentiment Impact')\n",
    "        \n",
    "        # Customize chart\n",
    "        plt.xlabel('Town')\n",
    "        plt.ylabel('Price Change (%)')\n",
    "        plt.title('Impact of Sentiment Analysis on Predicted Resale Prices')\n",
    "        plt.xticks(x, towns, rotation=45)\n",
    "        plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('sentiment_impact.png')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af40304b-d03a-4cea-92b3-3b39bb975ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Boosting Attempt 1 (Wrong Encoding)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "import xgboost as xgb\n",
    "from joblib import dump, load\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def optimize_xgboost_final(X, y, categorical_features, numerical_features, base_metrics, apply_pca=False, pca_components=0.95):\n",
    "    \"\"\"\n",
    "    Simplified XGBoost optimization compatible with XGBoost 2.1.4\n",
    "    \n",
    "    Args:\n",
    "        X: Feature DataFrame\n",
    "        y: Target series\n",
    "        categorical_features: List of categorical feature names\n",
    "        numerical_features: List of numerical feature names\n",
    "        base_metrics: Metrics from the base XGBoost model for comparison\n",
    "        apply_pca: Whether to apply PCA (True/False)\n",
    "        pca_components: Number of PCA components or variance threshold\n",
    "        \n",
    "    Returns:\n",
    "        Trained model, preprocessor, metrics, and feature importances\n",
    "    \"\"\"\n",
    "    print(\"\\n========== Training XGBoost Model V2 (XGBoost 2.1.4 Compatible) ==========\")\n",
    "    \n",
    "    # Create preprocessing pipeline\n",
    "    preprocessor = create_preprocessing_pipeline(\n",
    "        categorical_features, \n",
    "        numerical_features,\n",
    "        apply_pca=apply_pca,\n",
    "        pca_components=pca_components\n",
    "    )\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Preprocess the data\n",
    "    print(\"Preprocessing training data...\")\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    print(\"Preprocessing test data...\")\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "    \n",
    "    # Check processed data size and shape\n",
    "    print(f\"Processed training data shape: {X_train_processed.shape}\")\n",
    "    print(f\"Processed test data shape: {X_test_processed.shape}\")\n",
    "    print(f\"Memory usage estimation: {X_train_processed.nbytes / (1024**3):.2f} GB\")\n",
    "    \n",
    "    # Parameter combinations to try (simplified set)\n",
    "    param_combinations = [\n",
    "        # Basic configurations\n",
    "        {'n_estimators': 200, 'max_depth': 7, 'learning_rate': 0.1, 'subsample': 0.8, 'colsample_bytree': 0.8},\n",
    "        {'n_estimators': 300, 'max_depth': 9, 'learning_rate': 0.1, 'subsample': 1.0, 'colsample_bytree': 0.8},\n",
    "        \n",
    "        # Additional regularization\n",
    "        {'n_estimators': 300, 'max_depth': 9, 'learning_rate': 0.1, 'subsample': 1.0, \n",
    "         'colsample_bytree': 0.8, 'reg_alpha': 0.5, 'reg_lambda': 1.0},\n",
    "         \n",
    "        # Fine-tuning configuration\n",
    "        {'n_estimators': 300, 'max_depth': 9, 'learning_rate': 0.1, 'subsample': 1.0, \n",
    "         'colsample_bytree': 0.8, 'reg_alpha': 1.0, 'reg_lambda': 1.0, 'min_child_weight': 3}\n",
    "    ]\n",
    "    \n",
    "    best_model = None\n",
    "    best_score = float('inf')  # For RMSE (lower is better)\n",
    "    best_params = {}\n",
    "    \n",
    "    # Test each combination\n",
    "    for i, params in enumerate(param_combinations):\n",
    "        print(f\"\\nTrying combination {i+1}/{len(param_combinations)}:\")\n",
    "        print(params)\n",
    "        \n",
    "        # Create model with current parameters\n",
    "        model = xgb.XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            random_state=42,\n",
    "            tree_method='hist',  # Memory efficient algorithm\n",
    "            **params\n",
    "        )\n",
    "        \n",
    "        # Basic training without any additional parameters\n",
    "        try:\n",
    "            model.fit(X_train_processed, y_train)\n",
    "            \n",
    "            # Evaluate\n",
    "            y_pred = model.predict(X_test_processed)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            \n",
    "            print(f\"RMSE: {rmse:.2f}\")\n",
    "            \n",
    "            # Check if this is the best model so far\n",
    "            if rmse < best_score:\n",
    "                best_score = rmse\n",
    "                best_model = model\n",
    "                best_params = params\n",
    "                print(f\"New best model! RMSE: {rmse:.2f}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error training model: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # If no successful models, use a default configuration\n",
    "    if best_model is None:\n",
    "        print(\"No successful models were trained. Using a default model as fallback.\")\n",
    "        default_params = {\n",
    "            'n_estimators': 300,\n",
    "            'max_depth': 9,\n",
    "            'learning_rate': 0.1,\n",
    "            'subsample': 1.0,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'tree_method': 'hist',\n",
    "            'objective': 'reg:squarederror',\n",
    "            'random_state': 42\n",
    "        }\n",
    "        best_model = xgb.XGBRegressor(**default_params)\n",
    "        best_model.fit(X_train_processed, y_train)\n",
    "        best_params = default_params\n",
    "    \n",
    "    # Final evaluation\n",
    "    y_pred = best_model.predict(X_test_processed)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    metrics = {\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2\n",
    "    }\n",
    "    \n",
    "    print(\"\\nXGBoost V2 Final Metrics:\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"R2 Score: {r2:.4f}\")\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    \n",
    "    # Compare with base model\n",
    "    improvement_rmse = ((base_metrics['rmse'] - rmse) / base_metrics['rmse']) * 100\n",
    "    improvement_mae = ((base_metrics['mae'] - mae) / base_metrics['mae']) * 100\n",
    "    improvement_r2 = ((r2 - base_metrics['r2']) / base_metrics['r2']) * 100\n",
    "    \n",
    "    print(\"\\nImprovement over base XGBoost model:\")\n",
    "    print(f\"RMSE: {improvement_rmse:.2f}%\")\n",
    "    print(f\"MAE: {improvement_mae:.2f}%\")\n",
    "    print(f\"R2: {improvement_r2:.2f}%\")\n",
    "    \n",
    "    # Try to get feature names\n",
    "    try:\n",
    "        feature_names = preprocessor.get_feature_names_out()\n",
    "    except:\n",
    "        feature_names = [f\"feature_{i}\" for i in range(X_train_processed.shape[1])]\n",
    "    \n",
    "    # Handle potential mismatch in feature dimensions\n",
    "    if len(feature_names) != len(best_model.feature_importances_):\n",
    "        print(f\"Warning: Feature names count ({len(feature_names)}) doesn't match model features count ({len(best_model.feature_importances_)})\")\n",
    "        feature_names = [f\"feature_{i}\" for i in range(len(best_model.feature_importances_))]\n",
    "    \n",
    "    # Get feature importance\n",
    "    feature_importance = analyze_feature_importance(best_model, feature_names)\n",
    "    \n",
    "    # Sample data points for plotting to avoid memory issues\n",
    "    sample_size = min(5000, len(y_test))\n",
    "    indices = np.random.choice(len(y_test), sample_size, replace=False)\n",
    "    \n",
    "    # Plot actual vs predicted\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test.iloc[indices], y_pred[indices], alpha=0.5)\n",
    "    plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')\n",
    "    plt.xlabel('Actual Prices')\n",
    "    plt.ylabel('Predicted Prices')\n",
    "    plt.title('XGBoost V2: Actual vs Predicted Prices')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('xgboost_v2_results.png')\n",
    "    \n",
    "    return best_model, preprocessor, metrics, feature_importance\n",
    "\n",
    "def create_preprocessing_pipeline(categorical_features, numerical_features, apply_pca=False, pca_components=0.95):\n",
    "    \"\"\"\n",
    "    Create a preprocessing pipeline with memory efficiency in mind\n",
    "    \n",
    "    Args:\n",
    "        categorical_features: List of categorical feature names\n",
    "        numerical_features: List of numerical feature names\n",
    "        apply_pca: Whether to apply PCA for dimensionality reduction\n",
    "        pca_components: Number of components or variance ratio to keep\n",
    "        \n",
    "    Returns:\n",
    "        Scikit-learn preprocessing pipeline\n",
    "    \"\"\"\n",
    "    # Handle high cardinality features differently to save memory\n",
    "    high_cardinality_features = []\n",
    "    low_cardinality_features = []\n",
    "    \n",
    "    # Manually identify high cardinality features\n",
    "    high_cardinality_candidates = ['town', 'street_name', 'flat_model', 'nearest_mrt', 'month', 'block']\n",
    "    \n",
    "    for feature in categorical_features:\n",
    "        if feature in high_cardinality_candidates:\n",
    "            high_cardinality_features.append(feature)\n",
    "        else:\n",
    "            low_cardinality_features.append(feature)\n",
    "    \n",
    "    print(f\"High cardinality features: {high_cardinality_features}\")\n",
    "    print(f\"Low cardinality features: {low_cardinality_features}\")\n",
    "    \n",
    "    # Create transformers list\n",
    "    transformers = []\n",
    "    \n",
    "    # One-hot encoding for low-cardinality features\n",
    "    if low_cardinality_features:\n",
    "        transformers.append(\n",
    "            ('ohe', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                ('encoder', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
    "            ]), low_cardinality_features)\n",
    "        )\n",
    "    \n",
    "    # Ordinal encoding for high-cardinality features\n",
    "    if high_cardinality_features:\n",
    "        transformers.append(\n",
    "            ('ordinal', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "            ]), high_cardinality_features)\n",
    "        )\n",
    "    \n",
    "    # Standard scaling for numerical features\n",
    "    if numerical_features:\n",
    "        num_pipeline = [\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]\n",
    "        \n",
    "        # Add PCA if requested\n",
    "        if apply_pca:\n",
    "            num_pipeline.append(('pca', PCA(n_components=pca_components)))\n",
    "            \n",
    "        transformers.append(\n",
    "            ('num', Pipeline(num_pipeline), numerical_features)\n",
    "        )\n",
    "    \n",
    "    # Create the column transformer\n",
    "    try:\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=transformers,\n",
    "            remainder='drop',\n",
    "            verbose_feature_names_out=True\n",
    "        )\n",
    "    except TypeError:\n",
    "        # For older sklearn versions\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=transformers,\n",
    "            remainder='drop'\n",
    "        )\n",
    "    \n",
    "    return preprocessor\n",
    "\n",
    "def analyze_feature_importance(model, feature_names):\n",
    "    \"\"\"\n",
    "    Analyze feature importance from the XGBoost model\n",
    "    \n",
    "    Args:\n",
    "        model: Trained XGBoost model\n",
    "        feature_names: List of feature names\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with feature importance\n",
    "    \"\"\"\n",
    "    # Get feature importance\n",
    "    importance = model.feature_importances_\n",
    "    \n",
    "    # Create DataFrame with feature names\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importance\n",
    "    })\n",
    "    \n",
    "    # Sort by importance\n",
    "    feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Plot top 20 features (or fewer if less are available)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    top_n = min(20, len(feature_importance))\n",
    "    top_features = feature_importance.head(top_n)\n",
    "    sns.barplot(x='importance', y='feature', data=top_features)\n",
    "    plt.title(f'XGBoost V2 Feature Importance (Top {top_n})')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('xgboost_v2_feature_importance.png')\n",
    "    \n",
    "    # Print top important features\n",
    "    print(f\"\\nTop {top_n} important features:\")\n",
    "    for index, row in top_features.iterrows():\n",
    "        print(f\"{row['feature']}: {row['importance']:.4f}\")\n",
    "    \n",
    "    return feature_importance\n",
    "\n",
    "def analyze_pca_components(preprocessor, numerical_features):\n",
    "    \"\"\"\n",
    "    Analyze PCA components if PCA was used in preprocessing\n",
    "    \n",
    "    Args:\n",
    "        preprocessor: Fitted preprocessor\n",
    "        numerical_features: Original numerical feature names\n",
    "    \"\"\"\n",
    "    # Find the PCA step in the preprocessor if it exists\n",
    "    pca = None\n",
    "    for name, transformer, features in preprocessor.transformers_:\n",
    "        if name == 'num' and hasattr(transformer, 'named_steps') and 'pca' in transformer.named_steps:\n",
    "            pca = transformer.named_steps['pca']\n",
    "            break\n",
    "    \n",
    "    if pca is None:\n",
    "        print(\"PCA was not found in the pipeline\")\n",
    "        return\n",
    "    \n",
    "    # Get explained variance\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    cumulative_variance = np.cumsum(explained_variance)\n",
    "    \n",
    "    # Plot explained variance (limit components for readability)\n",
    "    max_components = min(20, len(explained_variance))\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(1, max_components + 1), explained_variance[:max_components], alpha=0.7)\n",
    "    plt.step(range(1, max_components + 1), cumulative_variance[:max_components], where='mid', color='red')\n",
    "    plt.axhline(y=0.95, color='k', linestyle='--', alpha=0.5)\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Explained Variance / Cumulative Variance')\n",
    "    plt.title('PCA Components Explained Variance')\n",
    "    plt.savefig('pca_explained_variance.png')\n",
    "    \n",
    "    # Print PCA summary\n",
    "    total_variance = cumulative_variance[-1] if len(cumulative_variance) > 0 else 0\n",
    "    print(f\"\\nPCA retained {len(explained_variance)} components, explaining {total_variance*100:.2f}% of variance\")\n",
    "    \n",
    "    return\n",
    "\n",
    "def compare_model_versions(metrics_dict):\n",
    "    \"\"\"\n",
    "    Compare the performance of different model versions\n",
    "    \n",
    "    Args:\n",
    "        metrics_dict: Dictionary with model names as keys and their metrics as values\n",
    "    \"\"\"\n",
    "    models = list(metrics_dict.keys())\n",
    "    rmse_values = [metrics_dict[model]['rmse'] for model in models]\n",
    "    mae_values = [metrics_dict[model]['mae'] for model in models]\n",
    "    r2_values = [metrics_dict[model]['r2'] for model in models]\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    \n",
    "    # For RMSE (lower is better)\n",
    "    plt.subplot(1, 3, 1)\n",
    "    ax = sns.barplot(x=models, y=rmse_values)\n",
    "    plt.title('RMSE Comparison (lower is better)')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, p in enumerate(ax.patches):\n",
    "        ax.annotate(f'{p.get_height():.0f}', \n",
    "                   (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                   ha = 'center', va = 'bottom',\n",
    "                   xytext = (0, 5), textcoords = 'offset points')\n",
    "    \n",
    "    # For MAE (lower is better)\n",
    "    plt.subplot(1, 3, 2)\n",
    "    ax = sns.barplot(x=models, y=mae_values)\n",
    "    plt.title('MAE Comparison (lower is better)')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, p in enumerate(ax.patches):\n",
    "        ax.annotate(f'{p.get_height():.0f}', \n",
    "                   (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                   ha = 'center', va = 'bottom',\n",
    "                   xytext = (0, 5), textcoords = 'offset points')\n",
    "    \n",
    "    # For R (higher is better)\n",
    "    plt.subplot(1, 3, 3)\n",
    "    ax = sns.barplot(x=models, y=r2_values)\n",
    "    plt.title('R Score Comparison (higher is better)')\n",
    "    plt.ylabel('R')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, p in enumerate(ax.patches):\n",
    "        ax.annotate(f'{p.get_height():.4f}', \n",
    "                   (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                   ha = 'center', va = 'bottom',\n",
    "                   xytext = (0, 5), textcoords = 'offset points')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_version_comparison.png')\n",
    "    \n",
    "    # Print comparison table\n",
    "    print(\"\\nModel Version Comparison:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Model':<15} {'RMSE':>10} {'MAE':>10} {'R':>10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for model in models:\n",
    "        print(f\"{model:<15} {metrics_dict[model]['rmse']:>10.2f} {metrics_dict[model]['mae']:>10.2f} \"\n",
    "              f\"{metrics_dict[model]['r2']:>10.4f}\")\n",
    "    \n",
    "    # Calculate improvements over baseline (Linear Regression)\n",
    "    baseline_model = 'Linear Regression'\n",
    "    if baseline_model in metrics_dict:\n",
    "        print(\"\\nImprovement over Linear Regression baseline:\")\n",
    "        baseline_rmse = metrics_dict[baseline_model]['rmse']\n",
    "        baseline_mae = metrics_dict[baseline_model]['mae']\n",
    "        baseline_r2 = metrics_dict[baseline_model]['r2']\n",
    "        \n",
    "        print(\"-\" * 80)\n",
    "        print(f\"{'Model':<15} {'RMSE Imp%':>15} {'MAE Imp%':>15} {'R Imp%':>15}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for model in models:\n",
    "            if model == baseline_model:\n",
    "                continue\n",
    "                \n",
    "            rmse_imp = ((baseline_rmse - metrics_dict[model]['rmse']) / baseline_rmse) * 100\n",
    "            mae_imp = ((baseline_mae - metrics_dict[model]['mae']) / baseline_mae) * 100\n",
    "            r2_imp = ((metrics_dict[model]['r2'] - baseline_r2) / baseline_r2) * 100\n",
    "            \n",
    "            print(f\"{model:<15} {rmse_imp:>15.2f}% {mae_imp:>15.2f}% {r2_imp:>15.2f}%\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the improved XGBoost training and evaluation\n",
    "    \"\"\"\n",
    "    # Assuming we have the base XGBoost metrics from your previous run\n",
    "    base_xgboost_metrics = {\n",
    "        'rmse': 25266.07,\n",
    "        'mae': 18066.53,\n",
    "        'r2': 0.9801\n",
    "    }\n",
    "    \n",
    "    # Call functions to load and prepare data (these functions should be defined elsewhere)\n",
    "    df = load_and_preprocess_data('merged_data_4.csv')\n",
    "    X, y, categorical_features, numerical_features = prepare_features(df)\n",
    "    \n",
    "    # Print shape information\n",
    "    print(f\"Input data shape: X: {X.shape}, y: {y.shape}\")\n",
    "    print(f\"Categorical features: {len(categorical_features)}\")\n",
    "    print(f\"Numerical features: {len(numerical_features)}\")\n",
    "    \n",
    "    # Train optimized XGBoost with XGBoost 2.1.4 compatibility\n",
    "    xgb_v2_model, xgb_v2_preprocessor, xgb_v2_metrics, feature_importance = optimize_xgboost_final(\n",
    "        X, y, \n",
    "        categorical_features, \n",
    "        numerical_features, \n",
    "        base_xgboost_metrics,\n",
    "        apply_pca=True,  # Apply PCA\n",
    "        pca_components=0.95  # Retain 95% of variance\n",
    "    )\n",
    "    \n",
    "    # Analyze PCA components if PCA was used\n",
    "    analyze_pca_components(xgb_v2_preprocessor, numerical_features)\n",
    "    \n",
    "    # Compare model versions\n",
    "    all_metrics = {\n",
    "        'Linear Regression': {'rmse': 86357.11, 'mae': 66020.02, 'r2': 0.7681},\n",
    "        'XGBoost V1': base_xgboost_metrics,\n",
    "        'DNN': {'rmse': 42849.78, 'mae': 31954.07, 'r2': 0.9429},\n",
    "        'XGBoost V2': xgb_v2_metrics\n",
    "    }\n",
    "    \n",
    "    compare_model_versions(all_metrics)\n",
    "    \n",
    "    # Print final recommendation\n",
    "    print(\"\\nFinal Model Recommendation:\")\n",
    "    best_model = min(all_metrics, key=lambda x: all_metrics[x]['rmse'])\n",
    "    print(f\"Based on RMSE: {best_model}\")\n",
    "    \n",
    "    best_model_r2 = max(all_metrics, key=lambda x: all_metrics[x]['r2'])\n",
    "    print(f\"Based on R: {best_model_r2}\")\n",
    "    \n",
    "    print(\"\\nTop features for predicting housing prices:\")\n",
    "    for index, row in feature_importance.head(10).iterrows():\n",
    "        print(f\"  - {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
